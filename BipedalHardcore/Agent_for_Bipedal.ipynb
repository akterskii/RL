{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Agent for Bipedal.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "hvYHyNAH_Ivr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow-gpu==2.0.0-alpha0\n",
        "!pip install gym\n",
        "!pip install gym[box2d]\n",
        "!pip install -U -q PyDrive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ullLgBKs_KB3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import gym\n",
        "import numpy as np\n",
        "import cv2 as cv\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import preprocessing\n",
        "\n",
        "import pickle\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yU6O4GysclzI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, seed):\n",
        "        self.buffer = []\n",
        "        self.random_generator = np.random.RandomState(seed=seed)\n",
        "        self.max_size = 1000000\n",
        "        self.index = -1\n",
        "\n",
        "    def append(self, cur_state, action, next_state, reward, done):\n",
        "        if done:\n",
        "            final = 1\n",
        "        else:\n",
        "            final = 0\n",
        "\n",
        "        self.index = (self.index + 1) % self.max_size\n",
        "        if self.index >= len(self.buffer):\n",
        "            self.buffer.append([cur_state, action, next_state, reward, final])\n",
        "        else:\n",
        "            self.buffer[self.index] = [cur_state, action, next_state, reward, final]\n",
        "\n",
        "    def get_size(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def get_batch(self, size):\n",
        "        mask = self.random_generator.randint(0, len(self.buffer), size)\n",
        "        \n",
        "        return [self.buffer[id] for id in mask]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6oUdR1ekcoP-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class QFunction:\n",
        "    def __init__(self, num_of_inputs, num_of_actions, image_size=None, layer_units_inputs=[1500, 1000], lr=0.0001):\n",
        "        \n",
        "        \n",
        "        layer_units = layer_units_inputs\n",
        "             \n",
        "        inputs = tf.keras.Input(shape=(num_of_inputs,))\n",
        "        x = tf.keras.layers.Dense(units=layer_units[0])(inputs)\n",
        "        x = tf.keras.layers.PReLU()(x)\n",
        "        for elem in range(1, len(layer_units)):\n",
        "            x = tf.keras.layers.Dense(units=layer_units[elem])(x)\n",
        "            x = tf.keras.layers.PReLU()(x)\n",
        "        outputs = tf.keras.layers.Dense(units=num_of_actions)(x)\n",
        "\n",
        "        self.Q_function = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "        self.Q_function.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(lr))\n",
        "\n",
        "    def predict(self, state):\n",
        "        return self.Q_function.predict(state)\n",
        "\n",
        "    def train_step(self, cur_states, targets):\n",
        "        loss = self.Q_function.train_on_batch(cur_states, targets)\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "olbuGIGW_ObE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    \n",
        "    def __init__(self, env_name='BipedalWalkerHardcore-v2', number_of_steps=10000000, discretization_steps=5, input_type=0,\n",
        "                 batch_size=32, seed=42, episodes_to_average=20, learning_rate=0.0001, scaler = None):\n",
        "\n",
        "        # environment\n",
        "        self.env_name = env_name\n",
        "        self.env = gym.make(env_name)\n",
        "        \n",
        "        # state normalisation - doesn't work here - params unbounded\n",
        "        self.samples_for_states_stats = 10000;\n",
        "        self.state_scaler = None\n",
        "        #self.get_states_stats()\n",
        "        \n",
        "        # learning params\n",
        "        self.number_of_steps = number_of_steps\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # input type\n",
        "        self.input_type = input_type  # 0 - parametrical, 1 -image\n",
        "\n",
        "        # random generator seed\n",
        "        self.seed = seed\n",
        "\n",
        "        # ReplayBuffer for observations\n",
        "        self.replay_buffer = ReplayBuffer(self.seed)\n",
        "        \n",
        "        # discretization\n",
        "        self.discretization_steps = discretization_steps\n",
        "        self.low = self.env.action_space.low\n",
        "        self.action_steps = (self.env.action_space.high - self.env.action_space.low) / discretization_steps\n",
        "        self.num_of_actions = len(self.low)\n",
        "        self.clip_action = 0.8\n",
        "\n",
        "        # Q function\n",
        "        \n",
        "        self.num_of_inputs = len(self.env.observation_space.high)\n",
        "        self.num_of_outputs = discretization_steps ** self.num_of_actions \n",
        "        self.Q_function = QFunction(self.num_of_inputs, self.num_of_outputs, lr=learning_rate, layer_units_inputs=[1500, 1300])\n",
        "        self.gamma = 0.99\n",
        "\n",
        "        # epsilon greedy policy\n",
        "        self.epsilon_start = 0.95\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon = self.epsilon_start\n",
        "        self.epsilon_decay_factor = 0.99991 # decay ~e times in 10000 steps\n",
        "\n",
        "        # training\n",
        "        self.losses = []\n",
        "        self.episodes_rewards = []\n",
        "        self.episodes_end_index = []\n",
        "        self.reward_before_fail = []\n",
        "        self.average_reward_before_fail = []\n",
        "        self.episodes_to_average = episodes_to_average\n",
        "        self.eps = []\n",
        "        \n",
        "        # file ids\n",
        "        self.buffer_file_id = '14ThpPSMN-xL3zp12Vqd-ksVT_qTI-yXf'\n",
        "        self.model_file_id = '15trlpnUvX-N4EYqL4H0P2H_q4aAF9Gdo'\n",
        "        \n",
        "        # scaler\n",
        "        self.scaler = scaler\n",
        "        \n",
        "    def prepropcess_state(self, state):\n",
        "        if self.scaler:\n",
        "            return self.scaler.transform(state.reshape(1, -1))[0]\n",
        "        else:\n",
        "            return state\n",
        "        \n",
        "    # convert number of discreet action to continuous space.\n",
        "    def num_to_action(self, action_num):\n",
        "        assert self.num_of_outputs > action_num\n",
        "        cur_actions = np.zeros(self.num_of_actions)\n",
        "        i = 0\n",
        "        while action_num > 0:\n",
        "            cur_actions[i] = self.low[i] + self.action_steps[i] * (action_num % self.discretization_steps + 0.5)\n",
        "            cur_actions[i] *= self.clip_action\n",
        "            action_num = action_num // self.discretization_steps\n",
        "            i += 1\n",
        "        return cur_actions\n",
        "\n",
        "    # calculate targets\n",
        "    def targets(self, batch): \n",
        "        cur_state, action, next_state, reward, done = zip(*batch)\n",
        "        cur_state = np.array(cur_state)\n",
        "        action = np.array(action)\n",
        "        next_state = np.array(next_state)\n",
        "        reward = np.array(reward)\n",
        "        done = np.array(done)\n",
        "\n",
        "        \n",
        "        target_val = self.Q_function.predict(cur_state)\n",
        "        target = np.max(self.Q_function.predict(next_state), axis=1) * (1 - done) + reward\n",
        "        for i in range(len(target_val)): \n",
        "            target_val[i][action[i]] = target[i]\n",
        "        return cur_state, target_val\n",
        "\n",
        "    def update_epsilon(self):\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay_factor)\n",
        "\n",
        "    # epsilon greedy action\n",
        "    def sample_action(self, state, epsilon):\n",
        "        random_num = np.random.random()\n",
        "        if random_num < epsilon:\n",
        "            action_num = np.random.randint(0, self.num_of_outputs)\n",
        "        else:\n",
        "            q_values = self.Q_function.predict([state])\n",
        "            action_num = np.argmax(q_values)\n",
        "        return action_num, self.num_to_action(action_num)\n",
        "\n",
        "    def plot_data(self, episode_rewards_per_plot=700, losses_per_plot=30000, plot_eps=True):\n",
        "        clear_output(True)\n",
        "        plt.figure(figsize=(20,5))\n",
        "        \n",
        "        plt.subplot(131)\n",
        "        plt.title('Rewards after {} steps'.format(len(self.losses) + self.batch_size - 1))\n",
        "        n = min(len(self.episodes_rewards), episode_rewards_per_plot)\n",
        "        episodes_index = np.arange(len(self.episodes_rewards)-n, len(self.episodes_rewards))\n",
        "        plt.plot(episodes_index, self.episodes_rewards[-n:])\n",
        "        plt.plot(episodes_index, self.average_reward_before_fail[-n:])\n",
        "        \n",
        "        plt.subplot(132)\n",
        "        plt.title('Loss')\n",
        "        n = min(len(self.losses), losses_per_plot)\n",
        "        plt.plot(np.arange(len(self.losses)-n, len(self.losses)), self.losses[-n:])\n",
        "        \n",
        "        if plot_eps:\n",
        "            plt.subplot(133)\n",
        "            plt.title('eps')\n",
        "            plt.plot(self.eps)\n",
        "        plt.show()\n",
        "    \n",
        "    def read_colab(self, buffer_id = '1ImWbV0O0Cbw1HG34-DFbGwh4womhikza', model_id = '1K8pUG7T11tEJ-iQxfoFJ8-0ERWH1377L'):\n",
        "        auth.authenticate_user()\n",
        "        gauth = GoogleAuth()\n",
        "        gauth.credentials = GoogleCredentials.get_application_default()\n",
        "        drive = GoogleDrive(gauth)\n",
        "        \n",
        "        fname = \"buffer.txt\"\n",
        "        file_obj = drive.CreateFile({'id': buffer_id})\n",
        "        file_obj.GetContentFile(fname)\n",
        "        with open (fname, 'rb') as fp:\n",
        "            self.replay_buffer =  pickle.load(fp)\n",
        "        \n",
        "        fname = 'model.h5'\n",
        "        file_obj = drive.CreateFile({'id': model_id})\n",
        "        file_obj.GetContentFile(fname)\n",
        "        self.Q_function.Q_function = tf.keras.models.load_model(fname)\n",
        "        \n",
        "        \n",
        "    def write_colab(self):\n",
        "        auth.authenticate_user()\n",
        "        gauth = GoogleAuth()\n",
        "        gauth.credentials = GoogleCredentials.get_application_default()\n",
        "        drive = GoogleDrive(gauth)\n",
        "        \n",
        "        fname = 'buffer.txt'\n",
        "        with open(fname, 'wb') as fp:\n",
        "            pickle.dump(self.replay_buffer, fp)\n",
        "\n",
        "\n",
        "        model_file = drive.CreateFile({'title' : fname})\n",
        "        model_file.SetContentFile(fname)\n",
        "        model_file.Upload()\n",
        "        \n",
        "        fname = 'model.h5'\n",
        "        self.Q_function.Q_function.save(fname)    \n",
        "        model_file = drive.CreateFile({'title' : fname})\n",
        "        model_file.SetContentFile(fname)\n",
        "        model_file.Upload()\n",
        "\n",
        "        # download to google drive\n",
        "        drive.CreateFile({'id': model_file.get('id')})\n",
        "    \n",
        "    \n",
        "    # run and learn\n",
        "    def run(self, continue_learning = False, set_epsilon_to_min = False):\n",
        "\n",
        "        done = False\n",
        "        cur_state = self.env.reset()\n",
        "        cur_state = self.prepropcess_state(cur_state)\n",
        "        \n",
        "        if set_epsilon_to_min:\n",
        "            self.epsilon = self.epsilon_min\n",
        "        \n",
        "        if not continue_learning:\n",
        "            self.losses = []\n",
        "            self.episodes_rewards = []\n",
        "            self.episodes_end_index = []\n",
        "            self.average_reward_before_fail = []\n",
        "            self.before_fail = []\n",
        "            self.epsilon = self.epsilon_start\n",
        "            self.eps = []\n",
        "            self.Q_function = QFunction(self.num_of_inputs, self.num_of_outputs)\n",
        "            \n",
        "        cur_episode_reward = 0\n",
        "        \n",
        "        for step_num in range(1, self.number_of_steps + 1):\n",
        "\n",
        "            self.update_epsilon() # epsilon decay\n",
        "            self.eps.append(self.epsilon)\n",
        "            \n",
        "            action_num, action = self.sample_action(tf.expand_dims(cur_state, 0), self.epsilon)\n",
        "            \n",
        "            next_state, reward, done, _ = self.env.step(action)            \n",
        "            next_state = self.prepropcess_state(next_state)\n",
        "            cur_episode_reward += reward\n",
        "\n",
        "            self.replay_buffer.append(cur_state, action_num, next_state, reward, done)\n",
        "            cur_state = next_state\n",
        "            \n",
        "            if self.replay_buffer.get_size() >= self.batch_size:\n",
        "                batch = self.replay_buffer.get_batch(self.batch_size)\n",
        "                states, target_qs = self.targets(batch)\n",
        "                \n",
        "                loss = self.Q_function.train_step(states, target_qs)\n",
        "                self.losses.append(loss)\n",
        "            \n",
        "            if done:\n",
        "                self.episodes_rewards.append(cur_episode_reward + 100) # to clear indentify\n",
        "                self.episodes_end_index.append(len(self.losses) - 1)\n",
        "                self.average_reward_before_fail.append(np.mean(self.episodes_rewards[-self.episodes_to_average:]))\n",
        "                cur_episode_reward = 0\n",
        "                cur_state = self.env.reset()\n",
        "                cur_state = self.prepropcess_state(cur_state)\n",
        "                \n",
        "                done = False\n",
        "                \n",
        "            if step_num % 1000 == 0:\n",
        "                self.plot_data()\n",
        "\n",
        "        self.env.close()\n",
        "       \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lQ80mwV6ZVkP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "        \n",
        "fname = \"buffer.txt\"\n",
        "file_obj = drive.CreateFile({'id': '1ImWbV0O0Cbw1HG34-DFbGwh4womhikza'})\n",
        "file_obj.GetContentFile(fname)\n",
        "with open (fname, 'rb') as fp:\n",
        "    replay_buffer =  pickle.load(fp)\n",
        "    \n",
        "states = [elem[0] for elem in replay_buffer.buffer]\n",
        "states = np.array(states)\n",
        "scaler = preprocessing.StandardScaler().fit(states)\n",
        "\n",
        "\n",
        "agent = Agent(env_name='BipedalWalkerHardcore-v2', batch_size=128, discretization_steps=6, number_of_steps=150000, scaler=scaler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "06kg5xeMLvj6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "agent.run()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}