{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Agent for Bipedal.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "hvYHyNAH_Ivr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "65343ca6-9245-4a5e-d571-a1fd6ea2826a"
      },
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow-gpu==2.0.0-alpha0\n",
        "!pip install gym\n",
        "!pip install gym[box2d]\n",
        "!pip install -U -q PyDrive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K    100% |████████████████████████████████| 332.1MB 50kB/s \n",
            "\u001b[K    100% |████████████████████████████████| 3.0MB 8.7MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 419kB 13.6MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 61kB 31.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.10.11)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.16.2)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.11.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2019.3.9)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n",
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.6/dist-packages (0.10.11)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.16.2)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (2.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.11.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.3.2)\n",
            "Collecting box2d-py>=2.3.5 (from gym[box2d])\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/bd/6cdc3fd994b0649dcf5d9bad85bd9e26172308bbe9a421bfc6fdbf5081a6/box2d_py-2.3.8-cp36-cp36m-manylinux1_x86_64.whl (448kB)\n",
            "\u001b[K    100% |████████████████████████████████| 450kB 9.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[box2d]) (2019.3.9)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[box2d]) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[box2d]) (2.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[box2d]) (1.22)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym[box2d]) (0.16.0)\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n",
            "\u001b[K    100% |████████████████████████████████| 993kB 23.6MB/s \n",
            "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ullLgBKs_KB3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import gym\n",
        "import numpy as np\n",
        "import cv2 as cv\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import preprocessing\n",
        "\n",
        "import pickle\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yU6O4GysclzI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, seed):\n",
        "        self.buffer = []\n",
        "        self.random_generator = np.random.RandomState(seed=seed)\n",
        "        self.max_size = 1000000\n",
        "        self.index = -1\n",
        "\n",
        "    def append(self, cur_state, action, next_state, reward, done):\n",
        "        if done:\n",
        "            final = 1\n",
        "        else:\n",
        "            final = 0\n",
        "\n",
        "        self.index = (self.index + 1) % self.max_size\n",
        "        if self.index >= len(self.buffer):\n",
        "            self.buffer.append([cur_state, action, next_state, reward, final])\n",
        "        else:\n",
        "            self.buffer[self.index] = [cur_state, action, next_state, reward, final]\n",
        "\n",
        "    def get_size(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def get_batch(self, size):\n",
        "        mask = self.random_generator.randint(0, len(self.buffer), size)\n",
        "        \n",
        "        return [self.buffer[id] for id in mask]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6oUdR1ekcoP-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class QFunction:\n",
        "    def __init__(self, num_of_inputs, num_of_actions, image_size=None, layer_units_inputs=[1500, 1000], lr=0.0001):\n",
        "        \n",
        "        \n",
        "        layer_units = layer_units_inputs\n",
        "             \n",
        "        inputs = tf.keras.Input(shape=(num_of_inputs,))\n",
        "        x = tf.keras.layers.Dense(units=layer_units[0])(inputs)\n",
        "        x = tf.keras.layers.PReLU()(x)\n",
        "        for elem in range(1, len(layer_units)):\n",
        "            x = tf.keras.layers.Dense(units=layer_units[elem])(x)\n",
        "            x = tf.keras.layers.PReLU()(x)\n",
        "        outputs = tf.keras.layers.Dense(units=num_of_actions)(x)\n",
        "\n",
        "        self.Q_function = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "        self.Q_function.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(lr))\n",
        "\n",
        "    def predict(self, state):\n",
        "        return self.Q_function.predict(state)\n",
        "\n",
        "    def train_step(self, cur_states, targets):\n",
        "        loss = self.Q_function.train_on_batch(cur_states, targets)\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "olbuGIGW_ObE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    \n",
        "    def __init__(self, env_name='BipedalWalkerHardcore-v2', number_of_steps=10000000, discretization_steps=5,\n",
        "                 batch_size=32, seed=42, episodes_to_average=40, learning_rate=0.0001, scaler = None, learning_steps=0):\n",
        "\n",
        "        # environment\n",
        "        self.env_name = env_name\n",
        "        self.env = gym.make(env_name)\n",
        "        \n",
        "        # state normalisation - doesn't work here - params unbounded\n",
        "        self.state_scaler = None\n",
        "        #self.get_states_stats()\n",
        "        \n",
        "        # learning params\n",
        "        self.number_of_steps = number_of_steps\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "\n",
        "        # random generator seed\n",
        "        self.seed = seed\n",
        "\n",
        "        # ReplayBuffer for observations\n",
        "        self.replay_buffer = ReplayBuffer(self.seed)\n",
        "        \n",
        "        # discretization\n",
        "        self.discretization_steps = discretization_steps\n",
        "        self.low = self.env.action_space.low\n",
        "        self.action_steps = (self.env.action_space.high - self.env.action_space.low) / discretization_steps\n",
        "        self.num_of_actions = len(self.low)\n",
        "        \n",
        "        self.clip_action = 0.7\n",
        "\n",
        "        # Q function\n",
        "        \n",
        "        self.num_of_inputs = len(self.env.observation_space.high)\n",
        "        self.num_of_outputs = discretization_steps ** self.num_of_actions \n",
        "        if self.discretization_steps % 2 == 0:\n",
        "            self.num_of_outputs += 1\n",
        "        self.Q_function = QFunction(self.num_of_inputs, self.num_of_outputs, lr=learning_rate, layer_units_inputs=[1500, 1300])\n",
        "        self.gamma = 0.99\n",
        "        \n",
        "        self.learning_steps = learning_steps\n",
        "        \n",
        "        # epsilon greedy policy\n",
        "        self.epsilon_start = 0.95\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon = self.epsilon_start\n",
        "        self.epsilon_decay_factor = 0.99991 # decay ~e times in 10000 steps\n",
        "\n",
        "        # training\n",
        "        self.losses = []\n",
        "        self.episodes_rewards = []\n",
        "        self.episodes_end_index = []\n",
        "        self.reward_before_fail = []\n",
        "        self.average_reward_before_fail = []\n",
        "        self.episodes_to_average = episodes_to_average\n",
        "        self.eps = []\n",
        "        \n",
        "        # file ids\n",
        "        self.buffer_file_id = '14ThpPSMN-xL3zp12Vqd-ksVT_qTI-yXf'\n",
        "        self.model_file_id = '15trlpnUvX-N4EYqL4H0P2H_q4aAF9Gdo'\n",
        "        \n",
        "        # scaler\n",
        "        self.scaler = scaler\n",
        "        \n",
        "        \n",
        "    def prepropcess_state(self, state):\n",
        "        if self.scaler:\n",
        "            return self.scaler.transform(state.reshape(1, -1))[0]\n",
        "        else:\n",
        "            return state\n",
        "        \n",
        "    # convert number of discreet action to continuous space.\n",
        "    def num_to_action(self, action_num):\n",
        "        assert self.num_of_outputs > action_num\n",
        "        \n",
        "        cur_actions = np.zeros(self.num_of_actions)\n",
        "        \n",
        "        if self.discretization_steps % 2 == 0 and action_num == self.num_of_outputs - 1:\n",
        "            return cur_actions\n",
        "        \n",
        "        i = 0\n",
        "        while action_num > 0:\n",
        "            cur_actions[i] += self.action_steps[i] * (action_num % self.discretization_steps )\n",
        "            action_num = action_num // self.discretization_steps\n",
        "            i += 1\n",
        "        for i in range(self.num_of_actions):\n",
        "            cur_actions[i] += self.low[i] + self.action_steps[i] * 0.5\n",
        "            cur_actions[i] *= self.clip_action\n",
        "        return cur_actions\n",
        "\n",
        "    # calculate targets\n",
        "    def targets(self, batch): \n",
        "        cur_state, action, next_state, reward, done = zip(*batch)\n",
        "        cur_state = np.array(cur_state)\n",
        "        action = np.array(action)\n",
        "        next_state = np.array(next_state)\n",
        "        reward = np.array(reward)\n",
        "        done = np.array(done)\n",
        "\n",
        "        \n",
        "        target_val = self.Q_function.predict(cur_state)\n",
        "        target = np.max(self.Q_function.predict(next_state), axis=1) * (1 - done) + reward\n",
        "        for i in range(len(target_val)): \n",
        "            target_val[i][action[i]] = target[i]\n",
        "        return cur_state, target_val\n",
        "\n",
        "    def update_epsilon(self):\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay_factor)\n",
        "\n",
        "    # epsilon greedy action\n",
        "    def sample_action(self, state, epsilon):\n",
        "        random_num = np.random.random()\n",
        "        if random_num < epsilon:\n",
        "            action_num = np.random.randint(0, self.num_of_outputs)\n",
        "        else:\n",
        "            q_values = self.Q_function.predict([state])\n",
        "            action_num = np.argmax(q_values)\n",
        "        return action_num, self.num_to_action(action_num)\n",
        "\n",
        "    def plot_data(self, episode_rewards_per_plot=700, losses_per_plot=30000, plot_eps=True):\n",
        "        clear_output(True)\n",
        "        plt.figure(figsize=(20,5))\n",
        "        \n",
        "        plt.subplot(131)\n",
        "        st = self.learning_steps\n",
        "        plt.title('Rewards after {} steps'.format(st))\n",
        "        n = min(len(self.episodes_rewards), episode_rewards_per_plot)\n",
        "        episodes_index = np.arange(len(self.episodes_rewards)-n, len(self.episodes_rewards))\n",
        "        \n",
        "        plt.plot(episodes_index, self.episodes_rewards[-n:])\n",
        "        plt.plot(episodes_index, np.zeros(n))\n",
        "        plt.plot(episodes_index, self.average_reward_before_fail[-n:])\n",
        "        \n",
        "        plt.subplot(132)\n",
        "        plt.title('Loss')\n",
        "        n = min(st, losses_per_plot)\n",
        "        plt.plot(np.arange(st-n, st), self.losses[-n:])\n",
        "        \n",
        "        if plot_eps:\n",
        "            plt.subplot(133)\n",
        "            plt.title('eps')\n",
        "            plt.plot(self.eps)\n",
        "        plt.show()\n",
        "    \n",
        "    def read_colab(self, buffer_id = None, model_id = None):\n",
        "        auth.authenticate_user()\n",
        "        gauth = GoogleAuth()\n",
        "        gauth.credentials = GoogleCredentials.get_application_default()\n",
        "        drive = GoogleDrive(gauth)\n",
        "        \n",
        "        if buffer_id is not None:\n",
        "            fname = \"buffer.txt\"\n",
        "            file_obj = drive.CreateFile({'id': buffer_id})\n",
        "            file_obj.GetContentFile(fname)\n",
        "            with open (fname, 'rb') as fp:\n",
        "                self.replay_buffer =  pickle.load(fp)\n",
        "\n",
        "        if model_id is not None:\n",
        "            fname = 'model.h5'\n",
        "            file_obj = drive.CreateFile({'id': model_id})\n",
        "            file_obj.GetContentFile(fname)\n",
        "            self.Q_function.Q_function = tf.keras.models.load_model(fname)\n",
        "        \n",
        "        \n",
        "    def write_colab(self):\n",
        "        auth.authenticate_user()\n",
        "        gauth = GoogleAuth()\n",
        "        gauth.credentials = GoogleCredentials.get_application_default()\n",
        "        drive = GoogleDrive(gauth)\n",
        "        \n",
        "        fname = 'buffer.txt'\n",
        "        with open(fname, 'wb') as fp:\n",
        "            pickle.dump(self.replay_buffer, fp)\n",
        "\n",
        "\n",
        "        model_file = drive.CreateFile({'title' : fname})\n",
        "        model_file.SetContentFile(fname)\n",
        "        model_file.Upload()\n",
        "        \n",
        "        fname = 'model.h5'\n",
        "        self.Q_function.Q_function.save(fname)    \n",
        "        model_file = drive.CreateFile({'title' : fname})\n",
        "        model_file.SetContentFile(fname)\n",
        "        model_file.Upload()\n",
        "\n",
        "        # download to google drive\n",
        "        drive.CreateFile({'id': model_file.get('id')})\n",
        "    \n",
        "    \n",
        "    # run and learn\n",
        "    def run(self, continue_learning = False, set_epsilon_to_min = False):\n",
        "\n",
        "        done = False\n",
        "        cur_state = self.env.reset()\n",
        "        cur_state = self.prepropcess_state(cur_state)\n",
        "        \n",
        "        if set_epsilon_to_min:\n",
        "            self.epsilon = self.epsilon_min\n",
        "        \n",
        "        if not continue_learning:\n",
        "            self.losses = []\n",
        "            self.episodes_rewards = []\n",
        "            self.episodes_end_index = []\n",
        "            self.average_reward_before_fail = []\n",
        "            self.before_fail = []\n",
        "            self.epsilon = self.epsilon_start\n",
        "            self.eps = []\n",
        "            self.Q_function = QFunction(self.num_of_inputs, self.num_of_outputs)\n",
        "            \n",
        "        cur_episode_reward = 0\n",
        "        \n",
        "        for step_num in range(1, self.number_of_steps + 1):\n",
        "\n",
        "            self.update_epsilon() # epsilon decay\n",
        "            self.eps.append(self.epsilon)\n",
        "            \n",
        "            action_num, action = self.sample_action(tf.expand_dims(cur_state, 0), self.epsilon)\n",
        "            \n",
        "            next_state, reward, done, _ = self.env.step(action)            \n",
        "            next_state = self.prepropcess_state(next_state)\n",
        "            cur_episode_reward += reward\n",
        "\n",
        "            self.replay_buffer.append(cur_state, action_num, next_state, reward, done)\n",
        "            cur_state = next_state\n",
        "            \n",
        "            if self.replay_buffer.get_size() >= self.batch_size:\n",
        "                batch = self.replay_buffer.get_batch(self.batch_size)\n",
        "                states, target_qs = self.targets(batch)\n",
        "                \n",
        "                loss = self.Q_function.train_step(states, target_qs)\n",
        "                self.learning_steps += 1\n",
        "                self.losses.append(loss)\n",
        "            \n",
        "            if done:\n",
        "                self.episodes_rewards.append(cur_episode_reward + 100) # to clear indentify\n",
        "                self.episodes_end_index.append(len(self.losses) - 1)\n",
        "                self.average_reward_before_fail.append(np.mean(self.episodes_rewards[-self.episodes_to_average:]))\n",
        "                cur_episode_reward = 0\n",
        "                cur_state = self.env.reset()\n",
        "                cur_state = self.prepropcess_state(cur_state)\n",
        "                \n",
        "                done = False\n",
        "                \n",
        "            if self.learning_steps % 1000 == 0 and self.learning_steps > 0:\n",
        "                self.plot_data()\n",
        "\n",
        "        self.env.close()\n",
        "       \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lQ80mwV6ZVkP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "        \n",
        "fname = \"buffer.txt\"\n",
        "file_obj = drive.CreateFile({'id': '14BR00UGXlwI7481G8jG6wsssihKYAy0t'})\n",
        "file_obj.GetContentFile(fname)\n",
        "with open (fname, 'rb') as fp:\n",
        "    replay_buffer =  pickle.load(fp)\n",
        "    \n",
        "states = [elem[0] for elem in replay_buffer.buffer]\n",
        "states = np.array(states)\n",
        "scaler = preprocessing.StandardScaler().fit(states)\n",
        "print(scaler.mean_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "har9TwM2cbf4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "agent = Agent(env_name='BipedalWalkerHardcore-v2', batch_size=128, discretization_steps=6, learning_rate=0.0001, number_of_steps=150000, scaler=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fkgu7wrIUb0F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "c0f19874-b68b-4b61-e64d-8a38b381dee5"
      },
      "cell_type": "code",
      "source": [
        "agent.read_colab( buffer_id = '1VJGHPpRi6FFHyS1-1-CEr4JAL_-Otz-X', model_id='1a0rQOC6fbO8r4rH6vc7QeIRgI5U44u1i')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0418 08:45:28.204307 140535876597632 __init__.py:44] file_cache is unavailable when using oauth2client >= 4.0.0\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 36, in autodetect\n",
            "    from google.appengine.api import memcache\n",
            "ModuleNotFoundError: No module named 'google.appengine'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 33, in <module>\n",
            "    from oauth2client.contrib.locked_file import LockedFile\n",
            "ModuleNotFoundError: No module named 'oauth2client.contrib.locked_file'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 37, in <module>\n",
            "    from oauth2client.locked_file import LockedFile\n",
            "ModuleNotFoundError: No module named 'oauth2client.locked_file'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 41, in autodetect\n",
            "    from . import file_cache\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 41, in <module>\n",
            "    'file_cache is unavailable when using oauth2client >= 4.0.0')\n",
            "ImportError: file_cache is unavailable when using oauth2client >= 4.0.0\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "FAeMzm5yBWKy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "agent.run()\n",
        "agent.write_colab()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l7l2pPPHBW0Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "agent.run(continue_learning = True, set_epsilon_to_min = True)\n",
        "agent.write_colab()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-kALIt38BX6Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "agent.run(continue_learning = True, set_epsilon_to_min = True)\n",
        "agent.write_colab()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jvm-cZfzBZnK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "agent.run(continue_learning = True, set_epsilon_to_min = True)\n",
        "agent.write_colab()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uYJpT-m7BYdS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "agent.run(continue_learning = True, set_epsilon_to_min = True)\n",
        "agent.write_colab()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AICoNG7M4rr0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "agent.run(continue_learning = True, set_epsilon_to_min = True)\n",
        "agent.write_colab()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}