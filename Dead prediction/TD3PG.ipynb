{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TD3PG.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "61VeKABEm9_O",
        "geRE4MK65Wnb",
        "UH9aTMAwIeTf",
        "YnZxLGmrlAyw"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akterskii/RL/blob/master/Dead%20prediction/TD3PG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4Ovhw-h0cfa",
        "colab_type": "code",
        "outputId": "90236b1c-759f-49db-99f7-5173cd54e016",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "source": [
        "!pip install gym['box2d']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.6/dist-packages (0.10.11)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (2.21.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.4.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.16.5)\n",
            "Collecting box2d-py>=2.3.5; extra == \"box2d\" (from gym[box2d])\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/bd/6cdc3fd994b0649dcf5d9bad85bd9e26172308bbe9a421bfc6fdbf5081a6/box2d_py-2.3.8-cp36-cp36m-manylinux1_x86_64.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 6.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[box2d]) (2019.6.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[box2d]) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[box2d]) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[box2d]) (2.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym[box2d]) (0.16.0)\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GuuZ4NNdAIa",
        "colab_type": "code",
        "outputId": "0b51c610-410f-4059-f27d-c51f27cb0273",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "# Run this cell to mount your Google Drive.\n",
        "from google.colab import drive\n",
        "mount = '/content/drive'\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e81Vm0EmsKyN",
        "colab_type": "text"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAAepgnysM01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os \n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "#for RAdam\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "\n",
        "# for custom env\n",
        "from gym.wrappers import TimeLimit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXdG4uSSr9Kx",
        "colab_type": "text"
      },
      "source": [
        "## RAdam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TuAFDpGr8aX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RAdam(Optimizer):\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
        "        self.buffer = [[None, None, None] for ind in range(10)]\n",
        "        super(RAdam, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(RAdam, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data.float()\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
        "\n",
        "                p_data_fp32 = p.data.float()\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
        "                else:\n",
        "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "\n",
        "                state['step'] += 1\n",
        "                buffered = self.buffer[int(state['step'] % 10)]\n",
        "                if state['step'] == buffered[0]:\n",
        "                    N_sma, step_size = buffered[1], buffered[2]\n",
        "                else:\n",
        "                    buffered[0] = state['step']\n",
        "                    beta2_t = beta2 ** state['step']\n",
        "                    N_sma_max = 2 / (1 - beta2) - 1\n",
        "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
        "                    buffered[1] = N_sma\n",
        "                    if N_sma > 5:\n",
        "                        step_size = group['lr'] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
        "                    else:\n",
        "                        step_size = group['lr'] / (1 - beta1 ** state['step'])\n",
        "                    buffered[2] = step_size\n",
        "\n",
        "                if group['weight_decay'] != 0:\n",
        "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
        "\n",
        "                if N_sma > 5:                    \n",
        "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n",
        "                else:\n",
        "                    p_data_fp32.add_(-step_size, exp_avg)\n",
        "\n",
        "                p.data.copy_(p_data_fp32)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu-XBmsVIJ-p",
        "colab_type": "text"
      },
      "source": [
        "## Actor and Critic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92njCCoUIMxr",
        "colab_type": "code",
        "outputId": "eb4778a1-bd69-448e-f018-c5ff49001435",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, max_action, name):\n",
        "        super(Actor, self).__init__()\n",
        "        \n",
        "        self.l1 = nn.Linear(state_dim, 400)\n",
        "        self.l2 = nn.Linear(400, 300)\n",
        "        self.l3 = nn.Linear(300, action_dim)\n",
        "        \n",
        "        self.max_action = max_action\n",
        "        \n",
        "        self.fname = name + '.pth'\n",
        "        \n",
        "    def forward(self, state):\n",
        "        a = F.relu(self.l1(state))\n",
        "        a = F.relu(self.l2(a))\n",
        "        a = torch.tanh(self.l3(a)) * self.max_action\n",
        "        return a\n",
        "    \n",
        "    \n",
        "        \n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, name, probability=False):\n",
        "        super(Critic, self).__init__()\n",
        "        \n",
        "        self.l1 = nn.Linear(state_dim + action_dim, 400)\n",
        "        self.l2 = nn.Linear(400, 300)\n",
        "        self.l3 = nn.Linear(300, 1)\n",
        "        \n",
        "        self.probability = probability\n",
        "        \n",
        "        self.name = name\n",
        "        self.fname = name + '.pth'\n",
        "        \n",
        "    def forward(self, state, action):\n",
        "        state_action = torch.cat([state, action], 1)\n",
        "        \n",
        "        q = F.relu(self.l1(state_action))\n",
        "        q = F.relu(self.l2(q))\n",
        "        q = self.l3(q)\n",
        "        if self.probability:\n",
        "            q = torch.sigmoid(q)\n",
        "        return q\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHv86zi1IR8G",
        "colab_type": "text"
      },
      "source": [
        "## Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39B6n_Mq0kfK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TD3:\n",
        "    def __init__(self, lr, state_dim, action_dim, max_action, danger, danger_threshold, directory, fname, epochs_for_danger):\n",
        "        opt_RAdam = True\n",
        "        \n",
        "        self.actor = Actor(state_dim, action_dim, max_action, 'actor').to(device)\n",
        "        self.actor_target = Actor(state_dim, action_dim, max_action, 'actor_target').to(device)\n",
        "        self.actor_perturbed = Actor(state_dim, action_dim, max_action, 'actor_target').to(device)\n",
        "        \n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "        \n",
        "        \n",
        "        if opt_RAdam:\n",
        "            self.actor_optimizer = RAdam(self.actor.parameters(), lr=lr)\n",
        "        else:\n",
        "            self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n",
        "        \n",
        "        \n",
        "        self.critic_1 = Critic(state_dim, action_dim, 'critic_1').to(device)\n",
        "        self.critic_1_target = Critic(state_dim, action_dim, 'critic_1_target').to(device)\n",
        "        self.critic_1_target.load_state_dict(self.critic_1.state_dict())\n",
        "        if opt_RAdam:\n",
        "            self.critic_1_optimizer = RAdam(self.critic_1.parameters(), lr=lr)\n",
        "        else:\n",
        "            self.critic_1_optimizer = optim.Adam(self.critic_1.parameters(), lr=lr)\n",
        "        \n",
        "        self.critic_2 = Critic(state_dim, action_dim, 'critic_2').to(device)\n",
        "        self.critic_2_target = Critic(state_dim, action_dim, 'critic_2_target').to(device)\n",
        "        self.critic_2_target.load_state_dict(self.critic_2.state_dict())\n",
        "        if opt_RAdam:\n",
        "            self.critic_2_optimizer = RAdam(self.critic_2.parameters(), lr=lr)\n",
        "        else:\n",
        "            self.critic_2_optimizer = optim.Adam(self.critic_2.parameters(), lr=lr)\n",
        "        \n",
        "        self.max_action = max_action\n",
        "        \n",
        "        #danger\n",
        "        self.danger = danger\n",
        "        self.actor_danger = Actor(state_dim, action_dim, max_action, 'actor_danger').to(device)\n",
        "        if opt_RAdam:\n",
        "            self.actor_danger_optimizer = RAdam(self.actor_danger.parameters(), lr=lr)\n",
        "        else:\n",
        "            self.actor_danger_optimizer = optim.Adam(self.actor_danger.parameters(), lr=lr)\n",
        "        \n",
        "        self.critic_danger = Critic(state_dim, action_dim, 'critic_danger', probability=True).to(device)\n",
        "        if opt_RAdam:\n",
        "            self.critic_danger_optimizer = RAdam(self.critic_danger.parameters(), lr=lr)\n",
        "        else:       \n",
        "            self.critic_danger_optimizer = optim.Adam(self.critic_danger.parameters(), lr=lr)\n",
        "        \n",
        "        self.threshold = danger_threshold\n",
        "        self.directory = directory\n",
        "        self.fname = fname\n",
        "        self.epochs_for_danger = epochs_for_danger\n",
        "        self.action_update = False\n",
        "          \n",
        "    \n",
        "    def select_action(self, state, danger=False, param_noise=False, debug=False):\n",
        "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)        \n",
        "        \n",
        "        if param_noise:\n",
        "            self.actor_perturbed.eval()\n",
        "            action = self.actor_perturbed(state)\n",
        "            self.actor_perturbed.train()\n",
        "        else:\n",
        "            self.actor.eval()\n",
        "            action = self.actor(state)\n",
        "            self.actor.train()\n",
        "            \n",
        "        if danger:\n",
        "            self.action_update = False\n",
        "            \n",
        "            # estimate probability of death\n",
        "            init_prob_danger = self.critic_danger(state, action).cpu().data.numpy().flatten()\n",
        "            if  init_prob_danger > self.threshold:\n",
        "                \n",
        "                # safe action in danger case\n",
        "                self.actor_danger.eval() # set evaluation mode\n",
        "                action = self.actor_danger(state)\n",
        "                self.actor_danger.train() # set vack train mode\n",
        "                \n",
        "                self.action_update = True\n",
        "                \n",
        "                if debug:\n",
        "                    new_prob_danger = self.critic_danger(state, action).cpu().data.numpy().flatten()\n",
        "                    print( \"\\t\\t\\tP before: {}. P after: {}\".format(init_prob_danger, new_prob_danger))\n",
        "        \n",
        "        return action.cpu().data.numpy().flatten()\n",
        "    \n",
        "    \n",
        "    def perturb_actor_parameters(self, param_noise):\n",
        "        \"\"\"Apply parameter noise to actor model, for exploration\"\"\"\n",
        "        hard_update(self.actor_perturbed, self.actor)\n",
        "        params = self.actor_perturbed.state_dict()\n",
        "        for name in params:\n",
        "            if 'ln' in name: \n",
        "                pass \n",
        "            param = params[name]\n",
        "            random = torch.randn(param.shape).to(device)\n",
        "            \n",
        "            param += random * param_noise.current_stddev\n",
        "        \n",
        "    \n",
        "    def update(self, replay_buffer, replay_buffer_danger, n_iter, batch_size, batch_size_danger, gamma, polyak, policy_noise, noise_clip, policy_delay):\n",
        "        \n",
        "        for i in range(n_iter):\n",
        "            # Sample a batch of transitions from replay buffer:\n",
        "            state, action_, reward, next_state, done = replay_buffer.sample(batch_size)            \n",
        "            state = torch.FloatTensor(state).to(device)\n",
        "            action = torch.FloatTensor(action_).to(device)\n",
        "            reward = torch.FloatTensor(reward).reshape((batch_size,1)).to(device)\n",
        "            next_state = torch.FloatTensor(next_state).to(device)\n",
        "            done = torch.FloatTensor(done).reshape((batch_size,1)).to(device)\n",
        "                                    \n",
        "            # Select next action according to target policy:\n",
        "            noise = torch.FloatTensor(action_).data.normal_(0, policy_noise).to(device)\n",
        "            noise = noise.clamp(-noise_clip, noise_clip)\n",
        "            next_action = (self.actor_target(next_state) + noise)\n",
        "            next_action = next_action.clamp(-self.max_action, self.max_action)\n",
        "            \n",
        "            # Compute target Q-value:\n",
        "            target_Q1 = self.critic_1_target(next_state, next_action)\n",
        "            target_Q2 = self.critic_2_target(next_state, next_action)\n",
        "            target_Q = torch.min(target_Q1, target_Q2)\n",
        "            target_Q = reward + ((1-done) * gamma * target_Q).detach()\n",
        "            \n",
        "                                    \n",
        "            # Optimize Critic 1:\n",
        "            current_Q1 = self.critic_1(state, action)\n",
        "            loss_Q1 = F.mse_loss(current_Q1, target_Q)\n",
        "            self.critic_1_optimizer.zero_grad()\n",
        "            loss_Q1.backward()\n",
        "            self.critic_1_optimizer.step()\n",
        "            \n",
        "            # Optimize Critic 2:\n",
        "            current_Q2 = self.critic_2(state, action)\n",
        "            loss_Q2 = F.mse_loss(current_Q2, target_Q)\n",
        "            self.critic_2_optimizer.zero_grad()\n",
        "            loss_Q2.backward()\n",
        "            self.critic_2_optimizer.step()\n",
        "            \n",
        "            \n",
        "            # Delayed policy updates:\n",
        "            if i % policy_delay == 0:\n",
        "                # Compute actor loss:\n",
        "                actor_loss = -self.critic_1(state, self.actor(state)).mean()\n",
        "                \n",
        "                # Optimize the actor\n",
        "                self.actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                self.actor_optimizer.step()                \n",
        "                \n",
        "                # Polyak averaging update:\n",
        "                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "                    target_param.data.copy_( (polyak * target_param.data) + ((1-polyak) * param.data))\n",
        "                \n",
        "                for param, target_param in zip(self.critic_1.parameters(), self.critic_1_target.parameters()):\n",
        "                    target_param.data.copy_( (polyak * target_param.data) + ((1-polyak) * param.data))\n",
        "                \n",
        "                for param, target_param in zip(self.critic_2.parameters(), self.critic_2_target.parameters()):\n",
        "                    target_param.data.copy_( (polyak * target_param.data) + ((1-polyak) * param.data))\n",
        "                  \n",
        "                  \n",
        "        \n",
        "        \n",
        "        if self.danger and len(replay_buffer_danger.buffer) > 0:\n",
        "            batch_steps = max(1, replay_buffer_danger.size // batch_size_danger)\n",
        "            for _ in range(self.epochs_for_danger):\n",
        "                for j in range(batch_steps):\n",
        "                    # Sample two batches of transitions: deadend and normals\n",
        "                    state_not_danger, action_not_danger, _, _, done_not_danger = replay_buffer.sample(batch_size_danger)            \n",
        "                    state_not_danger = torch.FloatTensor(state_not_danger).to(device)\n",
        "                    action_not_danger = torch.FloatTensor(action_not_danger).to(device)\n",
        "                    done_not_danger = torch.FloatTensor(done_not_danger).reshape((batch_size_danger, 1)).to(device)\n",
        "            \n",
        "                    state_danger, action_danger, _, _, done_danger = replay_buffer_danger.sample(batch_size_danger)            \n",
        "                    state_danger = torch.FloatTensor(state_danger).to(device)\n",
        "                    action_danger = torch.FloatTensor(action_danger).to(device)\n",
        "                    done_danger = torch.FloatTensor(done_danger).reshape((batch_size_danger, 1)).to(device)\n",
        "                  \n",
        "                    # Compute danger probabilities\n",
        "                    target_Q_not_danger = done_not_danger\n",
        "                    target_Q_danger = done_danger\n",
        "                    #pprint(\"dan_q\", target_Q_not_danger, target_Q_danger)\n",
        "                  \n",
        "                    # Optimize Critic Danger:\n",
        "                    current_Q_danger = self.critic_danger(state_danger, action_danger)\n",
        "                    current_Q_not_danger = self.critic_danger(state_not_danger, action_not_danger)\n",
        "                    loss_Q_danger = F.mse_loss(current_Q_danger, target_Q_danger)\n",
        "                    loss_Q_not_danger = F.mse_loss(current_Q_not_danger, target_Q_not_danger)\n",
        "                    loss_QD =(loss_Q_danger + loss_Q_not_danger)/2\n",
        "                    self.critic_danger_optimizer.zero_grad()\n",
        "                    loss_QD.backward()\n",
        "                    self.critic_danger_optimizer.step()\n",
        "                    \n",
        "                    if j % policy_delay == 0:\n",
        "                        actor_danger_loss = self.critic_danger(state_danger, self.actor_danger(state_danger)).mean()\n",
        "                    \n",
        "                        # Optimize the actor for danger\n",
        "                        self.actor_danger_optimizer.zero_grad()\n",
        "                        actor_danger_loss.backward()\n",
        "                        self.actor_danger_optimizer.step()           \n",
        "                     \n",
        "                \n",
        "    def save(self, directory=None, fname=None, optimizers = False, danger = False):\n",
        "        if directory is None:\n",
        "            directory = self.directory\n",
        "        if fname is None:\n",
        "            fname = self.fname\n",
        "            \n",
        "        base_path = \"%s/%s_\"% (directory, fname)\n",
        "        \n",
        "        torch.save(self.actor.state_dict(), base_path + self.actor.fname)\n",
        "        torch.save(self.actor_target.state_dict(), base_path + self.actor_target.fname)\n",
        "        \n",
        "        torch.save(self.critic_1.state_dict(), base_path + self.critic_1.fname)\n",
        "        torch.save(self.critic_1_target.state_dict(), base_path + self.critic_1_target.fname)\n",
        "        \n",
        "        torch.save(self.critic_2.state_dict(), base_path + self.critic_2.fname)\n",
        "        torch.save(self.critic_2_target.state_dict(), base_path + self.critic_2_target.fname)\n",
        "        \n",
        "        if danger:\n",
        "            torch.save(self.actor_danger.state_dict(),  base_path + self.actor_danger.fname)\n",
        "            torch.save(self.critic_danger.state_dict(), base_path + self.critic_danger.fname)\n",
        "        \n",
        "        if optimizers:\n",
        "            torch.save(self.actor_optimizer.state_dict(), '%s/%s_actor_optimizer.pth' % (directory, fname))\n",
        "            torch.save(self.critic_1_optimizer.state_dict(), '%s/%s_critic_1_optimizer.pth' % (directory, fname))\n",
        "            torch.save(self.critic_2_optimizer.state_dict(), '%s/%s_critic_2_optimizer.pth' % (directory, fname))\n",
        "            if danger:\n",
        "                torch.save(self.actor_danger_optimizer.state_dict(), '%s/%s_actor_danger_optimizer.pth' % (directory, fname))\n",
        "                torch.save(self.critic_danger_optimizer.state_dict(), '%s/%s_critic_danger_optimizer.pth' % (directory, fname))\n",
        "                \n",
        "        \n",
        "    def load(self, directory=None, fname=None, optimizers=False, danger = False):\n",
        "        if directory is None:\n",
        "            directory = self.directory\n",
        "        if fname is None:\n",
        "            fname = self.fname\n",
        "            \n",
        "        base_path = \"%s/%s_\"% (directory, fname)\n",
        "        \n",
        "        self.actor.load_state_dict(torch.load(base_path + self.actor.fname, map_location=lambda storage, loc: storage))\n",
        "        self.actor_target.load_state_dict(torch.load(base_path + self.actor_target.fname, map_location=lambda storage, loc: storage))\n",
        "        \n",
        "        self.critic_1.load_state_dict(torch.load(base_path + self.critic_1.fname, map_location=lambda storage, loc: storage))\n",
        "        self.critic_1_target.load_state_dict(torch.load(base_path + self.critic_1_target.fname, map_location=lambda storage, loc: storage))\n",
        "        \n",
        "        self.critic_2.load_state_dict(torch.load(base_path + self.critic_2.fname, map_location=lambda storage, loc: storage))\n",
        "        self.critic_2_target.load_state_dict(torch.load(base_path + self.critic_2_target.fname, map_location=lambda storage, loc: storage))\n",
        "        \n",
        "        if danger:\n",
        "            self.actor_danger.load_state_dict(torch.load('%s/%s_actor_danger.pth' % (directory, name), map_location=lambda storage, loc: storage))\n",
        "            self.critic_danger.load_state_dict(torch.load('%s/%s_critic_danger.pth' % (directory, name), map_location=lambda storage, loc: storage))\n",
        "        \n",
        "        if optimizers:\n",
        "            self.actor_optimizer.load_state_dict(torch.load( '%s/%s_actor_optimizer.pth' % (directory, name), map_location=lambda storage, loc: storage))\n",
        "            self.critic_1_optimizer.load_state_dict(torch.load('%s/%s_critic_1_optimizer.pth' % (directory, name), map_location=lambda storage, loc: storage))\n",
        "            self.critic_2_optimizer.load_state_dict(torch.load('%s/%s_critic_2_optimizer.pth' % (directory, name), map_location=lambda storage, loc: storage))\n",
        "            if danger:\n",
        "                self.actor_danger_optimizer.load_state_dict(torch.load(base_path + self.actor_danger.fname, map_location=lambda storage, loc: storage))\n",
        "                self.critic_danger_optimizer.load_state_dict(torch.load(base_path + self.critic_danger.fname, map_location=lambda storage, loc: storage))\n",
        "        \n",
        "        \n",
        "    def load_actor(self, directory=None, fname=None, danger=False):\n",
        "        if directory is None:\n",
        "            directory = self.directory\n",
        "        if fname is None:\n",
        "            fname = self.fname      \n",
        "      \n",
        "        base_path = \"%s/%s_\"% (directory, fname)\n",
        "        self.actor.load_state_dict(torch.load(base_path + self.actor.fname, map_location=lambda storage, loc: storage))\n",
        "        self.actor_target.load_state_dict(torch.load(base_path + self.actor_target.fname, map_location=lambda storage, loc: storage))\n",
        "        if danger:\n",
        "            self.actor_danger.load_state_dict(torch.load(base_path + self.actor_danger.fname, map_location=lambda storage, loc: storage))\n",
        "            self.critic_danger.load_state_dict(torch.load(base_path + self.critic_danger.fname, map_location=lambda storage, loc: storage))\n",
        "        \n",
        "        \n",
        "        \n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pr_4tXGr5Qzj",
        "colab_type": "text"
      },
      "source": [
        "##Custom Bipedal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQj8Rpz3yw2P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import warnings\n",
        "from gym.envs.box2d.bipedal_walker import *\n",
        "class CustomizableBipedalWalker(BipedalWalker):\n",
        "    def __init__(self):\n",
        "        self.default_params = {\n",
        "            'stump_height_low': 1,\n",
        "            'stump_height_high': 3,\n",
        "            'pit_depth': 4,\n",
        "            'pit_width_low': 3,\n",
        "            'pit_width_high': 5,\n",
        "            'stair_heights': [-.5, .5],\n",
        "            'stair_width_low': 4,\n",
        "            'stair_width_high': 5,\n",
        "            'stair_steps_low': 3,\n",
        "            'stair_steps_high': 5,\n",
        "            'states': [0],\n",
        "            'state_probs': None\n",
        "        }\n",
        "        self.params = {**self.default_params}\n",
        "        BipedalWalker.__init__(self)\n",
        "        \n",
        "    def _update_env_params(self, **kwargs):\n",
        "        # TODO: add kind of sanity check here\n",
        "        self.params = {**self.params, **kwargs}\n",
        "        _ = self.reset()\n",
        "        \n",
        "    def reset_env_params(self, hardcore=False):\n",
        "        params = {**self.default_params}\n",
        "        if hardcore:\n",
        "            params['states'] = np.arange(4)\n",
        "        self._update_env_params(**params)\n",
        "    \n",
        "    def set_env_states(self, state_mask, p=None):\n",
        "        \"\"\"\n",
        "        :param state_mask: np.array(,dtype=bool) that masks [\"GRASS\", \"STUMP\", \"STAIRS\", \"PIT\"].\n",
        "            Note that masking out \"GRASS\" takes no effect.\n",
        "        :param p: np.array or list of probabilities: [p_grass, p_stump, p_stairs, p_pit].\n",
        "            Probs corresponding to masked out states are ignored\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        states_ = np.arange(4)[state_mask]\n",
        "        p_ = None\n",
        "        if p is not None:\n",
        "            p_ = np.array(p)\n",
        "            if not np.all(p_ >= 0):\n",
        "                raise ValueError\n",
        "            p_ = p_[state_mask] / p_[state_mask].sum()\n",
        "        self._update_env_params(states=states_, state_probs=p_)\n",
        "    \n",
        "    def set_env_params(self, pit_width=None, stair_width=None, stair_steps=None, stump_height=None):\n",
        "        \"\"\"\n",
        "            NB: All params are integers or tuples of integers\n",
        "        \"\"\"\n",
        "        kwargs = {**locals()}\n",
        "        _ = kwargs.pop('self', None)\n",
        "        params = {}\n",
        "        for k,v in kwargs.items():\n",
        "            if type(v) is int:\n",
        "                params[k + '_low'] = v\n",
        "                params[k + '_high'] = v + 1\n",
        "            elif isinstance(v, (tuple, list)): \n",
        "                if v[1] - v[0] >= 1:\n",
        "                    params[k + '_low'] = v[0]\n",
        "                    params[k + '_high'] = v[1]\n",
        "                else:\n",
        "                    warnings.warn(f'{k} shoud be an integer. {k}[1] - {k}[0] < 1 '+\\\n",
        "                                  f'=> will set {k}_low = {v[0]}, {k}_high = {v[0]+1}')\n",
        "                    params[k + '_low'] = v[0]\n",
        "                    params[k + '_high'] = v[0] + 1\n",
        "        self._update_env_params(**params)\n",
        "        \n",
        "    def _generate_terrain(self, hardcore=True):\n",
        "        GRASS, STUMP, STAIRS, PIT, _STATES_ = range(5)\n",
        "        state    = GRASS\n",
        "        velocity = 0.0\n",
        "        y        = TERRAIN_HEIGHT\n",
        "        counter  = TERRAIN_STARTPAD\n",
        "        oneshot  = False\n",
        "        self.terrain   = []\n",
        "        self.terrain_x = []\n",
        "        self.terrain_y = []\n",
        "        for i in range(TERRAIN_LENGTH):\n",
        "            x = i*TERRAIN_STEP\n",
        "            self.terrain_x.append(x)\n",
        "\n",
        "            if state==GRASS and not oneshot:\n",
        "                velocity = 0.8*velocity + 0.01*np.sign(TERRAIN_HEIGHT - y)\n",
        "                if i > TERRAIN_STARTPAD: velocity += self.np_random.uniform(-1, 1)/SCALE   #1\n",
        "                y += velocity\n",
        "\n",
        "            elif state==PIT and oneshot:\n",
        "                counter = self.np_random.randint(self.params['pit_width_low'], \n",
        "                                                 self.params['pit_width_high'])\n",
        "                PIT_H = self.params['pit_depth']\n",
        "                poly = [\n",
        "                    (x,              y),\n",
        "                    (x+TERRAIN_STEP, y),\n",
        "                    (x+TERRAIN_STEP, y-PIT_H*TERRAIN_STEP),\n",
        "                    (x,              y-PIT_H*TERRAIN_STEP),\n",
        "                    ]\n",
        "                self.fd_polygon.shape.vertices=poly\n",
        "                t = self.world.CreateStaticBody(\n",
        "                    fixtures = self.fd_polygon)\n",
        "                t.color1, t.color2 = (1,1,1), (0.6,0.6,0.6)\n",
        "                self.terrain.append(t)\n",
        "\n",
        "                self.fd_polygon.shape.vertices=[(p[0]+TERRAIN_STEP*counter,p[1]) for p in poly]\n",
        "                t = self.world.CreateStaticBody(\n",
        "                    fixtures = self.fd_polygon)\n",
        "                t.color1, t.color2 = (1,1,1), (0.6,0.6,0.6)\n",
        "                self.terrain.append(t)\n",
        "                counter += 2\n",
        "                original_y = y\n",
        "\n",
        "            elif state==PIT and not oneshot:\n",
        "                y = original_y\n",
        "                if counter > 1:\n",
        "                    y -= PIT_H*TERRAIN_STEP\n",
        "\n",
        "            elif state==STUMP and oneshot:\n",
        "                counter = self.np_random.randint(self.params['stump_height_low'], self.params['stump_height_high'])\n",
        "                poly = [\n",
        "                    (x,                      y),\n",
        "                    (x+counter*TERRAIN_STEP, y),\n",
        "                    (x+counter*TERRAIN_STEP, y+counter*TERRAIN_STEP),\n",
        "                    (x,                      y+counter*TERRAIN_STEP),\n",
        "                    ]\n",
        "                self.fd_polygon.shape.vertices=poly\n",
        "                t = self.world.CreateStaticBody(\n",
        "                    fixtures = self.fd_polygon)\n",
        "                t.color1, t.color2 = (1,1,1), (0.6,0.6,0.6)\n",
        "                self.terrain.append(t)\n",
        "\n",
        "            elif state==STAIRS and oneshot:\n",
        "                stair_height = self.np_random.choice(self.params['stair_heights'])\n",
        "                stair_width = self.np_random.randint(self.params['stair_width_low'], \n",
        "                                                     self.params['stair_width_high'])\n",
        "                stair_steps = self.np_random.randint(self.params['stair_steps_low'], \n",
        "                                                     self.params['stair_steps_high'])\n",
        "                original_y = y\n",
        "                for s in range(stair_steps):\n",
        "                    poly = [\n",
        "                        (x+(    s*stair_width)*TERRAIN_STEP, y+(   s*stair_height)*TERRAIN_STEP),\n",
        "                        (x+((1+s)*stair_width)*TERRAIN_STEP, y+(   s*stair_height)*TERRAIN_STEP),\n",
        "                        (x+((1+s)*stair_width)*TERRAIN_STEP, y+(-1+s*stair_height)*TERRAIN_STEP),\n",
        "                        (x+(    s*stair_width)*TERRAIN_STEP, y+(-1+s*stair_height)*TERRAIN_STEP),\n",
        "                        ]\n",
        "                    self.fd_polygon.shape.vertices=poly\n",
        "                    t = self.world.CreateStaticBody(\n",
        "                        fixtures = self.fd_polygon)\n",
        "                    t.color1, t.color2 = (1,1,1), (0.6,0.6,0.6)\n",
        "                    self.terrain.append(t)\n",
        "                counter = stair_steps*stair_width\n",
        "\n",
        "            elif state==STAIRS and not oneshot:\n",
        "                s = stair_steps*stair_width - counter - stair_height\n",
        "                n = s/stair_width\n",
        "                y = original_y + (n*stair_height)*TERRAIN_STEP\n",
        "\n",
        "            oneshot = False\n",
        "            self.terrain_y.append(y)\n",
        "            counter -= 1\n",
        "            if counter==0:\n",
        "                counter = self.np_random.randint(TERRAIN_GRASS/2, TERRAIN_GRASS)\n",
        "                if state==GRASS:\n",
        "                    state = self.np_random.choice(self.params['states'], p=self.params['state_probs'])\n",
        "                    oneshot = True\n",
        "                else:\n",
        "                    state = GRASS\n",
        "                    oneshot = True\n",
        "\n",
        "        self.terrain_poly = []\n",
        "        for i in range(TERRAIN_LENGTH-1):\n",
        "            poly = [\n",
        "                (self.terrain_x[i],   self.terrain_y[i]),\n",
        "                (self.terrain_x[i+1], self.terrain_y[i+1])\n",
        "                ]\n",
        "            self.fd_edge.shape.vertices=poly\n",
        "            t = self.world.CreateStaticBody(\n",
        "                fixtures = self.fd_edge)\n",
        "            color = (0.3, 1.0 if i%2==0 else 0.8, 0.3)\n",
        "            t.color1 = color\n",
        "            t.color2 = color\n",
        "            self.terrain.append(t)\n",
        "            color = (0.4, 0.6, 0.3)\n",
        "            poly += [ (poly[1][0], 0), (poly[0][0], 0) ]\n",
        "            self.terrain_poly.append( (poly, color) )\n",
        "        self.terrain.reverse()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rqNDNCWf-Z3",
        "colab_type": "text"
      },
      "source": [
        "## Noise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_QrGdK9gC32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AdaptiveParamNoiseSpec(object):\n",
        "    def __init__(self, initial_stddev=0.1, desired_action_stddev=0.2, adaptation_coefficient=1.01):\n",
        "        \"\"\"\n",
        "        Note that initial_stddev and current_stddev refer to std of parameter noise, \n",
        "        but desired_action_stddev refers to (as name notes) desired std in action space\n",
        "        \"\"\"\n",
        "        self.initial_stddev = initial_stddev\n",
        "        self.desired_action_stddev = desired_action_stddev\n",
        "        self.adaptation_coefficient = adaptation_coefficient\n",
        "\n",
        "        self.current_stddev = initial_stddev\n",
        "\n",
        "    def adapt(self, sq_distance):\n",
        "        \"\"\"\n",
        "        Expaects \n",
        "        \"\"\"\n",
        "        \n",
        "        if sq_distance > self.desired_action_stddev ** 2:\n",
        "            # Decrease stddev.\n",
        "            self.current_stddev /= self.adaptation_coefficient\n",
        "        else:\n",
        "            # Increase stddev.\n",
        "            self.current_stddev *= self.adaptation_coefficient\n",
        "\n",
        "    def get_stats(self):\n",
        "        stats = {\n",
        "            'param_noise_stddev': self.current_stddev,\n",
        "        }\n",
        "        return stats\n",
        "\n",
        "    def __repr__(self):\n",
        "        fmt = 'AdaptiveParamNoiseSpec(initial_stddev={}, desired_action_stddev={}, adaptation_coefficient={})'\n",
        "        return fmt.format(self.initial_stddev, self.desired_action_stddev, self.adaptation_coefficient)\n",
        "\n",
        "def ddpg_sq_distance_metric(actions1, actions2):\n",
        "    \"\"\"\n",
        "    Compute SQUARE of the \"distance\" between actions taken by two policies at the same states\n",
        "    Expects numpy arrays\n",
        "    \"\"\"\n",
        "    diff = actions1-actions2\n",
        "    mean_diff = np.mean(np.square(diff), axis=0)\n",
        "    sq_dist = np.mean(mean_diff)\n",
        "    return sq_dist\n",
        "\n",
        "\n",
        "def hard_update(target, source):\n",
        "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
        "           target_param.data.copy_(param.data)\n",
        "            \n",
        "            \n",
        "class Noise:\n",
        "    def __init__(self, dim):\n",
        "        self.dim = dim\n",
        "    \n",
        "        #normal\n",
        "    \n",
        "        #ou\n",
        "    \n",
        "    \n",
        "    def get_noise(self, n_type='normal'):\n",
        "        if n_type == 'normal':\n",
        "            return \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oC4_CZH8IET9",
        "colab_type": "text"
      },
      "source": [
        "## Replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkalC0GW0zcf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size=1e6, base_path=None, name=None):\n",
        "        self.buffer = []\n",
        "        self.max_size = int(max_size)\n",
        "        self.size = 0\n",
        "        \n",
        "        if base_path is not None:\n",
        "            self.base_path = base_path\n",
        "        else:\n",
        "            self.base_path = ''\n",
        "            \n",
        "        if name is not None:\n",
        "            self.fname = name + '.pth'\n",
        "        else:\n",
        "            self.fname = 'buffer.pth'\n",
        "            \n",
        "        \n",
        "    \n",
        "    def add(self, transition):\n",
        "        self.size +=1\n",
        "        # transiton is tuple of (state, action, reward, next_state, done)\n",
        "        self.buffer.append(transition)\n",
        "       \n",
        "    \n",
        "    def save(self):\n",
        "        try:\n",
        "            with open(self.basename + self.fname , 'wb') as f:\n",
        "                pickle.dump(self.buffer, f)\n",
        "        except OSError:\n",
        "            print('Buffer is not saved!\\n\\n')\n",
        "            \n",
        "            \n",
        "    def load(self):\n",
        "        try:\n",
        "            with open(self.basename + self.fname , 'rb') as f:\n",
        "                self.buffer = pickle.load(f)\n",
        "        except OSError:\n",
        "            self.buffer = []\n",
        "            print('Buffer is not loaded!\\n\\n')\n",
        "    \n",
        "          \n",
        "    def sample(self, batch_size):\n",
        "        # delete 1/5th of the buffer when full\n",
        "        if self.size > self.max_size:\n",
        "            del self.buffer[0:int(self.size/5)]\n",
        "            self.size = len(self.buffer)\n",
        "        \n",
        "        indexes = np.random.randint(0, len(self.buffer), size=batch_size)\n",
        "        state, action, reward, next_state, done = [], [], [], [], []\n",
        "        \n",
        "        for i in indexes:\n",
        "            s, a, r, s_, d = self.buffer[i]\n",
        "            state.append(np.array(s, copy=False))\n",
        "            action.append(np.array(a, copy=False))\n",
        "            reward.append(np.array(r, copy=False))\n",
        "            next_state.append(np.array(s_, copy=False))\n",
        "            done.append(np.array(d, copy=False))\n",
        "        \n",
        "        return np.array(state), np.array(action), np.array(reward), np.array(next_state), np.array(done)\n",
        "     \n",
        "    def get_last(self, amount):\n",
        "        state, action, reward, next_state, done = [], [], [], [], []\n",
        "        if amount < len(self.buffer):\n",
        "            start = len(self.buffer) - amount\n",
        "        else:\n",
        "            start = 0\n",
        "        for i in range(start,len(self.buffer)):\n",
        "            s, a, r, s_, d = self.buffer[i]\n",
        "            state.append(np.array(s, copy=False))\n",
        "            action.append(np.array(a, copy=False))\n",
        "            reward.append(np.array(r, copy=False))\n",
        "            next_state.append(np.array(s_, copy=False))\n",
        "            done.append(np.array(d, copy=False))\n",
        "        return s, a, r, s_, d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61VeKABEm9_O",
        "colab_type": "text"
      },
      "source": [
        "## Trajectory buffer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CEo4_gcm-kh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import deque\n",
        "class TrajectoriesEndBuffer():\n",
        "    def __init__(self, max_trajectory_size=10, init_prob=0.7):\n",
        "        assert type(max_trajectory_size) is int\n",
        "        assert max_trajectory_size > 0\n",
        "        \n",
        "        self.max_trajectory_size = max_trajectory_size\n",
        "        self.buffer = deque()\n",
        "        \n",
        "        #linear probability params\n",
        "        assert type(init_prob) is float\n",
        "        assert 0 <= init_prob <= 1\n",
        "        self.min_prob = init_prob\n",
        "        \n",
        "        #TRY other cases\n",
        "        \n",
        "    def add(self, transition):\n",
        "        if len(self.buffer) >= self.max_trajectory_size:\n",
        "            self.buffer.popleft()    \n",
        "        self.buffer.append(transition)\n",
        "    \n",
        "    \n",
        "    def get_heruistc_probabilities(self):\n",
        "        cur_len = len(self.buffer)\n",
        "        if cur_len > 0 and self.max_trajectory_size == 1:\n",
        "            return [1]\n",
        "\n",
        "        if cur_len < self.max_trajectory_size:\n",
        "            shift = self.max_trajectory_size - cur_len\n",
        "        else:\n",
        "            shift = 0\n",
        "        prob = [0] * cur_len\n",
        "        \n",
        "        # linear from\n",
        "        min_prob = self.min_prob\n",
        "        for i in range(cur_len):\n",
        "            prob[i] = min_prob + (1 - min_prob) * ( (i + shift) / (self.max_trajectory_size - 1) )\n",
        "            \n",
        "        return prob\n",
        "    \n",
        "    def transfer_to_replay_buffer(self, replay_buffer):\n",
        "        if len(self.buffer) == 0: \n",
        "            print('Trajectories buffer is empty')\n",
        "            return\n",
        "        \n",
        "        if self.buffer[-1][4] == False:\n",
        "            print('Trajectory is not finished')\n",
        "            return\n",
        "          \n",
        "        cur_len = len(self.buffer)\n",
        "        prob = self.get_heruistc_probabilities()\n",
        "        \n",
        "        for i in range(cur_len):\n",
        "            s1, a, r, s2, done = self.buffer[i]\n",
        "            replay_buffer.add((s1, a, r, s2, float(prob[i])))\n",
        "        \n",
        "        self.buffer = deque()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEEchoyDrpAR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzz69TYi7f2q",
        "colab_type": "text"
      },
      "source": [
        "## Research class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqj3YyUT7ebB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random \n",
        "class Research:\n",
        "    def set_danger_batch_size(self):\n",
        "        self.batch_size_danger = self.batch_size // 2\n",
        "        \n",
        "    def update_params(self, params):\n",
        "        #print(params)\n",
        "        param_names = ['random_seed', \n",
        "                       'max_episodes', 'max_timesteps', 'batch_size', \n",
        "                       'lr', 'polyak', 'policy_delay', 'gamma', 'log_interval', \n",
        "                       'batch_size_danger', \n",
        "                       'policy_noise',\n",
        "                       'danger_enable', 'danger_threshold', 'max_trajectory_size',\n",
        "                       'directory','filename_load', 'filename_save', 'epochs_for_danger',\n",
        "                       'noise_random_enable', 'exploration_noise', 'noise_clip',\n",
        "                       'max_iter_danger', 'evaluate_after_episodes', 'evaluate_average_on_episodes', 'env_name_load', \n",
        "                       'env_name',\n",
        "                       'param_noise_enable','initial_stddev','desired_action_stddev','adaptation_coefficient']\n",
        "        \n",
        "        for param_name in param_names:\n",
        "            if param_name in params:\n",
        "                \n",
        "                setattr(self, param_name, params[param_name])\n",
        "                print(\"{} set to {}\".format(param_name, params[param_name]))\n",
        "        \n",
        "        \n",
        "    def __init__(self, params):\n",
        "        ###########################################\n",
        "        ###         default values\n",
        "        ###########################################\n",
        "        \n",
        "        self.random_seed = 1\n",
        "        \n",
        "        # environment\n",
        "        self.env_rewards = {'BipedalWalkerHardcore-v2':[-100,300], 'BipedalWalker-v2':[-100,300],\n",
        "                            'LunarLanderContinuous-v2':[-100,200],'Pendulum-v0':[-1000,200]}\n",
        "\n",
        "        self.env_name_load = 'BipedalWalker-v2'\n",
        "        self.env_name = 'BipedalWalker-v2'\n",
        "        \n",
        "        if self.env_name in self.env_rewards:\n",
        "            self.terminal_reward = self.env_rewards[self.env_name][0]\n",
        "            self.passed_reward   = self.env_rewards[self.env_name][1]\n",
        "        else:\n",
        "            self.terminal_reward = None\n",
        "            self.passed_reward   = None\n",
        "        \n",
        "        self.env = None\n",
        "        \n",
        "        ## training \n",
        "        self.max_episodes = 1000         # max num of episodes\n",
        "        self.max_timesteps = 2000        # max timesteps in one episode\n",
        "        self.batch_size = 100            # num of transitions sampled from replay buffer\n",
        "        self.lr = 0.001\n",
        "        self.polyak = 0.995              # target policy update parameter (1-tau)\n",
        "        self.policy_delay = 2            # delayed policy updates parameter\n",
        "        \n",
        "        ## model params\n",
        "        self.gamma = 0.99                # discount for future rewards\n",
        "        self.warmup_steps = 10**4\n",
        "        \n",
        "        ##############################   \n",
        "        ##   Noise params\n",
        "        ##############################   \n",
        "        \n",
        "        # Random noise\n",
        "        self.noise_random_enable = True\n",
        "        self.exploration_noise = 0.1\n",
        "        self.policy_noise = 0.2          # target policy smoothing noise\n",
        "        self.noise_clip = 0.5\n",
        "        \n",
        "        # Param noise\n",
        "        self.param_noise_enable = False\n",
        "        self.noise_distance_batch_size = 10 ** 4\n",
        "        self.initial_stddev=0.05\n",
        "        self.desired_action_stddev=0.45 \n",
        "        self.adaptation_coefficient=1.03\n",
        "        \n",
        "        ## Danger params\n",
        "        self.danger_enable = True               # enable dnager mode\n",
        "        self.danger_threshold = 0.7      # probability to perform safe action\n",
        "        self.epochs_for_danger = 3       # epochs to train danger\n",
        "        self.max_iter_danger = 5         # iteration per iteration\n",
        "        self.batch_size_danger = self.batch_size // 2\n",
        "        self.max_trajectory_size = 10    # length of trajectory before death\n",
        "        self.action_updates = 0\n",
        "        \n",
        "        ## evaluation \n",
        "        self.evaluate_average_on_episodes = 10\n",
        "        self.evaluate_after_episodes = 30\n",
        "        self.evaluate_after_timesteps = 10000\n",
        "        \n",
        "        ## InpOut params\n",
        "        self.log_interval = 10                                              # print avg reward after interval\n",
        "        self.directory = \"/content/drive/My Drive\"                          # save trained models\n",
        "        self.filename_load = \"TD3_{}_{}\".format(self.env_name_load, self.random_seed)\n",
        "        self.filename_save = \"TD3_{}_{}\".format(self.env_name, self.random_seed)\n",
        "        ###########################################\n",
        "        \n",
        "        ## Buffers\n",
        "        self.replay_buffer = None\n",
        "        self.replay_buffer_dead = None\n",
        "        self.trajectory_buffer = None\n",
        "        \n",
        "        \n",
        "        ## Policy\n",
        "        self.policy = None\n",
        "        \n",
        "        ###########################################\n",
        "        ## Update default params\n",
        "        ###########################################\n",
        "        self.update_params(params)\n",
        "        \n",
        "        \n",
        "    def load_model(self, env_name, random_seed):\n",
        "        pass\n",
        "        \n",
        "        \n",
        "    def init_env(self, params = None):\n",
        "        if params:\n",
        "            self.update_env(params)\n",
        "            \n",
        "        # create env\n",
        "        if self.env_name == \"BipedalWalkerHardcore-v2\":\n",
        "            \n",
        "            env = CustomizableBipedalWalker()\n",
        "            env.set_env_params(stump_height=1)\n",
        "            env.set_env_states(state_mask=np.array([1,1,0,0],dtype=bool), p=np.array([0.1,0.9,0.9,0.9]))\n",
        "            \n",
        "            self.env = TimeLimit(env, max_episode_steps=2000)\n",
        "        else:\n",
        "            self.env = gym.make(self.env_name)\n",
        "            \n",
        "        if '_max_episode_steps' in self.env.__dict__:\n",
        "            self.max_timesteps = self.env.__dict__['_max_episode_steps']\n",
        "        else:\n",
        "            self.max_timesteps = 10**7 ## \"inifinte\" value if nothing defined\n",
        "        \n",
        "        # env params\n",
        "        self.state_dim = self.env.observation_space.shape[0]\n",
        "        self.action_dim = self.env.action_space.shape[0]\n",
        "        self.max_action = float(self.env.action_space.high[0])\n",
        "        \n",
        "        #check symmetry of actions\n",
        "        assert  np.all((-self.env.action_space.low) == self.env.action_space.high)\n",
        "        \n",
        "        \n",
        "    def init_buffers_and_agent(self, load, solved):\n",
        "        \n",
        "        # Buffers\n",
        "        self.replay_buffer = ReplayBuffer()\n",
        "        if self.danger_enable:\n",
        "            self.replay_buffer_dead = ReplayBuffer()\n",
        "            self.trajectory_buffer = TrajectoriesEndBuffer(self.max_trajectory_size)\n",
        "    \n",
        "        # Policy\n",
        "        self.policy = TD3(self.lr, self.state_dim, self.action_dim, self.max_action,  self.danger_enable,\n",
        "                          self.danger_threshold, self.directory, self.filename_save, self.epochs_for_danger)\n",
        "    \n",
        "    \n",
        "        # load\n",
        "        if load:\n",
        "            full_fname = self.filename_load\n",
        "            if solved:\n",
        "                full_fname += \"_solved\"\n",
        "            self.policy.load(self.directory,  full_fname)\n",
        "           \n",
        "        \n",
        "    def train(self, params, debug=False):\n",
        "        self.update_params(params)\n",
        "        self.first_finish_ep_num  = -1\n",
        "        assert self.policy is not None\n",
        "        assert self.max_timesteps is not None\n",
        "        \n",
        "        # Random seed\n",
        "        if self.random_seed is not None:\n",
        "            os.environ['PYTHONHASHSEED']=str(self.random_seed)\n",
        "            print(\"Random Seed: {}\".format(self.random_seed))\n",
        "            np.random.seed(self.random_seed)\n",
        "            self.env.seed(self.random_seed)\n",
        "            random.seed(self.random_seed)\n",
        "            torch.manual_seed(self.random_seed)\n",
        "         \n",
        "    \n",
        "        # logging variables:\n",
        "        avg_reward = 0\n",
        "        ep_reward = 0\n",
        "        tot_time_steps = 0\n",
        "        average_time_steps = 0\n",
        "    \n",
        "        log_f = open(\"log.txt\",\"w+\")\n",
        "    \n",
        "        # training procedure:\n",
        "        print('Start training')\n",
        "        reach_the_end = False\n",
        "        \n",
        "        param_noise = AdaptiveParamNoiseSpec(initial_stddev=0.05,desired_action_stddev=0.45, adaptation_coefficient=1.05)\n",
        "        # amount of updates in dangers\n",
        "        tot_updates_amount = 0\n",
        "        self.action_updates = 0\n",
        "        ep_updates_amount = 0\n",
        "\n",
        "        \n",
        "        \n",
        "        for episode in range(1, self.max_episodes + 1):\n",
        "            \n",
        "            ep_updates_amount = 0\n",
        "            state = self.env.reset()        \n",
        "            \n",
        "            if  self.param_noise_enable:\n",
        "                self.policy.perturb_actor_parameters(param_noise)\n",
        "            \n",
        "            for t in range(self.max_timesteps):\n",
        "                # select action and add exploration noise:\n",
        "                \n",
        "                if tot_time_steps + t <= self.warmup_steps:\n",
        "                    action = self.env.action_space.sample()\n",
        "                else:\n",
        "                    action = self.policy.select_action(state, danger=self.danger_enable, param_noise=self.param_noise_enable)\n",
        "                    if self.policy.action_update:\n",
        "                        ep_updates_amount += 1\n",
        "                    \n",
        "                    if self.noise_random_enable:\n",
        "                        action = action + np.random.normal(0, self.exploration_noise, size=self.env.action_space.shape[0])\n",
        "                    action = action.clip(self.env.action_space.low, self.env.action_space.high)\n",
        "                    \n",
        "                #print(\"action {}\".format(action))\n",
        "                # take action in env:\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "            \n",
        "                self.replay_buffer.add((state, action, reward, next_state, float(done)))\n",
        "                if self.danger_enable:\n",
        "                    self.trajectory_buffer.add((state, action, reward, next_state, float(done)))\n",
        "            \n",
        "                if reward == self.terminal_reward:\n",
        "\n",
        "                    if self.danger_enable:\n",
        "                        self.trajectory_buffer.transfer_to_replay_buffer(self.replay_buffer_dead)                    \n",
        "                    \n",
        "                        if debug:\n",
        "                            tmp_st = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
        "                            tmp_a = torch.FloatTensor(action.reshape(1, -1)).to(device)\n",
        "                            print(\"\\tDead after pair with prob: \", self.policy.critic_danger(tmp_st, tmp_a).cpu().data.numpy().flatten())\n",
        "                    \n",
        "                state = next_state\n",
        "                avg_reward += reward\n",
        "                ep_reward += reward\n",
        "                \n",
        "                #if done and t < self.max_timesteps - 1 and reward != self.terminal_reward and reach_the_end==False:\n",
        "                #    print('Reach the end. Episode: {}. r={} t={}'.format(episode, ep_reward, tot_time_steps + t))\n",
        "                #    reach_the_end = True\n",
        "                #    self.first_finish_ep_num = episode\n",
        "                #    break\n",
        "                \n",
        "                if done or t==(self.max_timesteps-1): \n",
        "                    tot_time_steps += t\n",
        "                    average_time_steps += t\n",
        "                    tot_updates_amount += ep_updates_amount \n",
        "                    self.action_updates += ep_updates_amount\n",
        "\n",
        "                    self.policy.update(self.replay_buffer, self.replay_buffer_dead, t, self.batch_size, self.batch_size_danger, self.gamma, self.polyak, self.policy_noise, self.noise_clip, self.policy_delay)\n",
        "                    if t==(self.max_timesteps - 1):                    \n",
        "                        print(\"\\tStuck, ep reward {}\".format(ep_reward))\n",
        "                    break\n",
        "            \n",
        "            # logging updates:\n",
        "            log_f.write('{},{}\\n'.format(episode, ep_reward))\n",
        "            log_f.flush()\n",
        "            ep_reward = 0\n",
        "            \n",
        "            # if avg reward > 300 then save and stop traning:\n",
        "            if (avg_reward / self.log_interval) >= 300:\n",
        "                print(\"########## Solved! ###########\")\n",
        "                name = self.filename_save + '_solved'\n",
        "                self.policy.save(self.directory, name, optimizers=True, danger=True)\n",
        "                log_f.close()\n",
        "                break\n",
        "            \n",
        "            # update perturbed actor\n",
        "            #print('start perturtbated update')\n",
        "            if self.param_noise_enable:\n",
        "                states, actions_perturbed, _, _, _ = self.replay_buffer.get_last(t)\n",
        "                actions_perturbed = np.array(actions_perturbed, copy=False)\n",
        "            \n",
        "                actions_clear = self.policy.select_action(states, danger=False, param_noise=False)\n",
        "                ddpg_sq_distance = ddpg_sq_distance_metric(actions_perturbed, actions_clear)\n",
        "                param_noise.adapt(ddpg_sq_distance)\n",
        "            #print('end perturtbated update t={}'.format(t))\n",
        "            \n",
        "            # save policy \n",
        "            if episode > 0 and episode % (5 * self.log_interval) == 0:\n",
        "                self.policy.save(self.directory, self.filename_save, optimizers=True, danger=True)\n",
        "            \n",
        "            # print avg reward every log interval:\n",
        "            if episode % self.log_interval == 0:\n",
        "                avg_reward = int(avg_reward / self.log_interval)\n",
        "                average_time_steps = int(average_time_steps / self.log_interval)\n",
        "                print(\"Episode: {}\\tAverage Reward: {}\\tAverage steps: {}\\t Total: {}\".format(episode, avg_reward, average_time_steps, tot_time_steps))\n",
        "                avg_reward = 0\n",
        "                average_time_steps = 0\n",
        "            \n",
        "            if episode % self.evaluate_after_episodes == 0 and episode >= 100:\n",
        "                reach_the_end, mean_reward = self.evaluate(params ={}, time_steps = tot_time_steps, episode = episode)\n",
        "                if reach_the_end:\n",
        "                    self.first_finish_ep_num = episode\n",
        "                    print('Reach the end')\n",
        "                    print('Updates\\tSteps\\tReward\\tEpisode\\tSeed')\n",
        "                    print('{}\\t{}\\t{}\\t{}\\t{}'.format(self.action_updates, tot_time_steps, mean_reward, episode, self.random_seed))\n",
        "                    break\n",
        "    \n",
        "    def evaluate(self, params, time_steps = None, episode = None):\n",
        "        self.update_params(params)\n",
        "        print('Evaluation. Timesteps: {} Episodes: {}'.format(time_steps, episode))\n",
        "        reach_the_end = 0\n",
        "        rewards = np.zeros(self.evaluate_average_on_episodes)\n",
        "        for ep in range(self.evaluate_average_on_episodes):\n",
        "            ep_reward = 0\n",
        "            state = self.env.reset()\n",
        "            for t in range(self.max_timesteps):\n",
        "                action = self.policy.select_action(state, debug=False, danger=True)\n",
        "                state, reward, done, _ = self.env.step(action)\n",
        "                ep_reward += reward\n",
        "                if done:\n",
        "                    s = \"\\t\\tDead {}\\tUpdate {}\".format( reward == self.terminal_reward,  self.policy.action_update)\n",
        "                    \n",
        "                    if reward != self.terminal_reward and t !=self.max_timesteps-1:\n",
        "                        print(s+'\\tFinished')\n",
        "                        reach_the_end += 1\n",
        "                    else:\n",
        "                        print(s)\n",
        "                    break\n",
        "\n",
        "            rewards[ep] = ep_reward \n",
        "            \n",
        "        print('\\t\\tMean reward: {}. All rewards: {}'.format(int(np.mean(rewards)), list(map(int,rewards)) ))\n",
        "        return reach_the_end == self.evaluate_average_on_episodes, int(np.mean(rewards))\n",
        "    \n",
        "    \n",
        "    def is_passed(self, step_num, reward):\n",
        "        if reward != self.terminal_reward:\n",
        "            if step_num < self.terminal_reward:\n",
        "                return True\n",
        "        return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC-3p-bFIcMg",
        "colab_type": "text"
      },
      "source": [
        "# Debug\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx4nESZqVJ9q",
        "colab_type": "text"
      },
      "source": [
        "## Test Research class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsFU93i6VOFo",
        "colab_type": "code",
        "outputId": "d34199d5-eb29-4ff1-bc65-3f33c77efc31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "params = {'env_name': \"LunarLanderContinuous-v2\",\n",
        "          'random_seed':19152527, \n",
        "          'danger_enable':True,\n",
        "          'max_trajectory_size' : 10,\n",
        "          'evaluate_after_episodes' : 5,\n",
        "          'evaluate_average_on_episodes' : 5,\n",
        "          #'max_timesteps':1000,\n",
        "          'exploration_noise':0.3,\n",
        "          'max_episodes' : 1000,}\n",
        "new_research = Research(params)\n",
        "\n",
        "new_research.init_env()\n",
        "new_research.init_buffers_and_agent(load=False, solved=False)\n",
        "# prime = { 14245223, 21192173, 33535573, 285781, 332447659, 19152527}\n",
        "#  'max_timesteps':10,\n",
        "print(new_research.max_timesteps)\n",
        "new_research.train({}, debug=True)\n",
        "print(new_research.action_updates)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "random_seed set to 19152527\n",
            "max_episodes set to 1000\n",
            "danger_enable set to True\n",
            "max_trajectory_size set to 10\n",
            "evaluate_after_episodes set to 5\n",
            "evaluate_average_on_episodes set to 5\n",
            "env_name set to LunarLanderContinuous-v2\n",
            "1000\n",
            "Random Seed: 19152527\n",
            "Start training\n",
            "\tDead after pair with prob:  [0.50233954]\n",
            "\tDead after pair with prob:  [0.47791004]\n",
            "\tDead after pair with prob:  [0.49762288]\n",
            "\tDead after pair with prob:  [0.4736865]\n",
            "\tDead after pair with prob:  [0.5012723]\n",
            "\tDead after pair with prob:  [0.5021666]\n",
            "\tDead after pair with prob:  [0.5076005]\n",
            "\tDead after pair with prob:  [0.5148916]\n",
            "\tDead after pair with prob:  [0.50727504]\n",
            "\tDead after pair with prob:  [0.5090325]\n",
            "Episode: 10\tAverage Reward: -206\tAverage steps: 131\t Total: 1313\n",
            "\tDead after pair with prob:  [0.57849497]\n",
            "\tDead after pair with prob:  [0.5460924]\n",
            "\tDead after pair with prob:  [0.531719]\n",
            "\tDead after pair with prob:  [0.520782]\n",
            "\tDead after pair with prob:  [0.60593134]\n",
            "\tDead after pair with prob:  [0.3654922]\n",
            "\tDead after pair with prob:  [0.8519828]\n",
            "\tDead after pair with prob:  [0.81958115]\n",
            "\tDead after pair with prob:  [0.81630135]\n",
            "\tDead after pair with prob:  [0.7686772]\n",
            "Episode: 20\tAverage Reward: -259\tAverage steps: 118\t Total: 2498\n",
            "\tDead after pair with prob:  [0.79099673]\n",
            "\tDead after pair with prob:  [0.84864604]\n",
            "\tDead after pair with prob:  [0.7506884]\n",
            "\tDead after pair with prob:  [0.9904508]\n",
            "\tDead after pair with prob:  [0.90291613]\n",
            "\tDead after pair with prob:  [0.8966233]\n",
            "\tDead after pair with prob:  [0.8384594]\n",
            "\tStuck, ep reward 55.83682286558976\n",
            "\tDead after pair with prob:  [0.8735661]\n",
            "\tDead after pair with prob:  [0.30089912]\n",
            "Episode: 30\tAverage Reward: -203\tAverage steps: 198\t Total: 4486\n",
            "\tDead after pair with prob:  [0.924879]\n",
            "\tDead after pair with prob:  [0.5991276]\n",
            "\tDead after pair with prob:  [0.61740637]\n",
            "\tDead after pair with prob:  [0.7850347]\n",
            "\tDead after pair with prob:  [0.9139576]\n",
            "\tDead after pair with prob:  [0.69869363]\n",
            "\tDead after pair with prob:  [0.9714626]\n",
            "\tDead after pair with prob:  [0.75733805]\n",
            "\tDead after pair with prob:  [0.9072098]\n",
            "\tDead after pair with prob:  [0.98925054]\n",
            "Episode: 40\tAverage Reward: -188\tAverage steps: 106\t Total: 5549\n",
            "\tDead after pair with prob:  [0.9445065]\n",
            "\tDead after pair with prob:  [0.94803303]\n",
            "\tDead after pair with prob:  [0.9192155]\n",
            "\tDead after pair with prob:  [0.77729803]\n",
            "\tDead after pair with prob:  [0.8091042]\n",
            "\tDead after pair with prob:  [0.5229869]\n",
            "\tDead after pair with prob:  [0.4382594]\n",
            "\tDead after pair with prob:  [0.9127041]\n",
            "\tDead after pair with prob:  [0.85311717]\n",
            "\tDead after pair with prob:  [0.5627446]\n",
            "Episode: 50\tAverage Reward: -172\tAverage steps: 118\t Total: 6737\n",
            "\tDead after pair with prob:  [0.9889086]\n",
            "\tDead after pair with prob:  [0.974571]\n",
            "\tDead after pair with prob:  [0.8349913]\n",
            "\tDead after pair with prob:  [0.9980697]\n",
            "\tDead after pair with prob:  [0.9574788]\n",
            "\tDead after pair with prob:  [0.9712461]\n",
            "\tDead after pair with prob:  [0.934007]\n",
            "\tDead after pair with prob:  [0.8231654]\n",
            "\tDead after pair with prob:  [0.93853754]\n",
            "\tDead after pair with prob:  [0.9501817]\n",
            "Episode: 60\tAverage Reward: -247\tAverage steps: 114\t Total: 7878\n",
            "\tDead after pair with prob:  [0.92165923]\n",
            "\tDead after pair with prob:  [0.8108476]\n",
            "\tDead after pair with prob:  [0.6329578]\n",
            "\tDead after pair with prob:  [0.97543675]\n",
            "\tDead after pair with prob:  [0.88983124]\n",
            "\tDead after pair with prob:  [0.9939904]\n",
            "\tDead after pair with prob:  [0.9694909]\n",
            "\tDead after pair with prob:  [0.7509509]\n",
            "\tDead after pair with prob:  [0.9943574]\n",
            "\tDead after pair with prob:  [0.9166136]\n",
            "Episode: 70\tAverage Reward: -269\tAverage steps: 126\t Total: 9147\n",
            "\tDead after pair with prob:  [0.917257]\n",
            "\tDead after pair with prob:  [0.9654006]\n",
            "\tDead after pair with prob:  [0.9628492]\n",
            "\tDead after pair with prob:  [0.65370864]\n",
            "\tDead after pair with prob:  [0.9523809]\n",
            "\tDead after pair with prob:  [0.88287485]\n",
            "\tDead after pair with prob:  [0.78223777]\n",
            "\tDead after pair with prob:  [0.8899739]\n",
            "\tDead after pair with prob:  [0.90562385]\n",
            "\tDead after pair with prob:  [0.8749994]\n",
            "Episode: 80\tAverage Reward: -112\tAverage steps: 130\t Total: 10448\n",
            "\tDead after pair with prob:  [0.9127193]\n",
            "\tDead after pair with prob:  [0.884804]\n",
            "\tDead after pair with prob:  [0.94255716]\n",
            "\tDead after pair with prob:  [0.96604884]\n",
            "\tDead after pair with prob:  [0.77376217]\n",
            "\tDead after pair with prob:  [0.84054804]\n",
            "\tDead after pair with prob:  [0.88197035]\n",
            "\tDead after pair with prob:  [0.8841975]\n",
            "\tDead after pair with prob:  [0.92618656]\n",
            "\tDead after pair with prob:  [0.8521861]\n",
            "Episode: 90\tAverage Reward: -112\tAverage steps: 165\t Total: 12101\n",
            "\tDead after pair with prob:  [0.93389124]\n",
            "\tDead after pair with prob:  [0.93497115]\n",
            "\tDead after pair with prob:  [0.9617207]\n",
            "\tDead after pair with prob:  [0.86904424]\n",
            "\tDead after pair with prob:  [0.9009369]\n",
            "\tDead after pair with prob:  [0.95937014]\n",
            "\tStuck, ep reward -251.35547858868793\n",
            "\tStuck, ep reward -213.90073774882626\n",
            "\tStuck, ep reward -150.9600857074699\n",
            "\tDead after pair with prob:  [0.13366818]\n",
            "Episode: 100\tAverage Reward: -182\tAverage steps: 554\t Total: 17641\n",
            "Evaluation. Timesteps: 17641 Episodes: 100\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tMean reward: -97. All rewards: [-162, -55, -126, -152, 8]\n",
            "\tDead after pair with prob:  [0.8367495]\n",
            "\tStuck, ep reward -120.22198910584552\n",
            "\tDead after pair with prob:  [0.10337092]\n",
            "\tDead after pair with prob:  [0.9598907]\n",
            "\tDead after pair with prob:  [0.604667]\n",
            "Evaluation. Timesteps: 19604 Episodes: 105\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -157. All rewards: [-193, -193, -141, -170, -87]\n",
            "\tDead after pair with prob:  [0.9515198]\n",
            "\tStuck, ep reward -174.0452498605609\n",
            "\tDead after pair with prob:  [0.4281197]\n",
            "\tDead after pair with prob:  [0.39306253]\n",
            "\tDead after pair with prob:  [0.95641905]\n",
            "Episode: 110\tAverage Reward: -88\tAverage steps: 354\t Total: 21186\n",
            "Evaluation. Timesteps: 21186 Episodes: 110\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tMean reward: -102. All rewards: [-114, -138, -135, -302, 180]\n",
            "\tDead after pair with prob:  [0.9615782]\n",
            "\tDead after pair with prob:  [0.94474703]\n",
            "\tDead after pair with prob:  [0.88618934]\n",
            "\tDead after pair with prob:  [0.67243785]\n",
            "\tDead after pair with prob:  [0.93375236]\n",
            "Evaluation. Timesteps: 21754 Episodes: 115\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -11. All rewards: [-53, -122, -9, 211, -86]\n",
            "\tDead after pair with prob:  [0.18473658]\n",
            "\tDead after pair with prob:  [0.0158675]\n",
            "\tDead after pair with prob:  [0.9373254]\n",
            "\tDead after pair with prob:  [0.85465676]\n",
            "\tDead after pair with prob:  [0.9129159]\n",
            "Episode: 120\tAverage Reward: -105\tAverage steps: 108\t Total: 22267\n",
            "Evaluation. Timesteps: 22267 Episodes: 120\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tMean reward: 36. All rewards: [-71, -111, 267, 32, 64]\n",
            "\tDead after pair with prob:  [0.9347054]\n",
            "\tDead after pair with prob:  [0.91712904]\n",
            "\tDead after pair with prob:  [0.86753947]\n",
            "\tDead after pair with prob:  [0.9155924]\n",
            "\tDead after pair with prob:  [0.4711327]\n",
            "Evaluation. Timesteps: 22813 Episodes: 125\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -38. All rewards: [-1, 6, -73, -105, -19]\n",
            "\tDead after pair with prob:  [0.85266924]\n",
            "\tDead after pair with prob:  [0.16647774]\n",
            "\tDead after pair with prob:  [0.12191979]\n",
            "Episode: 130\tAverage Reward: -5\tAverage steps: 175\t Total: 24025\n",
            "Evaluation. Timesteps: 24025 Episodes: 130\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -12. All rewards: [-52, 216, -66, -139, -20]\n",
            "\tDead after pair with prob:  [0.67528194]\n",
            "\tDead after pair with prob:  [0.91311294]\n",
            "\tDead after pair with prob:  [0.924168]\n",
            "\tDead after pair with prob:  [0.92966455]\n",
            "Evaluation. Timesteps: 24930 Episodes: 135\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -79. All rewards: [-184, -30, -10, -99, -71]\n",
            "\tDead after pair with prob:  [0.9283385]\n",
            "\tDead after pair with prob:  [0.43822265]\n",
            "\tDead after pair with prob:  [0.88388056]\n",
            "\tDead after pair with prob:  [0.6351865]\n",
            "\tDead after pair with prob:  [0.9117853]\n",
            "Episode: 140\tAverage Reward: -47\tAverage steps: 163\t Total: 25662\n",
            "Evaluation. Timesteps: 25662 Episodes: 140\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: 2. All rewards: [4, -10, -40, 184, -122]\n",
            "\tDead after pair with prob:  [0.8746444]\n",
            "\tDead after pair with prob:  [0.7911423]\n",
            "\tDead after pair with prob:  [0.7415192]\n",
            "\tDead after pair with prob:  [0.8402346]\n",
            "Evaluation. Timesteps: 26945 Episodes: 145\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: 171. All rewards: [190, 216, 273, 210, -32]\n",
            "\tDead after pair with prob:  [0.96060765]\n",
            "\tDead after pair with prob:  [0.9282347]\n",
            "\tDead after pair with prob:  [0.01438968]\n",
            "\tDead after pair with prob:  [0.9122823]\n",
            "Episode: 150\tAverage Reward: 4\tAverage steps: 224\t Total: 27903\n",
            "Evaluation. Timesteps: 27903 Episodes: 150\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tMean reward: -66. All rewards: [270, -322, -303, 260, -235]\n",
            "\tDead after pair with prob:  [0.2890703]\n",
            "\tDead after pair with prob:  [0.93826866]\n",
            "\tDead after pair with prob:  [0.9378211]\n",
            "\tDead after pair with prob:  [0.9143199]\n",
            "\tDead after pair with prob:  [0.49164826]\n",
            "Evaluation. Timesteps: 28810 Episodes: 155\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: 48. All rewards: [-68, 241, 276, -168, -39]\n",
            "\tDead after pair with prob:  [0.7616892]\n",
            "\tDead after pair with prob:  [0.9110891]\n",
            "\tDead after pair with prob:  [0.8128836]\n",
            "\tDead after pair with prob:  [0.94014204]\n",
            "Episode: 160\tAverage Reward: -78\tAverage steps: 198\t Total: 29885\n",
            "Evaluation. Timesteps: 29885 Episodes: 160\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -145. All rewards: [-134, -238, -119, -135, -100]\n",
            "\tDead after pair with prob:  [0.91096544]\n",
            "\tDead after pair with prob:  [0.87620264]\n",
            "\tDead after pair with prob:  [0.9038624]\n",
            "\tDead after pair with prob:  [0.94239074]\n",
            "\tDead after pair with prob:  [0.8320093]\n",
            "Evaluation. Timesteps: 30569 Episodes: 165\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -63. All rewards: [17, -8, -117, -16, -189]\n",
            "\tDead after pair with prob:  [0.91458386]\n",
            "\tDead after pair with prob:  [0.849536]\n",
            "\tDead after pair with prob:  [0.9123516]\n",
            "\tDead after pair with prob:  [0.95450586]\n",
            "\tDead after pair with prob:  [0.57575524]\n",
            "Episode: 170\tAverage Reward: -83\tAverage steps: 217\t Total: 32055\n",
            "Evaluation. Timesteps: 32055 Episodes: 170\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -15. All rewards: [26, -79, -6, 1, -21]\n",
            "\tDead after pair with prob:  [0.93421793]\n",
            "\tDead after pair with prob:  [0.96408415]\n",
            "\tDead after pair with prob:  [0.78737205]\n",
            "\tDead after pair with prob:  [0.93645954]\n",
            "Evaluation. Timesteps: 32800 Episodes: 175\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -49. All rewards: [-69, -52, -124, 43, -43]\n",
            "\tDead after pair with prob:  [0.7678888]\n",
            "\tDead after pair with prob:  [0.9552249]\n",
            "\tDead after pair with prob:  [0.5289105]\n",
            "\tDead after pair with prob:  [0.9582054]\n",
            "Episode: 180\tAverage Reward: 18\tAverage steps: 152\t Total: 33581\n",
            "Evaluation. Timesteps: 33581 Episodes: 180\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tMean reward: 6. All rewards: [-17, 15, -15, 30, 20]\n",
            "\tDead after pair with prob:  [0.94684947]\n",
            "\tDead after pair with prob:  [0.85052574]\n",
            "\tDead after pair with prob:  [0.8694216]\n",
            "\tDead after pair with prob:  [0.98889333]\n",
            "\tDead after pair with prob:  [0.94422007]\n",
            "Evaluation. Timesteps: 34288 Episodes: 185\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -39. All rewards: [-36, -25, -3, -42, -88]\n",
            "\tDead after pair with prob:  [0.9506634]\n",
            "\tDead after pair with prob:  [0.8930574]\n",
            "\tDead after pair with prob:  [0.9409497]\n",
            "\tDead after pair with prob:  [0.9475027]\n",
            "\tDead after pair with prob:  [0.6526721]\n",
            "Episode: 190\tAverage Reward: -65\tAverage steps: 270\t Total: 36285\n",
            "Evaluation. Timesteps: 36285 Episodes: 190\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -139. All rewards: [-111, -137, -176, -124, -146]\n",
            "\tDead after pair with prob:  [0.9500758]\n",
            "\tDead after pair with prob:  [0.9229656]\n",
            "\tDead after pair with prob:  [0.9221594]\n",
            "\tDead after pair with prob:  [0.98581845]\n",
            "\tDead after pair with prob:  [0.9095212]\n",
            "Evaluation. Timesteps: 37606 Episodes: 195\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tMean reward: -168. All rewards: [-182, -193, -120, -244, -99]\n",
            "\tDead after pair with prob:  [0.00244185]\n",
            "\tDead after pair with prob:  [0.4628074]\n",
            "\tDead after pair with prob:  [0.70836073]\n",
            "\tDead after pair with prob:  [0.45368138]\n",
            "Episode: 200\tAverage Reward: -75\tAverage steps: 257\t Total: 38863\n",
            "Evaluation. Timesteps: 38863 Episodes: 200\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tMean reward: -65. All rewards: [-42, 9, -48, -71, -172]\n",
            "\tDead after pair with prob:  [0.9617396]\n",
            "\tDead after pair with prob:  [0.95494723]\n",
            "\tDead after pair with prob:  [0.614406]\n",
            "\tDead after pair with prob:  [0.02499055]\n",
            "\tDead after pair with prob:  [0.84458727]\n",
            "Evaluation. Timesteps: 39513 Episodes: 205\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -104. All rewards: [-64, -27, -326, -35, -70]\n",
            "\tStuck, ep reward -55.743901944696766\n",
            "\tDead after pair with prob:  [0.9516516]\n",
            "\tDead after pair with prob:  [0.60357445]\n",
            "Episode: 210\tAverage Reward: -23\tAverage steps: 398\t Total: 42848\n",
            "Evaluation. Timesteps: 42848 Episodes: 210\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -100. All rewards: [-96, -55, -233, -50, -66]\n",
            "\tDead after pair with prob:  [0.9123675]\n",
            "\tStuck, ep reward -23.965477267458\n",
            "\tDead after pair with prob:  [0.96285707]\n",
            "\tDead after pair with prob:  [0.9448784]\n",
            "\tStuck, ep reward -19.774216512611012\n",
            "Evaluation. Timesteps: 45105 Episodes: 215\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -101. All rewards: [-69, 243, -240, -370, -70]\n",
            "\tDead after pair with prob:  [0.9904484]\n",
            "\tStuck, ep reward -68.92828692479188\n",
            "\tStuck, ep reward -73.44474378708802\n",
            "\tDead after pair with prob:  [0.4674961]\n",
            "\tStuck, ep reward -63.52421296329975\n",
            "Episode: 220\tAverage Reward: -58\tAverage steps: 578\t Total: 48628\n",
            "Evaluation. Timesteps: 48628 Episodes: 220\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -96. All rewards: [-72, -108, -116, -115, -69]\n",
            "\tStuck, ep reward -111.96654004212112\n",
            "\tStuck, ep reward -55.283990384678845\n",
            "\tStuck, ep reward -136.61177778113097\n",
            "\tStuck, ep reward -95.83764213921673\n",
            "\tStuck, ep reward -104.19969876381163\n",
            "Evaluation. Timesteps: 53623 Episodes: 225\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -81. All rewards: [-67, -82, -64, -135, -58]\n",
            "\tStuck, ep reward -77.12392422664121\n",
            "\tStuck, ep reward -48.93740287909426\n",
            "\tStuck, ep reward -140.7982587653735\n",
            "Episode: 230\tAverage Reward: -27\tAverage steps: 861\t Total: 57247\n",
            "Evaluation. Timesteps: 57247 Episodes: 230\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -83. All rewards: [-116, -81, -68, -77, -75]\n",
            "\tStuck, ep reward -129.16416821061708\n",
            "\tStuck, ep reward -49.44433748775275\n",
            "\tDead after pair with prob:  [0.3648676]\n",
            "\tDead after pair with prob:  [0.04369624]\n",
            "Evaluation. Timesteps: 61378 Episodes: 235\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -99. All rewards: [-119, -140, -73, -115, -50]\n",
            "\tStuck, ep reward -106.8454300873894\n",
            "\tStuck, ep reward -91.16961032993329\n",
            "\tStuck, ep reward -66.21718095870511\n",
            "\tStuck, ep reward -60.06689578927667\n",
            "\tStuck, ep reward -39.16663657468991\n",
            "Episode: 240\tAverage Reward: -41\tAverage steps: 912\t Total: 66373\n",
            "Evaluation. Timesteps: 66373 Episodes: 240\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -68. All rewards: [-60, -93, -76, -68, -45]\n",
            "\tStuck, ep reward -90.20208795324541\n",
            "\tStuck, ep reward -31.21021140412788\n",
            "\tStuck, ep reward -82.39842842041973\n",
            "\tDead after pair with prob:  [0.94798636]\n",
            "\tStuck, ep reward -37.15761195209589\n",
            "Evaluation. Timesteps: 70580 Episodes: 245\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -81. All rewards: [-70, -81, -98, -85, -72]\n",
            "\tStuck, ep reward -38.057202072693805\n",
            "\tStuck, ep reward -83.88868298781003\n",
            "\tStuck, ep reward -47.61336559578519\n",
            "\tStuck, ep reward -41.50654070624295\n",
            "\tStuck, ep reward -135.0740625440194\n",
            "Episode: 250\tAverage Reward: -60\tAverage steps: 920\t Total: 75575\n",
            "Evaluation. Timesteps: 75575 Episodes: 250\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -35. All rewards: [11, -49, -38, -30, -68]\n",
            "\tStuck, ep reward -22.739365368307766\n",
            "\tStuck, ep reward -57.285133711487134\n",
            "\tDead after pair with prob:  [0.9289248]\n",
            "\tStuck, ep reward -50.586247781286545\n",
            "\tDead after pair with prob:  [0.9487476]\n",
            "Evaluation. Timesteps: 78876 Episodes: 255\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -133. All rewards: [-102, -288, -88, -96, -88]\n",
            "\tDead after pair with prob:  [0.9410808]\n",
            "\tDead after pair with prob:  [0.9992563]\n",
            "\tDead after pair with prob:  [0.86911803]\n",
            "\tDead after pair with prob:  [0.94995004]\n",
            "\tDead after pair with prob:  [0.9695636]\n",
            "Episode: 260\tAverage Reward: -72\tAverage steps: 469\t Total: 80265\n",
            "Evaluation. Timesteps: 80265 Episodes: 260\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -82. All rewards: [-55, -165, -107, -72, -9]\n",
            "\tDead after pair with prob:  [0.91534364]\n",
            "\tDead after pair with prob:  [0.7746714]\n",
            "\tStuck, ep reward 56.78722853460921\n",
            "\tDead after pair with prob:  [0.6399213]\n",
            "\tDead after pair with prob:  [0.9538878]\n",
            "Evaluation. Timesteps: 81985 Episodes: 265\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -157. All rewards: [-201, -174, 1, -249, -164]\n",
            "\tDead after pair with prob:  [0.00397617]\n",
            "\tDead after pair with prob:  [0.89979136]\n",
            "\tDead after pair with prob:  [0.97938854]\n",
            "\tDead after pair with prob:  [0.919743]\n",
            "Episode: 270\tAverage Reward: -18\tAverage steps: 290\t Total: 83167\n",
            "Evaluation. Timesteps: 83167 Episodes: 270\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tMean reward: 31. All rewards: [239, -83, -142, -60, 203]\n",
            "\tDead after pair with prob:  [0.9744801]\n",
            "\tDead after pair with prob:  [0.91078293]\n",
            "\tDead after pair with prob:  [0.9790854]\n",
            "\tDead after pair with prob:  [0.3902943]\n",
            "\tDead after pair with prob:  [0.47790146]\n",
            "Evaluation. Timesteps: 84269 Episodes: 275\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -85. All rewards: [-98, -63, -93, -89, -84]\n",
            "\tDead after pair with prob:  [0.9508876]\n",
            "\tDead after pair with prob:  [0.9175263]\n",
            "\tDead after pair with prob:  [0.92924196]\n",
            "\tDead after pair with prob:  [0.9056231]\n",
            "\tDead after pair with prob:  [0.9263754]\n",
            "Episode: 280\tAverage Reward: -96\tAverage steps: 156\t Total: 84727\n",
            "Evaluation. Timesteps: 84727 Episodes: 280\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -124. All rewards: [-104, -131, -145, -148, -92]\n",
            "\tDead after pair with prob:  [0.9020003]\n",
            "\tDead after pair with prob:  [0.9364577]\n",
            "\tDead after pair with prob:  [0.89810455]\n",
            "\tDead after pair with prob:  [0.86476284]\n",
            "\tDead after pair with prob:  [0.3223156]\n",
            "Evaluation. Timesteps: 85719 Episodes: 285\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tMean reward: 26. All rewards: [-13, 208, -163, -132, 234]\n",
            "\tDead after pair with prob:  [0.957671]\n",
            "\tDead after pair with prob:  [0.83940095]\n",
            "\tDead after pair with prob:  [0.00957716]\n",
            "\tDead after pair with prob:  [0.9506832]\n",
            "Episode: 290\tAverage Reward: -105\tAverage steps: 286\t Total: 87590\n",
            "Evaluation. Timesteps: 87590 Episodes: 290\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -84. All rewards: [-121, -92, -107, 19, -121]\n",
            "\tDead after pair with prob:  [0.95675784]\n",
            "\tStuck, ep reward -106.30421160545382\n",
            "\tStuck, ep reward -127.99886001739472\n",
            "\tDead after pair with prob:  [0.6603455]\n",
            "Evaluation. Timesteps: 90693 Episodes: 295\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tMean reward: 1. All rewards: [-96, -80, -4, -94, 286]\n",
            "\tDead after pair with prob:  [0.94978106]\n",
            "\tDead after pair with prob:  [0.9723528]\n",
            "\tDead after pair with prob:  [0.9587252]\n",
            "\tStuck, ep reward -11.930157533740536\n",
            "Episode: 300\tAverage Reward: -26\tAverage steps: 490\t Total: 92493\n",
            "Evaluation. Timesteps: 92493 Episodes: 300\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -135. All rewards: [-49, -97, -57, -263, -211]\n",
            "\tDead after pair with prob:  [0.9275235]\n",
            "\tDead after pair with prob:  [0.9418288]\n",
            "\tDead after pair with prob:  [0.9048617]\n",
            "Evaluation. Timesteps: 93614 Episodes: 305\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tMean reward: -74. All rewards: [-114, -79, -45, -49, -83]\n",
            "\tDead after pair with prob:  [0.9242034]\n",
            "\tDead after pair with prob:  [0.92299706]\n",
            "\tStuck, ep reward 61.36924442510554\n",
            "\tDead after pair with prob:  [0.9501278]\n",
            "Episode: 310\tAverage Reward: 22\tAverage steps: 271\t Total: 95205\n",
            "Evaluation. Timesteps: 95205 Episodes: 310\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -43. All rewards: [-348, 251, -44, -19, -56]\n",
            "\tStuck, ep reward -73.95306576113838\n",
            "\tDead after pair with prob:  [0.9926744]\n",
            "\tDead after pair with prob:  [0.99392456]\n",
            "Evaluation. Timesteps: 97304 Episodes: 315\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tMean reward: -107. All rewards: [-188, -181, -143, 3, -29]\n",
            "\tDead after pair with prob:  [0.9590023]\n",
            "\tDead after pair with prob:  [0.9989697]\n",
            "\tStuck, ep reward -44.83525528292368\n",
            "\tDead after pair with prob:  [0.9418612]\n",
            "Episode: 320\tAverage Reward: -6\tAverage steps: 372\t Total: 98931\n",
            "Evaluation. Timesteps: 98931 Episodes: 320\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -169. All rewards: [-99, -304, -197, -142, -101]\n",
            "\tStuck, ep reward -25.336176165926073\n",
            "\tDead after pair with prob:  [0.53809375]\n",
            "\tDead after pair with prob:  [0.97072405]\n",
            "\tDead after pair with prob:  [0.9724983]\n",
            "\tDead after pair with prob:  [0.98671305]\n",
            "Evaluation. Timesteps: 100230 Episodes: 325\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -52. All rewards: [172, -157, -101, -86, -89]\n",
            "\tDead after pair with prob:  [0.95186365]\n",
            "\tDead after pair with prob:  [0.00363209]\n",
            "\tDead after pair with prob:  [0.39423025]\n",
            "\tStuck, ep reward -24.526077283432173\n",
            "Episode: 330\tAverage Reward: -21\tAverage steps: 328\t Total: 102216\n",
            "Evaluation. Timesteps: 102216 Episodes: 330\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tMean reward: 72. All rewards: [-68, 190, -124, 202, 161]\n",
            "\tStuck, ep reward -86.88799545637242\n",
            "\tDead after pair with prob:  [0.12139717]\n",
            "\tStuck, ep reward -15.045015770486916\n",
            "\tStuck, ep reward 2.081348224246258\n",
            "\tDead after pair with prob:  [7.4129736e-10]\n",
            "Evaluation. Timesteps: 106452 Episodes: 335\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -70. All rewards: [-69, -86, -76, -96, -24]\n",
            "\tStuck, ep reward -83.82765853659988\n",
            "\tStuck, ep reward -53.81753134960361\n",
            "\tDead after pair with prob:  [2.1635886e-08]\n",
            "\tStuck, ep reward -104.12735289874645\n",
            "Episode: 340\tAverage Reward: -34\tAverage steps: 845\t Total: 110672\n",
            "Evaluation. Timesteps: 110672 Episodes: 340\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -12. All rewards: [266, -180, -53, -21, -72]\n",
            "\tDead after pair with prob:  [0.95455444]\n",
            "\tStuck, ep reward -35.43061386927102\n",
            "\tStuck, ep reward -24.081775104271237\n",
            "\tStuck, ep reward -51.61948972975194\n",
            "Evaluation. Timesteps: 114585 Episodes: 345\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -45. All rewards: [-53, -15, -39, -74, -42]\n",
            "\tStuck, ep reward -37.809104860697005\n",
            "\tStuck, ep reward -39.77018365627422\n",
            "\tDead after pair with prob:  [0.8849466]\n",
            "\tDead after pair with prob:  [0.00078534]\n",
            "Episode: 350\tAverage Reward: -38\tAverage steps: 767\t Total: 118345\n",
            "Evaluation. Timesteps: 118345 Episodes: 350\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tMean reward: 119. All rewards: [168, 2, 0, 247, 180]\n",
            "\tStuck, ep reward -10.386414212367525\n",
            "\tStuck, ep reward -40.03710641669972\n",
            "\tStuck, ep reward -27.325234926844164\n",
            "Evaluation. Timesteps: 122441 Episodes: 355\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: 106. All rewards: [187, 182, 270, -53, -53]\n",
            "\tDead after pair with prob:  [0.92939526]\n",
            "Episode: 360\tAverage Reward: 103\tAverage steps: 710\t Total: 125447\n",
            "Evaluation. Timesteps: 125447 Episodes: 360\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -9. All rewards: [-53, -43, 183, -66, -67]\n",
            "\tStuck, ep reward -76.95813022631334\n",
            "\tStuck, ep reward -33.809846360802084\n",
            "\tStuck, ep reward 66.62958236891683\n",
            "\tStuck, ep reward -21.32945346820472\n",
            "Evaluation. Timesteps: 130402 Episodes: 365\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tMean reward: -64. All rewards: [38, -87, -24, -241, -9]\n",
            "\tStuck, ep reward -26.60787652377495\n",
            "\tDead after pair with prob:  [0.03700201]\n",
            "\tStuck, ep reward -25.81658502385288\n",
            "\tStuck, ep reward -16.055570347842394\n",
            "Episode: 370\tAverage Reward: 1\tAverage steps: 836\t Total: 133810\n",
            "Evaluation. Timesteps: 133810 Episodes: 370\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tMean reward: 78. All rewards: [16, -25, -2, 219, 184]\n",
            "\tStuck, ep reward -55.771193367701684\n",
            "\tStuck, ep reward 17.97263687681069\n",
            "\tStuck, ep reward -45.86300044573406\n",
            "\tStuck, ep reward -2.1174394557022564\n",
            "\tDead after pair with prob:  [0.5577022]\n",
            "Evaluation. Timesteps: 138729 Episodes: 375\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tMean reward: 17. All rewards: [5, 1, -41, 7, 115]\n",
            "\tStuck, ep reward -15.3261065457698\n",
            "\tStuck, ep reward -5.211656494203156\n",
            "\tStuck, ep reward -24.67423390361951\n",
            "\tStuck, ep reward -175.05359745046772\n",
            "Episode: 380\tAverage Reward: -31\tAverage steps: 988\t Total: 143691\n",
            "Evaluation. Timesteps: 143691 Episodes: 380\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tMean reward: 2. All rewards: [-163, 6, -29, -14, 213]\n",
            "\tStuck, ep reward 21.34067162644831\n",
            "\tStuck, ep reward -102.27617492864539\n",
            "\tStuck, ep reward -16.39948933610299\n",
            "Evaluation. Timesteps: 147949 Episodes: 385\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tMean reward: -32. All rewards: [-60, -140, -8, -67, 112]\n",
            "\tStuck, ep reward -15.46874481761523\n",
            "\tStuck, ep reward -14.716641631769058\n",
            "\tDead after pair with prob:  [0.59944886]\n",
            "\tStuck, ep reward -62.84548844882364\n",
            "Episode: 390\tAverage Reward: 29\tAverage steps: 835\t Total: 152043\n",
            "Evaluation. Timesteps: 152043 Episodes: 390\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: 64. All rewards: [200, 223, -48, -42, -11]\n",
            "\tStuck, ep reward 0.8741884387279304\n",
            "Evaluation. Timesteps: 155219 Episodes: 395\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tMean reward: 157. All rewards: [119, -52, 244, 247, 227]\n",
            "\tStuck, ep reward -66.9398100084137\n",
            "\tStuck, ep reward -55.19353813539625\n",
            "\tStuck, ep reward -10.203781778686569\n",
            "Episode: 400\tAverage Reward: 119\tAverage steps: 720\t Total: 159248\n",
            "Evaluation. Timesteps: 159248 Episodes: 400\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -27. All rewards: [-12, 9, -51, -68, -15]\n",
            "\tStuck, ep reward -28.837831459829303\n",
            "\tStuck, ep reward -14.960710508206741\n",
            "\tStuck, ep reward -28.042032033060536\n",
            "\tDead after pair with prob:  [0.7582254]\n",
            "\tStuck, ep reward 22.547407423149867\n",
            "Evaluation. Timesteps: 163423 Episodes: 405\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tMean reward: 139. All rewards: [269, -74, 151, 181, 168]\n",
            "\tDead after pair with prob:  [0.9956338]\n",
            "\tStuck, ep reward -67.69479839164387\n",
            "Episode: 410\tAverage Reward: 0\tAverage steps: 683\t Total: 166087\n",
            "Evaluation. Timesteps: 166087 Episodes: 410\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tMean reward: 146. All rewards: [28, 39, 190, 179, 293]\n",
            "\tStuck, ep reward 11.784105954954573\n",
            "\tStuck, ep reward -23.577900945782073\n",
            "Evaluation. Timesteps: 169690 Episodes: 415\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tMean reward: 116. All rewards: [-84, 244, 233, -4, 194]\n",
            "\tDead after pair with prob:  [0.08953554]\n",
            "\tStuck, ep reward 40.84309913968369\n",
            "Episode: 420\tAverage Reward: 104\tAverage steps: 684\t Total: 172928\n",
            "Evaluation. Timesteps: 172928 Episodes: 420\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tMean reward: 40. All rewards: [-76, 222, -20, -92, 167]\n",
            "\tDead after pair with prob:  [0.42006075]\n",
            "\tStuck, ep reward -61.12382287676304\n",
            "Evaluation. Timesteps: 176949 Episodes: 425\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tDead False\tUpdate False\tFinished\n",
            "\t\tMean reward: 172. All rewards: [253, 110, 159, 112, 227]\n",
            "Reach the end\n",
            "Updated steps\tSteps\tReward\tEpisode\tSeed\n",
            "2131\t176949.0\t172\t425\t19152527\n",
            "2131\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}