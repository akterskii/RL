{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TD3PG.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "61VeKABEm9_O",
        "geRE4MK65Wnb",
        "UH9aTMAwIeTf",
        "YnZxLGmrlAyw"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akterskii/RL/blob/master/Dead%20prediction/TD3PG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4Ovhw-h0cfa",
        "colab_type": "code",
        "outputId": "33ff385d-8d7a-4a3b-e1e0-80226e04f5ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "source": [
        "!pip install gym['box2d']"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.6/dist-packages (0.10.11)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.3.1)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.4.2)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (2.21.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.16.5)\n",
            "Collecting box2d-py>=2.3.5; extra == \"box2d\" (from gym[box2d])\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/bd/6cdc3fd994b0649dcf5d9bad85bd9e26172308bbe9a421bfc6fdbf5081a6/box2d_py-2.3.8-cp36-cp36m-manylinux1_x86_64.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 6.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym[box2d]) (0.16.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[box2d]) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[box2d]) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[box2d]) (2019.6.16)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[box2d]) (1.24.3)\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GuuZ4NNdAIa",
        "colab_type": "code",
        "outputId": "58b96192-f04d-4793-8e51-d62665772802",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "# Run this cell to mount your Google Drive.\n",
        "from google.colab import drive\n",
        "mount = '/content/drive'\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e81Vm0EmsKyN",
        "colab_type": "text"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAAepgnysM01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os \n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "#for RAdam\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "\n",
        "# for custom env\n",
        "from gym.wrappers import TimeLimit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXdG4uSSr9Kx",
        "colab_type": "text"
      },
      "source": [
        "## RAdam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TuAFDpGr8aX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RAdam(Optimizer):\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
        "        self.buffer = [[None, None, None] for ind in range(10)]\n",
        "        super(RAdam, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(RAdam, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data.float()\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
        "\n",
        "                p_data_fp32 = p.data.float()\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
        "                else:\n",
        "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "\n",
        "                state['step'] += 1\n",
        "                buffered = self.buffer[int(state['step'] % 10)]\n",
        "                if state['step'] == buffered[0]:\n",
        "                    N_sma, step_size = buffered[1], buffered[2]\n",
        "                else:\n",
        "                    buffered[0] = state['step']\n",
        "                    beta2_t = beta2 ** state['step']\n",
        "                    N_sma_max = 2 / (1 - beta2) - 1\n",
        "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
        "                    buffered[1] = N_sma\n",
        "                    if N_sma > 5:\n",
        "                        step_size = group['lr'] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
        "                    else:\n",
        "                        step_size = group['lr'] / (1 - beta1 ** state['step'])\n",
        "                    buffered[2] = step_size\n",
        "\n",
        "                if group['weight_decay'] != 0:\n",
        "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
        "\n",
        "                if N_sma > 5:                    \n",
        "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n",
        "                else:\n",
        "                    p_data_fp32.add_(-step_size, exp_avg)\n",
        "\n",
        "                p.data.copy_(p_data_fp32)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu-XBmsVIJ-p",
        "colab_type": "text"
      },
      "source": [
        "## Actor and Critic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92njCCoUIMxr",
        "colab_type": "code",
        "outputId": "f554637a-b9e5-436c-adf2-e38a61702284",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, max_action, name):\n",
        "        super(Actor, self).__init__()\n",
        "        \n",
        "        self.l1 = nn.Linear(state_dim, 400)\n",
        "        self.l2 = nn.Linear(400, 300)\n",
        "        self.l3 = nn.Linear(300, action_dim)\n",
        "        \n",
        "        self.max_action = max_action\n",
        "        \n",
        "        self.fname = name + '.pth'\n",
        "        \n",
        "    def forward(self, state):\n",
        "        a = F.relu(self.l1(state))\n",
        "        a = F.relu(self.l2(a))\n",
        "        a = torch.tanh(self.l3(a)) * self.max_action\n",
        "        return a\n",
        "    \n",
        "    \n",
        "        \n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, name, probability=False):\n",
        "        super(Critic, self).__init__()\n",
        "        \n",
        "        self.l1 = nn.Linear(state_dim + action_dim, 400)\n",
        "        self.l2 = nn.Linear(400, 300)\n",
        "        self.l3 = nn.Linear(300, 1)\n",
        "        \n",
        "        self.probability = probability\n",
        "        \n",
        "        self.name = name\n",
        "        self.fname = name + '.pth'\n",
        "        \n",
        "    def forward(self, state, action):\n",
        "        state_action = torch.cat([state, action], 1)\n",
        "        \n",
        "        q = F.relu(self.l1(state_action))\n",
        "        q = F.relu(self.l2(q))\n",
        "        q = self.l3(q)\n",
        "        if self.probability:\n",
        "            q = torch.sigmoid(q)\n",
        "        return q\n",
        "    "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHv86zi1IR8G",
        "colab_type": "text"
      },
      "source": [
        "## Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39B6n_Mq0kfK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TD3:\n",
        "    def __init__(self, lr, state_dim, action_dim, max_action, danger, danger_threshold, directory, fname, epochs_for_danger):\n",
        "        opt_RAdam = True\n",
        "        \n",
        "        self.actor = Actor(state_dim, action_dim, max_action, 'actor').to(device)\n",
        "        self.actor_target = Actor(state_dim, action_dim, max_action, 'actor_target').to(device)\n",
        "        self.actor_perturbed = Actor(state_dim, action_dim, max_action, 'actor_target').to(device)\n",
        "        \n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "        \n",
        "        \n",
        "        if opt_RAdam:\n",
        "            self.actor_optimizer = RAdam(self.actor.parameters(), lr=lr)\n",
        "        else:\n",
        "            self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n",
        "        \n",
        "        \n",
        "        self.critic_1 = Critic(state_dim, action_dim, 'critic_1').to(device)\n",
        "        self.critic_1_target = Critic(state_dim, action_dim, 'critic_1_target').to(device)\n",
        "        self.critic_1_target.load_state_dict(self.critic_1.state_dict())\n",
        "        if opt_RAdam:\n",
        "            self.critic_1_optimizer = RAdam(self.critic_1.parameters(), lr=lr)\n",
        "        else:\n",
        "            self.critic_1_optimizer = optim.Adam(self.critic_1.parameters(), lr=lr)\n",
        "        \n",
        "        self.critic_2 = Critic(state_dim, action_dim, 'critic_2').to(device)\n",
        "        self.critic_2_target = Critic(state_dim, action_dim, 'critic_2_target').to(device)\n",
        "        self.critic_2_target.load_state_dict(self.critic_2.state_dict())\n",
        "        if opt_RAdam:\n",
        "            self.critic_2_optimizer = RAdam(self.critic_2.parameters(), lr=lr)\n",
        "        else:\n",
        "            self.critic_2_optimizer = optim.Adam(self.critic_2.parameters(), lr=lr)\n",
        "        \n",
        "        self.max_action = max_action\n",
        "        \n",
        "        #danger\n",
        "        self.danger = danger\n",
        "        self.actor_danger = Actor(state_dim, action_dim, max_action, 'actor_danger').to(device)\n",
        "        if opt_RAdam:\n",
        "            self.actor_danger_optimizer = RAdam(self.actor_danger.parameters(), lr=lr)\n",
        "        else:\n",
        "            self.actor_danger_optimizer = optim.Adam(self.actor_danger.parameters(), lr=lr)\n",
        "        \n",
        "        self.critic_danger = Critic(state_dim, action_dim, 'critic_danger', probability=True).to(device)\n",
        "        if opt_RAdam:\n",
        "            self.critic_danger_optimizer = RAdam(self.critic_danger.parameters(), lr=lr)\n",
        "        else:       \n",
        "            self.critic_danger_optimizer = optim.Adam(self.critic_danger.parameters(), lr=lr)\n",
        "        \n",
        "        self.threshold = danger_threshold\n",
        "        self.directory = directory\n",
        "        self.fname = fname\n",
        "        self.epochs_for_danger = epochs_for_danger\n",
        "        self.action_update = False\n",
        "          \n",
        "    \n",
        "    def select_action(self, state, danger=False, param_noise=False, debug=False):\n",
        "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)        \n",
        "        \n",
        "        if param_noise:\n",
        "            self.actor_perturbed.eval()\n",
        "            action = self.actor_perturbed(state)\n",
        "            self.actor_perturbed.train()\n",
        "        else:\n",
        "            self.actor.eval()\n",
        "            action = self.actor(state)\n",
        "            self.actor.train()\n",
        "            \n",
        "        if danger:\n",
        "            self.action_update = False\n",
        "            \n",
        "            # estimate probability of death\n",
        "            init_prob_danger = self.critic_danger(state, action).cpu().data.numpy().flatten()\n",
        "            if  init_prob_danger > self.threshold:\n",
        "                \n",
        "                # safe action in danger case\n",
        "                self.actor_danger.eval() # set evaluation mode\n",
        "                action = self.actor_danger(state)\n",
        "                self.actor_danger.train() # set vack train mode\n",
        "                \n",
        "                self.action_update = True\n",
        "                \n",
        "                if debug:\n",
        "                    new_prob_danger = self.critic_danger(state, action).cpu().data.numpy().flatten()\n",
        "                    print( \"\\t\\t\\tP before: {}. P after: {}\".format(init_prob_danger, new_prob_danger))\n",
        "        \n",
        "        return action.cpu().data.numpy().flatten()\n",
        "    \n",
        "    \n",
        "    def perturb_actor_parameters(self, param_noise):\n",
        "        \"\"\"Apply parameter noise to actor model, for exploration\"\"\"\n",
        "        hard_update(self.actor_perturbed, self.actor)\n",
        "        params = self.actor_perturbed.state_dict()\n",
        "        for name in params:\n",
        "            if 'ln' in name: \n",
        "                pass \n",
        "            param = params[name]\n",
        "            random = torch.randn(param.shape).to(device)\n",
        "            \n",
        "            param += random * param_noise.current_stddev\n",
        "        \n",
        "    \n",
        "    def update(self, replay_buffer, replay_buffer_danger, n_iter, batch_size, batch_size_danger, gamma, polyak, policy_noise, noise_clip, policy_delay):\n",
        "        \n",
        "        for i in range(n_iter):\n",
        "            # Sample a batch of transitions from replay buffer:\n",
        "            state, action_, reward, next_state, done = replay_buffer.sample(batch_size)            \n",
        "            state = torch.FloatTensor(state).to(device)\n",
        "            action = torch.FloatTensor(action_).to(device)\n",
        "            reward = torch.FloatTensor(reward).reshape((batch_size,1)).to(device)\n",
        "            next_state = torch.FloatTensor(next_state).to(device)\n",
        "            done = torch.FloatTensor(done).reshape((batch_size,1)).to(device)\n",
        "                                    \n",
        "            # Select next action according to target policy:\n",
        "            noise = torch.FloatTensor(action_).data.normal_(0, policy_noise).to(device)\n",
        "            noise = noise.clamp(-noise_clip, noise_clip)\n",
        "            next_action = (self.actor_target(next_state) + noise)\n",
        "            next_action = next_action.clamp(-self.max_action, self.max_action)\n",
        "            \n",
        "            # Compute target Q-value:\n",
        "            target_Q1 = self.critic_1_target(next_state, next_action)\n",
        "            target_Q2 = self.critic_2_target(next_state, next_action)\n",
        "            target_Q = torch.min(target_Q1, target_Q2)\n",
        "            target_Q = reward + ((1-done) * gamma * target_Q).detach()\n",
        "            \n",
        "                                    \n",
        "            # Optimize Critic 1:\n",
        "            current_Q1 = self.critic_1(state, action)\n",
        "            loss_Q1 = F.mse_loss(current_Q1, target_Q)\n",
        "            self.critic_1_optimizer.zero_grad()\n",
        "            loss_Q1.backward()\n",
        "            self.critic_1_optimizer.step()\n",
        "            \n",
        "            # Optimize Critic 2:\n",
        "            current_Q2 = self.critic_2(state, action)\n",
        "            loss_Q2 = F.mse_loss(current_Q2, target_Q)\n",
        "            self.critic_2_optimizer.zero_grad()\n",
        "            loss_Q2.backward()\n",
        "            self.critic_2_optimizer.step()\n",
        "            \n",
        "            \n",
        "            # Delayed policy updates:\n",
        "            if i % policy_delay == 0:\n",
        "                # Compute actor loss:\n",
        "                actor_loss = -self.critic_1(state, self.actor(state)).mean()\n",
        "                \n",
        "                # Optimize the actor\n",
        "                self.actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                self.actor_optimizer.step()                \n",
        "                \n",
        "                # Polyak averaging update:\n",
        "                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "                    target_param.data.copy_( (polyak * target_param.data) + ((1-polyak) * param.data))\n",
        "                \n",
        "                for param, target_param in zip(self.critic_1.parameters(), self.critic_1_target.parameters()):\n",
        "                    target_param.data.copy_( (polyak * target_param.data) + ((1-polyak) * param.data))\n",
        "                \n",
        "                for param, target_param in zip(self.critic_2.parameters(), self.critic_2_target.parameters()):\n",
        "                    target_param.data.copy_( (polyak * target_param.data) + ((1-polyak) * param.data))\n",
        "                  \n",
        "                  \n",
        "        \n",
        "        \n",
        "        if self.danger and len(replay_buffer_danger.buffer) > 0:\n",
        "            batch_steps = max(1, replay_buffer_danger.size // batch_size_danger)\n",
        "            for _ in range(self.epochs_for_danger):\n",
        "                for j in range(batch_steps):\n",
        "                    # Sample two batches of transitions: deadend and normals\n",
        "                    state_not_danger, action_not_danger, _, _, done_not_danger = replay_buffer.sample(batch_size_danger)            \n",
        "                    state_not_danger = torch.FloatTensor(state_not_danger).to(device)\n",
        "                    action_not_danger = torch.FloatTensor(action_not_danger).to(device)\n",
        "                    done_not_danger = torch.FloatTensor(done_not_danger).reshape((batch_size_danger, 1)).to(device)\n",
        "            \n",
        "                    state_danger, action_danger, _, _, done_danger = replay_buffer_danger.sample(batch_size_danger)            \n",
        "                    state_danger = torch.FloatTensor(state_danger).to(device)\n",
        "                    action_danger = torch.FloatTensor(action_danger).to(device)\n",
        "                    done_danger = torch.FloatTensor(done_danger).reshape((batch_size_danger, 1)).to(device)\n",
        "                  \n",
        "                    # Compute danger probabilities\n",
        "                    target_Q_not_danger = done_not_danger\n",
        "                    target_Q_danger = done_danger\n",
        "                    #pprint(\"dan_q\", target_Q_not_danger, target_Q_danger)\n",
        "                  \n",
        "                    # Optimize Critic Danger:\n",
        "                    current_Q_danger = self.critic_danger(state_danger, action_danger)\n",
        "                    current_Q_not_danger = self.critic_danger(state_not_danger, action_not_danger)\n",
        "                    loss_Q_danger = F.mse_loss(current_Q_danger, target_Q_danger)\n",
        "                    loss_Q_not_danger = F.mse_loss(current_Q_not_danger, target_Q_not_danger)\n",
        "                    loss_QD =(loss_Q_danger + loss_Q_not_danger)/2\n",
        "                    self.critic_danger_optimizer.zero_grad()\n",
        "                    loss_QD.backward()\n",
        "                    self.critic_danger_optimizer.step()\n",
        "                    \n",
        "                    if j % policy_delay == 0:\n",
        "                        actor_danger_loss = self.critic_danger(state_danger, self.actor_danger(state_danger)).mean()\n",
        "                    \n",
        "                        # Optimize the actor for danger\n",
        "                        self.actor_danger_optimizer.zero_grad()\n",
        "                        actor_danger_loss.backward()\n",
        "                        self.actor_danger_optimizer.step()           \n",
        "                     \n",
        "                \n",
        "    def save(self, directory=None, fname=None, optimizers = False, danger = False):\n",
        "        if directory is None:\n",
        "            directory = self.directory\n",
        "        if fname is None:\n",
        "            fname = self.fname\n",
        "            \n",
        "        base_path = \"%s/%s_\"% (directory, fname)\n",
        "        \n",
        "        torch.save(self.actor.state_dict(), base_path + self.actor.fname)\n",
        "        torch.save(self.actor_target.state_dict(), base_path + self.actor_target.fname)\n",
        "        \n",
        "        torch.save(self.critic_1.state_dict(), base_path + self.critic_1.fname)\n",
        "        torch.save(self.critic_1_target.state_dict(), base_path + self.critic_1_target.fname)\n",
        "        \n",
        "        torch.save(self.critic_2.state_dict(), base_path + self.critic_2.fname)\n",
        "        torch.save(self.critic_2_target.state_dict(), base_path + self.critic_2_target.fname)\n",
        "        \n",
        "        if danger:\n",
        "            torch.save(self.actor_danger.state_dict(),  base_path + self.actor_danger.fname)\n",
        "            torch.save(self.critic_danger.state_dict(), base_path + self.critic_danger.fname)\n",
        "        \n",
        "        if optimizers:\n",
        "            torch.save(self.actor_optimizer.state_dict(), '%s/%s_actor_optimizer.pth' % (directory, fname))\n",
        "            torch.save(self.critic_1_optimizer.state_dict(), '%s/%s_critic_1_optimizer.pth' % (directory, fname))\n",
        "            torch.save(self.critic_2_optimizer.state_dict(), '%s/%s_critic_2_optimizer.pth' % (directory, fname))\n",
        "            if danger:\n",
        "                torch.save(self.actor_danger_optimizer.state_dict(), '%s/%s_actor_danger_optimizer.pth' % (directory, fname))\n",
        "                torch.save(self.critic_danger_optimizer.state_dict(), '%s/%s_critic_danger_optimizer.pth' % (directory, fname))\n",
        "                \n",
        "        \n",
        "    def load(self, directory=None, fname=None, optimizers=False, danger = False):\n",
        "        if directory is None:\n",
        "            directory = self.directory\n",
        "        if fname is None:\n",
        "            fname = self.fname\n",
        "            \n",
        "        base_path = \"%s/%s_\"% (directory, fname)\n",
        "        \n",
        "        self.actor.load_state_dict(torch.load(base_path + self.actor.fname, map_location=lambda storage, loc: storage))\n",
        "        self.actor_target.load_state_dict(torch.load(base_path + self.actor_target.fname, map_location=lambda storage, loc: storage))\n",
        "        \n",
        "        self.critic_1.load_state_dict(torch.load(base_path + self.critic_1.fname, map_location=lambda storage, loc: storage))\n",
        "        self.critic_1_target.load_state_dict(torch.load(base_path + self.critic_1_target.fname, map_location=lambda storage, loc: storage))\n",
        "        \n",
        "        self.critic_2.load_state_dict(torch.load(base_path + self.critic_2.fname, map_location=lambda storage, loc: storage))\n",
        "        self.critic_2_target.load_state_dict(torch.load(base_path + self.critic_2_target.fname, map_location=lambda storage, loc: storage))\n",
        "        \n",
        "        if danger:\n",
        "            self.actor_danger.load_state_dict(torch.load('%s/%s_actor_danger.pth' % (directory, name), map_location=lambda storage, loc: storage))\n",
        "            self.critic_danger.load_state_dict(torch.load('%s/%s_critic_danger.pth' % (directory, name), map_location=lambda storage, loc: storage))\n",
        "        \n",
        "        if optimizers:\n",
        "            self.actor_optimizer.load_state_dict(torch.load( '%s/%s_actor_optimizer.pth' % (directory, name), map_location=lambda storage, loc: storage))\n",
        "            self.critic_1_optimizer.load_state_dict(torch.load('%s/%s_critic_1_optimizer.pth' % (directory, name), map_location=lambda storage, loc: storage))\n",
        "            self.critic_2_optimizer.load_state_dict(torch.load('%s/%s_critic_2_optimizer.pth' % (directory, name), map_location=lambda storage, loc: storage))\n",
        "            if danger:\n",
        "                self.actor_danger_optimizer.load_state_dict(torch.load(base_path + self.actor_danger.fname, map_location=lambda storage, loc: storage))\n",
        "                self.critic_danger_optimizer.load_state_dict(torch.load(base_path + self.critic_danger.fname, map_location=lambda storage, loc: storage))\n",
        "        \n",
        "        \n",
        "    def load_actor(self, directory=None, fname=None, danger=False):\n",
        "        if directory is None:\n",
        "            directory = self.directory\n",
        "        if fname is None:\n",
        "            fname = self.fname      \n",
        "      \n",
        "        base_path = \"%s/%s_\"% (directory, fname)\n",
        "        self.actor.load_state_dict(torch.load(base_path + self.actor.fname, map_location=lambda storage, loc: storage))\n",
        "        self.actor_target.load_state_dict(torch.load(base_path + self.actor_target.fname, map_location=lambda storage, loc: storage))\n",
        "        if danger:\n",
        "            self.actor_danger.load_state_dict(torch.load(base_path + self.actor_danger.fname, map_location=lambda storage, loc: storage))\n",
        "            self.critic_danger.load_state_dict(torch.load(base_path + self.critic_danger.fname, map_location=lambda storage, loc: storage))\n",
        "        \n",
        "        \n",
        "        \n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pr_4tXGr5Qzj",
        "colab_type": "text"
      },
      "source": [
        "##Custom Bipedal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQj8Rpz3yw2P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import warnings\n",
        "from gym.envs.box2d.bipedal_walker import *\n",
        "class CustomizableBipedalWalker(BipedalWalker):\n",
        "    def __init__(self):\n",
        "        self.default_params = {\n",
        "            'stump_height_low': 1,\n",
        "            'stump_height_high': 3,\n",
        "            'pit_depth': 4,\n",
        "            'pit_width_low': 3,\n",
        "            'pit_width_high': 5,\n",
        "            'stair_heights': [-.5, .5],\n",
        "            'stair_width_low': 4,\n",
        "            'stair_width_high': 5,\n",
        "            'stair_steps_low': 3,\n",
        "            'stair_steps_high': 5,\n",
        "            'states': [0],\n",
        "            'state_probs': None\n",
        "        }\n",
        "        self.params = {**self.default_params}\n",
        "        BipedalWalker.__init__(self)\n",
        "        \n",
        "    def _update_env_params(self, **kwargs):\n",
        "        # TODO: add kind of sanity check here\n",
        "        self.params = {**self.params, **kwargs}\n",
        "        _ = self.reset()\n",
        "        \n",
        "    def reset_env_params(self, hardcore=False):\n",
        "        params = {**self.default_params}\n",
        "        if hardcore:\n",
        "            params['states'] = np.arange(4)\n",
        "        self._update_env_params(**params)\n",
        "    \n",
        "    def set_env_states(self, state_mask, p=None):\n",
        "        \"\"\"\n",
        "        :param state_mask: np.array(,dtype=bool) that masks [\"GRASS\", \"STUMP\", \"STAIRS\", \"PIT\"].\n",
        "            Note that masking out \"GRASS\" takes no effect.\n",
        "        :param p: np.array or list of probabilities: [p_grass, p_stump, p_stairs, p_pit].\n",
        "            Probs corresponding to masked out states are ignored\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        states_ = np.arange(4)[state_mask]\n",
        "        p_ = None\n",
        "        if p is not None:\n",
        "            p_ = np.array(p)\n",
        "            if not np.all(p_ >= 0):\n",
        "                raise ValueError\n",
        "            p_ = p_[state_mask] / p_[state_mask].sum()\n",
        "        self._update_env_params(states=states_, state_probs=p_)\n",
        "    \n",
        "    def set_env_params(self, pit_width=None, stair_width=None, stair_steps=None, stump_height=None):\n",
        "        \"\"\"\n",
        "            NB: All params are integers or tuples of integers\n",
        "        \"\"\"\n",
        "        kwargs = {**locals()}\n",
        "        _ = kwargs.pop('self', None)\n",
        "        params = {}\n",
        "        for k,v in kwargs.items():\n",
        "            if type(v) is int:\n",
        "                params[k + '_low'] = v\n",
        "                params[k + '_high'] = v + 1\n",
        "            elif isinstance(v, (tuple, list)): \n",
        "                if v[1] - v[0] >= 1:\n",
        "                    params[k + '_low'] = v[0]\n",
        "                    params[k + '_high'] = v[1]\n",
        "                else:\n",
        "                    warnings.warn(f'{k} shoud be an integer. {k}[1] - {k}[0] < 1 '+\\\n",
        "                                  f'=> will set {k}_low = {v[0]}, {k}_high = {v[0]+1}')\n",
        "                    params[k + '_low'] = v[0]\n",
        "                    params[k + '_high'] = v[0] + 1\n",
        "        self._update_env_params(**params)\n",
        "        \n",
        "    def _generate_terrain(self, hardcore=True):\n",
        "        GRASS, STUMP, STAIRS, PIT, _STATES_ = range(5)\n",
        "        state    = GRASS\n",
        "        velocity = 0.0\n",
        "        y        = TERRAIN_HEIGHT\n",
        "        counter  = TERRAIN_STARTPAD\n",
        "        oneshot  = False\n",
        "        self.terrain   = []\n",
        "        self.terrain_x = []\n",
        "        self.terrain_y = []\n",
        "        for i in range(TERRAIN_LENGTH):\n",
        "            x = i*TERRAIN_STEP\n",
        "            self.terrain_x.append(x)\n",
        "\n",
        "            if state==GRASS and not oneshot:\n",
        "                velocity = 0.8*velocity + 0.01*np.sign(TERRAIN_HEIGHT - y)\n",
        "                if i > TERRAIN_STARTPAD: velocity += self.np_random.uniform(-1, 1)/SCALE   #1\n",
        "                y += velocity\n",
        "\n",
        "            elif state==PIT and oneshot:\n",
        "                counter = self.np_random.randint(self.params['pit_width_low'], \n",
        "                                                 self.params['pit_width_high'])\n",
        "                PIT_H = self.params['pit_depth']\n",
        "                poly = [\n",
        "                    (x,              y),\n",
        "                    (x+TERRAIN_STEP, y),\n",
        "                    (x+TERRAIN_STEP, y-PIT_H*TERRAIN_STEP),\n",
        "                    (x,              y-PIT_H*TERRAIN_STEP),\n",
        "                    ]\n",
        "                self.fd_polygon.shape.vertices=poly\n",
        "                t = self.world.CreateStaticBody(\n",
        "                    fixtures = self.fd_polygon)\n",
        "                t.color1, t.color2 = (1,1,1), (0.6,0.6,0.6)\n",
        "                self.terrain.append(t)\n",
        "\n",
        "                self.fd_polygon.shape.vertices=[(p[0]+TERRAIN_STEP*counter,p[1]) for p in poly]\n",
        "                t = self.world.CreateStaticBody(\n",
        "                    fixtures = self.fd_polygon)\n",
        "                t.color1, t.color2 = (1,1,1), (0.6,0.6,0.6)\n",
        "                self.terrain.append(t)\n",
        "                counter += 2\n",
        "                original_y = y\n",
        "\n",
        "            elif state==PIT and not oneshot:\n",
        "                y = original_y\n",
        "                if counter > 1:\n",
        "                    y -= PIT_H*TERRAIN_STEP\n",
        "\n",
        "            elif state==STUMP and oneshot:\n",
        "                counter = self.np_random.randint(self.params['stump_height_low'], self.params['stump_height_high'])\n",
        "                poly = [\n",
        "                    (x,                      y),\n",
        "                    (x+counter*TERRAIN_STEP, y),\n",
        "                    (x+counter*TERRAIN_STEP, y+counter*TERRAIN_STEP),\n",
        "                    (x,                      y+counter*TERRAIN_STEP),\n",
        "                    ]\n",
        "                self.fd_polygon.shape.vertices=poly\n",
        "                t = self.world.CreateStaticBody(\n",
        "                    fixtures = self.fd_polygon)\n",
        "                t.color1, t.color2 = (1,1,1), (0.6,0.6,0.6)\n",
        "                self.terrain.append(t)\n",
        "\n",
        "            elif state==STAIRS and oneshot:\n",
        "                stair_height = self.np_random.choice(self.params['stair_heights'])\n",
        "                stair_width = self.np_random.randint(self.params['stair_width_low'], \n",
        "                                                     self.params['stair_width_high'])\n",
        "                stair_steps = self.np_random.randint(self.params['stair_steps_low'], \n",
        "                                                     self.params['stair_steps_high'])\n",
        "                original_y = y\n",
        "                for s in range(stair_steps):\n",
        "                    poly = [\n",
        "                        (x+(    s*stair_width)*TERRAIN_STEP, y+(   s*stair_height)*TERRAIN_STEP),\n",
        "                        (x+((1+s)*stair_width)*TERRAIN_STEP, y+(   s*stair_height)*TERRAIN_STEP),\n",
        "                        (x+((1+s)*stair_width)*TERRAIN_STEP, y+(-1+s*stair_height)*TERRAIN_STEP),\n",
        "                        (x+(    s*stair_width)*TERRAIN_STEP, y+(-1+s*stair_height)*TERRAIN_STEP),\n",
        "                        ]\n",
        "                    self.fd_polygon.shape.vertices=poly\n",
        "                    t = self.world.CreateStaticBody(\n",
        "                        fixtures = self.fd_polygon)\n",
        "                    t.color1, t.color2 = (1,1,1), (0.6,0.6,0.6)\n",
        "                    self.terrain.append(t)\n",
        "                counter = stair_steps*stair_width\n",
        "\n",
        "            elif state==STAIRS and not oneshot:\n",
        "                s = stair_steps*stair_width - counter - stair_height\n",
        "                n = s/stair_width\n",
        "                y = original_y + (n*stair_height)*TERRAIN_STEP\n",
        "\n",
        "            oneshot = False\n",
        "            self.terrain_y.append(y)\n",
        "            counter -= 1\n",
        "            if counter==0:\n",
        "                counter = self.np_random.randint(TERRAIN_GRASS/2, TERRAIN_GRASS)\n",
        "                if state==GRASS:\n",
        "                    state = self.np_random.choice(self.params['states'], p=self.params['state_probs'])\n",
        "                    oneshot = True\n",
        "                else:\n",
        "                    state = GRASS\n",
        "                    oneshot = True\n",
        "\n",
        "        self.terrain_poly = []\n",
        "        for i in range(TERRAIN_LENGTH-1):\n",
        "            poly = [\n",
        "                (self.terrain_x[i],   self.terrain_y[i]),\n",
        "                (self.terrain_x[i+1], self.terrain_y[i+1])\n",
        "                ]\n",
        "            self.fd_edge.shape.vertices=poly\n",
        "            t = self.world.CreateStaticBody(\n",
        "                fixtures = self.fd_edge)\n",
        "            color = (0.3, 1.0 if i%2==0 else 0.8, 0.3)\n",
        "            t.color1 = color\n",
        "            t.color2 = color\n",
        "            self.terrain.append(t)\n",
        "            color = (0.4, 0.6, 0.3)\n",
        "            poly += [ (poly[1][0], 0), (poly[0][0], 0) ]\n",
        "            self.terrain_poly.append( (poly, color) )\n",
        "        self.terrain.reverse()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rqNDNCWf-Z3",
        "colab_type": "text"
      },
      "source": [
        "## Noise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_QrGdK9gC32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AdaptiveParamNoiseSpec(object):\n",
        "    def __init__(self, initial_stddev=0.1, desired_action_stddev=0.2, adaptation_coefficient=1.01):\n",
        "        \"\"\"\n",
        "        Note that initial_stddev and current_stddev refer to std of parameter noise, \n",
        "        but desired_action_stddev refers to (as name notes) desired std in action space\n",
        "        \"\"\"\n",
        "        self.initial_stddev = initial_stddev\n",
        "        self.desired_action_stddev = desired_action_stddev\n",
        "        self.adaptation_coefficient = adaptation_coefficient\n",
        "\n",
        "        self.current_stddev = initial_stddev\n",
        "\n",
        "    def adapt(self, sq_distance):\n",
        "        \"\"\"\n",
        "        Expaects \n",
        "        \"\"\"\n",
        "        \n",
        "        if sq_distance > self.desired_action_stddev ** 2:\n",
        "            # Decrease stddev.\n",
        "            self.current_stddev /= self.adaptation_coefficient\n",
        "        else:\n",
        "            # Increase stddev.\n",
        "            self.current_stddev *= self.adaptation_coefficient\n",
        "\n",
        "    def get_stats(self):\n",
        "        stats = {\n",
        "            'param_noise_stddev': self.current_stddev,\n",
        "        }\n",
        "        return stats\n",
        "\n",
        "    def __repr__(self):\n",
        "        fmt = 'AdaptiveParamNoiseSpec(initial_stddev={}, desired_action_stddev={}, adaptation_coefficient={})'\n",
        "        return fmt.format(self.initial_stddev, self.desired_action_stddev, self.adaptation_coefficient)\n",
        "\n",
        "def ddpg_sq_distance_metric(actions1, actions2):\n",
        "    \"\"\"\n",
        "    Compute SQUARE of the \"distance\" between actions taken by two policies at the same states\n",
        "    Expects numpy arrays\n",
        "    \"\"\"\n",
        "    diff = actions1-actions2\n",
        "    mean_diff = np.mean(np.square(diff), axis=0)\n",
        "    sq_dist = np.mean(mean_diff)\n",
        "    return sq_dist\n",
        "\n",
        "\n",
        "def hard_update(target, source):\n",
        "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
        "           target_param.data.copy_(param.data)\n",
        "            \n",
        "            \n",
        "class Noise:\n",
        "    def __init__(self, dim):\n",
        "        self.dim = dim\n",
        "    \n",
        "        #normal\n",
        "    \n",
        "        #ou\n",
        "    \n",
        "    \n",
        "    def get_noise(self, n_type='normal'):\n",
        "        if n_type == 'normal':\n",
        "            return \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oC4_CZH8IET9",
        "colab_type": "text"
      },
      "source": [
        "## Replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkalC0GW0zcf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size=1e6, base_path=None, name=None):\n",
        "        self.buffer = []\n",
        "        self.max_size = int(max_size)\n",
        "        self.size = 0\n",
        "        \n",
        "        if base_path is not None:\n",
        "            self.base_path = base_path\n",
        "        else:\n",
        "            self.base_path = ''\n",
        "            \n",
        "        if name is not None:\n",
        "            self.fname = name + '.pth'\n",
        "        else:\n",
        "            self.fname = 'buffer.pth'\n",
        "            \n",
        "        \n",
        "    \n",
        "    def add(self, transition):\n",
        "        self.size +=1\n",
        "        # transiton is tuple of (state, action, reward, next_state, done)\n",
        "        self.buffer.append(transition)\n",
        "       \n",
        "    \n",
        "    def save(self):\n",
        "        try:\n",
        "            with open(self.basename + self.fname , 'wb') as f:\n",
        "                pickle.dump(self.buffer, f)\n",
        "        except OSError:\n",
        "            print('Buffer is not saved!\\n\\n')\n",
        "            \n",
        "            \n",
        "    def load(self):\n",
        "        try:\n",
        "            with open(self.basename + self.fname , 'rb') as f:\n",
        "                self.buffer = pickle.load(f)\n",
        "        except OSError:\n",
        "            self.buffer = []\n",
        "            print('Buffer is not loaded!\\n\\n')\n",
        "    \n",
        "          \n",
        "    def sample(self, batch_size):\n",
        "        # delete 1/5th of the buffer when full\n",
        "        if self.size > self.max_size:\n",
        "            del self.buffer[0:int(self.size/5)]\n",
        "            self.size = len(self.buffer)\n",
        "        \n",
        "        indexes = np.random.randint(0, len(self.buffer), size=batch_size)\n",
        "        state, action, reward, next_state, done = [], [], [], [], []\n",
        "        \n",
        "        for i in indexes:\n",
        "            s, a, r, s_, d = self.buffer[i]\n",
        "            state.append(np.array(s, copy=False))\n",
        "            action.append(np.array(a, copy=False))\n",
        "            reward.append(np.array(r, copy=False))\n",
        "            next_state.append(np.array(s_, copy=False))\n",
        "            done.append(np.array(d, copy=False))\n",
        "        \n",
        "        return np.array(state), np.array(action), np.array(reward), np.array(next_state), np.array(done)\n",
        "     \n",
        "    def get_last(self, amount):\n",
        "        state, action, reward, next_state, done = [], [], [], [], []\n",
        "        if amount < len(self.buffer):\n",
        "            start = len(self.buffer) - amount\n",
        "        else:\n",
        "            start = 0\n",
        "        for i in range(start,len(self.buffer)):\n",
        "            s, a, r, s_, d = self.buffer[i]\n",
        "            state.append(np.array(s, copy=False))\n",
        "            action.append(np.array(a, copy=False))\n",
        "            reward.append(np.array(r, copy=False))\n",
        "            next_state.append(np.array(s_, copy=False))\n",
        "            done.append(np.array(d, copy=False))\n",
        "        return s, a, r, s_, d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61VeKABEm9_O",
        "colab_type": "text"
      },
      "source": [
        "## Trajectory buffer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CEo4_gcm-kh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import deque\n",
        "class TrajectoriesEndBuffer():\n",
        "    def __init__(self, max_trajectory_size=10, init_prob=0.7):\n",
        "        assert type(max_trajectory_size) is int\n",
        "        assert max_trajectory_size > 0\n",
        "        \n",
        "        self.max_trajectory_size = max_trajectory_size\n",
        "        self.buffer = deque()\n",
        "        \n",
        "        #linear probability params\n",
        "        assert type(init_prob) is float\n",
        "        assert 0 <= init_prob <= 1\n",
        "        self.min_prob = init_prob\n",
        "        \n",
        "        #TRY other cases\n",
        "        \n",
        "    def add(self, transition):\n",
        "        if len(self.buffer) >= self.max_trajectory_size:\n",
        "            self.buffer.popleft()    \n",
        "        self.buffer.append(transition)\n",
        "    \n",
        "    \n",
        "    def get_heruistc_probabilities(self):\n",
        "        cur_len = len(self.buffer)\n",
        "        if cur_len > 0 and self.max_trajectory_size == 1:\n",
        "            return [1]\n",
        "\n",
        "        if cur_len < self.max_trajectory_size:\n",
        "            shift = self.max_trajectory_size - cur_len\n",
        "        else:\n",
        "            shift = 0\n",
        "        prob = [0] * cur_len\n",
        "        \n",
        "        # linear from\n",
        "        min_prob = self.min_prob\n",
        "        for i in range(cur_len):\n",
        "            prob[i] = min_prob + (1 - min_prob) * ( (i + shift) / (self.max_trajectory_size - 1) )\n",
        "            \n",
        "        return prob\n",
        "    \n",
        "    def transfer_to_replay_buffer(self, replay_buffer):\n",
        "        if len(self.buffer) == 0: \n",
        "            print('Trajectories buffer is empty')\n",
        "            return\n",
        "        \n",
        "        if self.buffer[-1][4] == False:\n",
        "            print('Trajectory is not finished')\n",
        "            return\n",
        "          \n",
        "        cur_len = len(self.buffer)\n",
        "        prob = self.get_heruistc_probabilities()\n",
        "        \n",
        "        for i in range(cur_len):\n",
        "            s1, a, r, s2, done = self.buffer[i]\n",
        "            replay_buffer.add((s1, a, r, s2, float(prob[i])))\n",
        "        \n",
        "        self.buffer = deque()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEEchoyDrpAR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzz69TYi7f2q",
        "colab_type": "text"
      },
      "source": [
        "## Research class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqj3YyUT7ebB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random \n",
        "class Research:\n",
        "    def set_danger_batch_size(self):\n",
        "        self.batch_size_danger = self.batch_size // 2\n",
        "        \n",
        "    def update_params(self, params):\n",
        "        #print(params)\n",
        "        param_names = ['random_seed', \n",
        "                       'max_episodes', 'max_timesteps', 'batch_size', \n",
        "                       'lr', 'polyak', 'policy_delay', 'gamma', 'log_interval', \n",
        "                       'batch_size_danger', \n",
        "                       'policy_noise',\n",
        "                       'danger_enable', 'danger_threshold', 'max_trajectory_size',\n",
        "                       'directory','filename_load', 'filename_save', 'epochs_for_danger',\n",
        "                       'noise_random_enable', 'exploration_noise', 'noise_clip',\n",
        "                       'max_iter_danger', 'evaluate_after_episodes', 'evaluate_average_on_episodes', 'env_name_load', \n",
        "                       'env_name',\n",
        "                       'param_noise_enable','initial_stddev','desired_action_stddev','adaptation_coefficient']\n",
        "        \n",
        "        for param_name in param_names:\n",
        "            if param_name in params:\n",
        "                \n",
        "                setattr(self, param_name, params[param_name])\n",
        "                print(\"{} set to {}\".format(param_name, params[param_name]))\n",
        "        \n",
        "        \n",
        "    def __init__(self, params):\n",
        "        ###########################################\n",
        "        ###         default values\n",
        "        ###########################################\n",
        "        \n",
        "        self.random_seed = 1\n",
        "        \n",
        "        # environment\n",
        "        self.env_rewards = {'BipedalWalkerHardcore-v2':[-100,300], 'BipedalWalker-v2':[-100,300],\n",
        "                            'LunarLanderContinuous-v2':[-100,200],'Pendulum-v0':[-1000,200]}\n",
        "\n",
        "        self.env_name_load = 'BipedalWalker-v2'\n",
        "        self.env_name = 'BipedalWalker-v2'\n",
        "        \n",
        "        if self.env_name in self.env_rewards:\n",
        "            self.terminal_reward = self.env_rewards[self.env_name][0]\n",
        "            self.passed_reward   = self.env_rewards[self.env_name][1]\n",
        "        else:\n",
        "            self.terminal_reward = None\n",
        "            self.passed_reward   = None\n",
        "        \n",
        "        self.env = None\n",
        "        \n",
        "        ## training \n",
        "        self.max_episodes = 1000         # max num of episodes\n",
        "        self.max_timesteps = 2000        # max timesteps in one episode\n",
        "        self.batch_size = 100            # num of transitions sampled from replay buffer\n",
        "        self.lr = 0.001\n",
        "        self.polyak = 0.995              # target policy update parameter (1-tau)\n",
        "        self.policy_delay = 2            # delayed policy updates parameter\n",
        "        \n",
        "        ## model params\n",
        "        self.gamma = 0.99                # discount for future rewards\n",
        "        self.warmup_steps = 10**4\n",
        "        \n",
        "        ##############################   \n",
        "        ##   Noise params\n",
        "        ##############################   \n",
        "        \n",
        "        # Random noise\n",
        "        self.noise_random_enable = True\n",
        "        self.exploration_noise = 0.1\n",
        "        self.policy_noise = 0.2          # target policy smoothing noise\n",
        "        self.noise_clip = 0.5\n",
        "        \n",
        "        # Param noise\n",
        "        self.param_noise_enable = False\n",
        "        self.noise_distance_batch_size = 10 ** 4\n",
        "        self.initial_stddev=0.05\n",
        "        self.desired_action_stddev=0.45 \n",
        "        self.adaptation_coefficient=1.03\n",
        "        \n",
        "        ## Danger params\n",
        "        self.danger_enable = True               # enable dnager mode\n",
        "        self.danger_threshold = 0.7      # probability to perform safe action\n",
        "        self.epochs_for_danger = 3       # epochs to train danger\n",
        "        self.max_iter_danger = 5         # iteration per iteration\n",
        "        self.batch_size_danger = self.batch_size // 2\n",
        "        self.max_trajectory_size = 10    # length of trajectory before death\n",
        "        self.action_updates = 0\n",
        "        \n",
        "        ## evaluation \n",
        "        self.evaluate_average_on_episodes = 10\n",
        "        self.evaluate_after_episodes = 30\n",
        "        self.evaluate_after_timesteps = 10000\n",
        "        \n",
        "        ## InpOut params\n",
        "        self.log_interval = 10                                              # print avg reward after interval\n",
        "        self.directory = \"/content/drive/My Drive\"                          # save trained models\n",
        "        self.filename_load = \"TD3_{}_{}\".format(self.env_name_load, self.random_seed)\n",
        "        self.filename_save = \"TD3_{}_{}\".format(self.env_name, self.random_seed)\n",
        "        ###########################################\n",
        "        \n",
        "        ## Buffers\n",
        "        self.replay_buffer = None\n",
        "        self.replay_buffer_dead = None\n",
        "        self.trajectory_buffer = None\n",
        "        \n",
        "        \n",
        "        ## Policy\n",
        "        self.policy = None\n",
        "        \n",
        "        ###########################################\n",
        "        ## Update default params\n",
        "        ###########################################\n",
        "        self.update_params(params)\n",
        "        \n",
        "        \n",
        "    def load_model(self, env_name, random_seed):\n",
        "        pass\n",
        "        \n",
        "        \n",
        "    def init_env(self, params = None):\n",
        "        if params:\n",
        "            self.update_env(params)\n",
        "            \n",
        "        # create env\n",
        "        if self.env_name == \"BipedalWalkerHardcore-v2\":\n",
        "            \n",
        "            env = CustomizableBipedalWalker()\n",
        "            env.set_env_params(stump_height=1)\n",
        "            env.set_env_states(state_mask=np.array([1,1,0,0],dtype=bool), p=np.array([0.1,0.9,0.9,0.9]))\n",
        "            \n",
        "            self.env = TimeLimit(env, max_episode_steps=2000)\n",
        "        else:\n",
        "            self.env = gym.make(self.env_name)\n",
        "            \n",
        "        if '_max_episode_steps' in self.env.__dict__:\n",
        "            self.max_timesteps = self.env.__dict__['_max_episode_steps']\n",
        "        else:\n",
        "            self.max_timesteps = 10**7 ## \"inifinte\" value if nothing defined\n",
        "        \n",
        "        # env params\n",
        "        self.state_dim = self.env.observation_space.shape[0]\n",
        "        self.action_dim = self.env.action_space.shape[0]\n",
        "        self.max_action = float(self.env.action_space.high[0])\n",
        "        \n",
        "        #check symmetry of actions\n",
        "        assert  np.all((-self.env.action_space.low) == self.env.action_space.high)\n",
        "        \n",
        "        \n",
        "    def init_buffers_and_agent(self, load, solved):\n",
        "        \n",
        "        # Buffers\n",
        "        self.replay_buffer = ReplayBuffer()\n",
        "        if self.danger_enable:\n",
        "            self.replay_buffer_dead = ReplayBuffer()\n",
        "            self.trajectory_buffer = TrajectoriesEndBuffer(self.max_trajectory_size)\n",
        "    \n",
        "        # Policy\n",
        "        self.policy = TD3(self.lr, self.state_dim, self.action_dim, self.max_action,  self.danger_enable,\n",
        "                          self.danger_threshold, self.directory, self.filename_save, self.epochs_for_danger)\n",
        "    \n",
        "    \n",
        "        # load\n",
        "        if load:\n",
        "            full_fname = self.filename_load\n",
        "            if solved:\n",
        "                full_fname += \"_solved\"\n",
        "            self.policy.load(self.directory,  full_fname)\n",
        "           \n",
        "        \n",
        "    def train(self, params, debug=False):\n",
        "        self.update_params(params)\n",
        "        self.first_finish_ep_num  = -1\n",
        "        assert self.policy is not None\n",
        "        assert self.max_timesteps is not None\n",
        "        \n",
        "        # Random seed\n",
        "        if self.random_seed is not None:\n",
        "            os.environ['PYTHONHASHSEED']=str(self.random_seed)\n",
        "            print(\"Random Seed: {}\".format(self.random_seed))\n",
        "            np.random.seed(self.random_seed)\n",
        "            self.env.seed(self.random_seed)\n",
        "            random.seed(self.random_seed)\n",
        "            torch.manual_seed(self.random_seed)\n",
        "         \n",
        "    \n",
        "        # logging variables:\n",
        "        avg_reward = 0\n",
        "        ep_reward = 0\n",
        "        tot_time_steps = 0\n",
        "        average_time_steps = 0\n",
        "    \n",
        "        log_f = open(\"log.txt\",\"w+\")\n",
        "    \n",
        "        # training procedure:\n",
        "        print('Start training')\n",
        "        reach_the_end = False\n",
        "        \n",
        "        param_noise = AdaptiveParamNoiseSpec(initial_stddev=0.05,desired_action_stddev=0.45, adaptation_coefficient=1.05)\n",
        "        # amount of updates in dangers\n",
        "        tot_updates_amount = 0\n",
        "        self.action_updates = 0\n",
        "        ep_updates_amount = 0\n",
        "\n",
        "        \n",
        "        \n",
        "        for episode in range(1, self.max_episodes + 1):\n",
        "            \n",
        "            ep_updates_amount = 0\n",
        "            state = self.env.reset()        \n",
        "            \n",
        "            if  self.param_noise_enable:\n",
        "                self.policy.perturb_actor_parameters(param_noise)\n",
        "            \n",
        "            for t in range(self.max_timesteps):\n",
        "                # select action and add exploration noise:\n",
        "                \n",
        "                if tot_time_steps + t <= self.warmup_steps:\n",
        "                    action = self.env.action_space.sample()\n",
        "                else:\n",
        "                    action = self.policy.select_action(state, danger=self.danger_enable, param_noise=self.param_noise_enable)\n",
        "                    if self.policy.action_update:\n",
        "                        ep_updates_amount += 1\n",
        "                    \n",
        "                    if self.noise_random_enable:\n",
        "                        action = action + np.random.normal(0, self.exploration_noise, size=self.env.action_space.shape[0])\n",
        "                    action = action.clip(self.env.action_space.low, self.env.action_space.high)\n",
        "                    \n",
        "                #print(\"action {}\".format(action))\n",
        "                # take action in env:\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "            \n",
        "                self.replay_buffer.add((state, action, reward, next_state, float(done)))\n",
        "                if self.danger_enable:\n",
        "                    self.trajectory_buffer.add((state, action, reward, next_state, float(done)))\n",
        "            \n",
        "                if reward == self.terminal_reward:\n",
        "\n",
        "                    if self.danger_enable:\n",
        "                        self.trajectory_buffer.transfer_to_replay_buffer(self.replay_buffer_dead)                    \n",
        "                    \n",
        "                        if debug:\n",
        "                            tmp_st = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
        "                            tmp_a = torch.FloatTensor(action.reshape(1, -1)).to(device)\n",
        "                            print(\"\\tDead after pair with prob: \", self.policy.critic_danger(tmp_st, tmp_a).cpu().data.numpy().flatten())\n",
        "                    \n",
        "                state = next_state\n",
        "                avg_reward += reward\n",
        "                ep_reward += reward\n",
        "                \n",
        "                #if done and t < self.max_timesteps - 1 and reward != self.terminal_reward and reach_the_end==False:\n",
        "                #    print('Reach the end. Episode: {}. r={} t={}'.format(episode, ep_reward, tot_time_steps + t))\n",
        "                #    reach_the_end = True\n",
        "                #    self.first_finish_ep_num = episode\n",
        "                #    break\n",
        "                \n",
        "                if done or t==(self.max_timesteps-1): \n",
        "                    tot_time_steps += t\n",
        "                    average_time_steps += t\n",
        "                    tot_updates_amount += ep_updates_amount \n",
        "                    self.action_updates += ep_updates_amount\n",
        "\n",
        "                    self.policy.update(self.replay_buffer, self.replay_buffer_dead, t, self.batch_size, self.batch_size_danger, self.gamma, self.polyak, self.policy_noise, self.noise_clip, self.policy_delay)\n",
        "                    if t==(self.max_timesteps - 1):                    \n",
        "                        print(\"\\tStuck, ep reward {}\".format(ep_reward))\n",
        "                    break\n",
        "            \n",
        "            # logging updates:\n",
        "            log_f.write('{},{}\\n'.format(episode, ep_reward))\n",
        "            log_f.flush()\n",
        "            ep_reward = 0\n",
        "            \n",
        "            # if avg reward > 300 then save and stop traning:\n",
        "            if (avg_reward / self.log_interval) >= 300:\n",
        "                print(\"########## Solved! ###########\")\n",
        "                name = self.filename_save + '_solved'\n",
        "                self.policy.save(self.directory, name, optimizers=True, danger=True)\n",
        "                log_f.close()\n",
        "                break\n",
        "            \n",
        "            # update perturbed actor\n",
        "            #print('start perturtbated update')\n",
        "            if self.param_noise_enable:\n",
        "                states, actions_perturbed, _, _, _ = self.replay_buffer.get_last(t)\n",
        "                actions_perturbed = np.array(actions_perturbed, copy=False)\n",
        "            \n",
        "                actions_clear = self.policy.select_action(states, danger=False, param_noise=False)\n",
        "                ddpg_sq_distance = ddpg_sq_distance_metric(actions_perturbed, actions_clear)\n",
        "                param_noise.adapt(ddpg_sq_distance)\n",
        "            #print('end perturtbated update t={}'.format(t))\n",
        "            \n",
        "            # save policy \n",
        "            if episode > 0 and episode % (5 * self.log_interval) == 0:\n",
        "                self.policy.save(self.directory, self.filename_save, optimizers=True, danger=True)\n",
        "            \n",
        "            # print avg reward every log interval:\n",
        "            if episode % self.log_interval == 0:\n",
        "                avg_reward = int(avg_reward / self.log_interval)\n",
        "                average_time_steps = int(average_time_steps / self.log_interval)\n",
        "                print(\"Episode: {}\\tAverage Reward: {}\\tAverage steps: {}\\t Total: {}\".format(episode, avg_reward, average_time_steps, tot_time_steps))\n",
        "                avg_reward = 0\n",
        "                average_time_steps = 0\n",
        "            \n",
        "            if episode % self.evaluate_after_episodes == 0 and episode >= 100:\n",
        "                reach_the_end = self.evaluate(params ={}, time_steps = tot_time_steps, episode = episode)\n",
        "                if reach_the_end:\n",
        "                    self.first_finish_ep_num = episode\n",
        "                    print('Reach the end. Episode: {}. r={:.1f} Steps={} Updated steps={}'.format(episode, ep_reward, tot_time_steps, self.action_updates))\n",
        "                    break\n",
        "    \n",
        "    def evaluate(self, params, time_steps = None, episode = None):\n",
        "        self.update_params(params)\n",
        "        print('Evaluation. Timesteps: {} Episodes: {}'.format(time_steps, episode))\n",
        "        reach_the_end = 0\n",
        "        rewards = np.zeros(self.evaluate_average_on_episodes)\n",
        "        for ep in range(self.evaluate_average_on_episodes):\n",
        "            ep_reward = 0\n",
        "            state = self.env.reset()\n",
        "            for t in range(self.max_timesteps):\n",
        "                action = self.policy.select_action(state, debug=False, danger=True)\n",
        "                state, reward, done, _ = self.env.step(action)\n",
        "                ep_reward += reward\n",
        "                if done:\n",
        "                    s = \"\\t\\tDead {}\\tUpdate {}\".format( reward == self.terminal_reward,  self.policy.action_update)\n",
        "                    \n",
        "                    if reward != self.terminal_reward and t !=self.max_timesteps-1:\n",
        "                        print(s+'\\tFinished')\n",
        "                        reach_the_end += 1\n",
        "                    else:\n",
        "                        print(s)\n",
        "                    break\n",
        "\n",
        "            rewards[ep] = ep_reward \n",
        "            \n",
        "        print('\\t\\tMean reward: {}. All rewards: {}'.format(int(np.mean(rewards)), list(map(int,rewards)) ))\n",
        "        return reach_the_end == self.evaluate_average_on_episodes\n",
        "    \n",
        "    \n",
        "    def is_passed(self, step_num, reward):\n",
        "        if reward != self.terminal_reward:\n",
        "            if step_num < self.terminal_reward:\n",
        "                return True\n",
        "        return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC-3p-bFIcMg",
        "colab_type": "text"
      },
      "source": [
        "# Debug\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx4nESZqVJ9q",
        "colab_type": "text"
      },
      "source": [
        "## Test Research class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsFU93i6VOFo",
        "colab_type": "code",
        "outputId": "f8d9b595-b1c0-4947-9b95-2cbc1b98736f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "params = {'env_name': \"BipedalWalker-v2\",\n",
        "          'random_seed':19152527, \n",
        "          'danger_enable':True,\n",
        "          'max_trajectory_size' : 10,\n",
        "          'evaluate_after_episodes' : 5,\n",
        "          'evaluate_average_on_episodes' : 5,\n",
        "          #'max_timesteps':1000,\n",
        "          'max_episodes' : 1000,}\n",
        "new_research = Research(params)\n",
        "\n",
        "new_research.init_env()\n",
        "new_research.init_buffers_and_agent(load=False, solved=False)\n",
        "# prime = { 14245223, 21192173, 33535573, 285781, 332447659, 19152527}\n",
        "#  'max_timesteps':10,\n",
        "print(new_research.max_timesteps)\n",
        "new_research.train({}, debug=True)\n",
        "print(new_research.action_updates)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'env_name': 'BipedalWalker-v2', 'random_seed': 19152527, 'danger_enable': True, 'max_trajectory_size': 10, 'evaluate_after_episodes': 5, 'evaluate_average_on_episodes': 5, 'max_episodes': 1000}\n",
            "random_seed set to 19152527\n",
            "max_episodes set to 1000\n",
            "danger_enable set to True\n",
            "max_trajectory_size set to 10\n",
            "evaluate_after_episodes set to 5\n",
            "evaluate_average_on_episodes set to 5\n",
            "env_name set to BipedalWalker-v2\n",
            "1600\n",
            "{}\n",
            "Random Seed: 19152527\n",
            "Start training\n",
            "\tDead after pair with prob:  [0.49458262]\n",
            "\tStuck, ep reward -83.1391156426803\n",
            "\tStuck, ep reward -84.26254670211733\n",
            "\tStuck, ep reward -89.37516646568369\n",
            "\tDead after pair with prob:  [0.4811209]\n",
            "\tDead after pair with prob:  [0.4699037]\n",
            "\tDead after pair with prob:  [0.48621106]\n",
            "\tStuck, ep reward -88.41914543146048\n",
            "\tDead after pair with prob:  [0.46752092]\n",
            "\tDead after pair with prob:  [0.41021678]\n",
            "Episode: 10\tAverage Reward: -99\tAverage steps: 687\t Total: 6875\n",
            "\tDead after pair with prob:  [0.4688238]\n",
            "\tDead after pair with prob:  [0.41013598]\n",
            "\tStuck, ep reward -81.45565131091509\n",
            "\tDead after pair with prob:  [0.45328122]\n",
            "\tDead after pair with prob:  [0.44839996]\n",
            "\tDead after pair with prob:  [0.4467378]\n",
            "\tDead after pair with prob:  [0.4106465]\n",
            "\tDead after pair with prob:  [0.5584293]\n",
            "\tDead after pair with prob:  [0.484226]\n",
            "\tDead after pair with prob:  [0.54512817]\n",
            "Episode: 20\tAverage Reward: -106\tAverage steps: 224\t Total: 9121\n",
            "\tDead after pair with prob:  [0.6811457]\n",
            "\tDead after pair with prob:  [0.24955907]\n",
            "\tDead after pair with prob:  [0.3157679]\n",
            "\tDead after pair with prob:  [0.23187457]\n",
            "\tDead after pair with prob:  [0.2529697]\n",
            "\tStuck, ep reward -182.2485489602962\n",
            "\tStuck, ep reward -167.05712195359507\n",
            "\tDead after pair with prob:  [0.9767576]\n",
            "\tDead after pair with prob:  [0.9713478]\n",
            "\tDead after pair with prob:  [0.8922822]\n",
            "Episode: 30\tAverage Reward: -164\tAverage steps: 673\t Total: 15855\n",
            "\tDead after pair with prob:  [0.9747673]\n",
            "\tDead after pair with prob:  [0.8742829]\n",
            "\tDead after pair with prob:  [0.77378684]\n",
            "\tDead after pair with prob:  [0.78599745]\n",
            "\tDead after pair with prob:  [0.8069233]\n",
            "\tDead after pair with prob:  [0.9046002]\n",
            "\tDead after pair with prob:  [0.9288989]\n",
            "\tDead after pair with prob:  [0.92886907]\n",
            "\tDead after pair with prob:  [0.94151664]\n",
            "\tDead after pair with prob:  [0.9006913]\n",
            "Episode: 40\tAverage Reward: -129\tAverage steps: 78\t Total: 16642\n",
            "\tDead after pair with prob:  [0.9701627]\n",
            "\tDead after pair with prob:  [0.9657955]\n",
            "\tDead after pair with prob:  [0.8889689]\n",
            "\tDead after pair with prob:  [0.8138391]\n",
            "\tDead after pair with prob:  [0.93449765]\n",
            "\tDead after pair with prob:  [0.93348825]\n",
            "\tDead after pair with prob:  [0.93937105]\n",
            "\tDead after pair with prob:  [0.953711]\n",
            "\tDead after pair with prob:  [0.949019]\n",
            "\tDead after pair with prob:  [0.99206185]\n",
            "Episode: 50\tAverage Reward: -129\tAverage steps: 83\t Total: 17477\n",
            "\tDead after pair with prob:  [0.99237263]\n",
            "\tDead after pair with prob:  [0.93765754]\n",
            "\tDead after pair with prob:  [0.96264863]\n",
            "\tDead after pair with prob:  [0.9748736]\n",
            "\tDead after pair with prob:  [0.7427249]\n",
            "\tDead after pair with prob:  [0.93755496]\n",
            "\tDead after pair with prob:  [0.96211916]\n",
            "\tDead after pair with prob:  [0.00866475]\n",
            "\tDead after pair with prob:  [0.9913139]\n",
            "\tDead after pair with prob:  [0.98943114]\n",
            "Episode: 60\tAverage Reward: -134\tAverage steps: 245\t Total: 19933\n",
            "\tDead after pair with prob:  [0.07328242]\n",
            "\tStuck, ep reward -113.32013895004569\n",
            "\tStuck, ep reward -119.88021097352114\n",
            "\tDead after pair with prob:  [0.9934999]\n",
            "\tDead after pair with prob:  [0.99257904]\n",
            "\tStuck, ep reward -126.31318701836359\n",
            "\tDead after pair with prob:  [0.9525725]\n",
            "\tStuck, ep reward -167.9025627812756\n",
            "\tDead after pair with prob:  [0.6936559]\n",
            "\tDead after pair with prob:  [0.10839153]\n",
            "Episode: 70\tAverage Reward: -134\tAverage steps: 816\t Total: 28102\n",
            "\tDead after pair with prob:  [0.99086833]\n",
            "\tStuck, ep reward -145.58337297560573\n",
            "\tDead after pair with prob:  [0.98537934]\n",
            "\tStuck, ep reward -107.69826997200309\n",
            "\tStuck, ep reward -102.62340894673962\n",
            "\tStuck, ep reward -136.8759872720753\n",
            "\tStuck, ep reward -173.8485421137088\n",
            "\tStuck, ep reward -95.50166942358521\n",
            "\tStuck, ep reward -87.05101566453759\n",
            "\tStuck, ep reward -161.45095012762835\n",
            "Episode: 80\tAverage Reward: -127\tAverage steps: 1326\t Total: 41371\n",
            "\tDead after pair with prob:  [0.28629512]\n",
            "\tDead after pair with prob:  [0.97229856]\n",
            "\tDead after pair with prob:  [0.9688807]\n",
            "\tDead after pair with prob:  [0.9948773]\n",
            "\tStuck, ep reward -85.8031187788275\n",
            "\tDead after pair with prob:  [0.93956214]\n",
            "\tStuck, ep reward -83.22991230153151\n",
            "\tDead after pair with prob:  [0.93871146]\n",
            "\tDead after pair with prob:  [0.9248864]\n",
            "\tDead after pair with prob:  [0.85122037]\n",
            "Episode: 90\tAverage Reward: -116\tAverage steps: 460\t Total: 45973\n",
            "\tStuck, ep reward -103.03115484967769\n",
            "\tStuck, ep reward -88.86488767326816\n",
            "\tStuck, ep reward -85.34993158852592\n",
            "\tStuck, ep reward -58.3122049986787\n",
            "\tStuck, ep reward -105.20410267490233\n",
            "\tStuck, ep reward -146.84194745076368\n",
            "\tDead after pair with prob:  [0.76042575]\n",
            "\tStuck, ep reward -86.56573553196552\n",
            "\tStuck, ep reward -68.91823432453842\n",
            "\tStuck, ep reward -95.68120040174668\n",
            "Episode: 100\tAverage Reward: -95\tAverage steps: 1446\t Total: 60435\n",
            "{}\n",
            "Evaluation. Timesteps: 60435 Episodes: 100\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -87. All rewards: [-85, -85, -84, -88, -93]\n",
            "\tStuck, ep reward -86.67322075731438\n",
            "\tStuck, ep reward -96.14630122463964\n",
            "\tStuck, ep reward -98.20080523225353\n",
            "\tStuck, ep reward -100.35346576828451\n",
            "\tStuck, ep reward -88.07906018227571\n",
            "{}\n",
            "Evaluation. Timesteps: 68430 Episodes: 105\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -94. All rewards: [-98, -103, -64, -100, -106]\n",
            "\tStuck, ep reward -64.53094629979924\n",
            "\tStuck, ep reward -52.77618274239969\n",
            "\tDead after pair with prob:  [0.70632684]\n",
            "\tStuck, ep reward -96.92445407750604\n",
            "\tStuck, ep reward -112.3439930372597\n",
            "Episode: 110\tAverage Reward: -92\tAverage steps: 1447\t Total: 74912\n",
            "{}\n",
            "Evaluation. Timesteps: 74912 Episodes: 110\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -140. All rewards: [-140, -138, -145, -137, -141]\n",
            "\tStuck, ep reward -148.57281903437843\n",
            "\tStuck, ep reward -113.63906382105714\n",
            "\tDead after pair with prob:  [0.96446425]\n",
            "\tDead after pair with prob:  [0.977728]\n",
            "\tDead after pair with prob:  [0.96505827]\n",
            "{}\n",
            "Evaluation. Timesteps: 78409 Episodes: 115\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -130. All rewards: [-126, -132, -127, -131, -134]\n",
            "\tDead after pair with prob:  [0.9871159]\n",
            "\tDead after pair with prob:  [0.9793108]\n",
            "\tDead after pair with prob:  [0.98016447]\n",
            "\tDead after pair with prob:  [0.99113244]\n",
            "\tDead after pair with prob:  [0.8493668]\n",
            "Episode: 120\tAverage Reward: -127\tAverage steps: 393\t Total: 78851\n",
            "{}\n",
            "Evaluation. Timesteps: 78851 Episodes: 120\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -127. All rewards: [-126, -131, -121, -130, -126]\n",
            "\tDead after pair with prob:  [0.8973798]\n",
            "\tDead after pair with prob:  [0.9165798]\n",
            "\tDead after pair with prob:  [0.8748208]\n",
            "\tDead after pair with prob:  [0.97179246]\n",
            "\tDead after pair with prob:  [0.8877522]\n",
            "{}\n",
            "Evaluation. Timesteps: 79236 Episodes: 125\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -121. All rewards: [-121, -121, -122, -121, -121]\n",
            "\tDead after pair with prob:  [0.8650624]\n",
            "\tDead after pair with prob:  [0.9439917]\n",
            "\tDead after pair with prob:  [0.88665336]\n",
            "\tDead after pair with prob:  [0.92938054]\n",
            "\tDead after pair with prob:  [0.91860044]\n",
            "Episode: 130\tAverage Reward: -123\tAverage steps: 71\t Total: 79562\n",
            "{}\n",
            "Evaluation. Timesteps: 79562 Episodes: 130\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -117. All rewards: [-122, -117, -117, -114, -114]\n",
            "\tDead after pair with prob:  [0.97810787]\n",
            "\tDead after pair with prob:  [0.8557538]\n",
            "\tDead after pair with prob:  [0.7646809]\n",
            "\tDead after pair with prob:  [0.93772507]\n",
            "\tDead after pair with prob:  [0.90865916]\n",
            "{}\n",
            "Evaluation. Timesteps: 79971 Episodes: 135\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -132. All rewards: [-137, -133, -129, -133, -129]\n",
            "\tDead after pair with prob:  [0.8643143]\n",
            "\tDead after pair with prob:  [0.9182985]\n",
            "\tDead after pair with prob:  [0.9620764]\n",
            "\tDead after pair with prob:  [0.89564466]\n",
            "\tDead after pair with prob:  [0.95821]\n",
            "Episode: 140\tAverage Reward: -126\tAverage steps: 94\t Total: 80504\n",
            "{}\n",
            "Evaluation. Timesteps: 80504 Episodes: 140\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tMean reward: -139. All rewards: [-139, -138, -140, -139, -138]\n",
            "\tDead after pair with prob:  [0.05303711]\n",
            "\tDead after pair with prob:  [0.95039153]\n",
            "\tDead after pair with prob:  [0.87330145]\n",
            "\tDead after pair with prob:  [0.90286857]\n",
            "\tDead after pair with prob:  [0.862048]\n",
            "{}\n",
            "Evaluation. Timesteps: 81093 Episodes: 145\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -118. All rewards: [-118, -118, -118, -118, -118]\n",
            "\tDead after pair with prob:  [0.93638045]\n",
            "\tDead after pair with prob:  [0.8876159]\n",
            "\tDead after pair with prob:  [0.96835864]\n",
            "\tDead after pair with prob:  [0.9015118]\n",
            "\tDead after pair with prob:  [0.890388]\n",
            "Episode: 150\tAverage Reward: -127\tAverage steps: 99\t Total: 81503\n",
            "{}\n",
            "Evaluation. Timesteps: 81503 Episodes: 150\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -117. All rewards: [-122, -115, -116, -116, -115]\n",
            "\tDead after pair with prob:  [0.99880505]\n",
            "\tDead after pair with prob:  [0.98055536]\n",
            "\tDead after pair with prob:  [0.98507047]\n",
            "\tDead after pair with prob:  [0.97781914]\n",
            "\tDead after pair with prob:  [0.92015845]\n",
            "{}\n",
            "Evaluation. Timesteps: 81816 Episodes: 155\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -122. All rewards: [-122, -123, -123, -121, -121]\n",
            "\tDead after pair with prob:  [0.9315098]\n",
            "\tDead after pair with prob:  [0.9763243]\n",
            "\tDead after pair with prob:  [0.82942253]\n",
            "\tDead after pair with prob:  [0.93772316]\n",
            "\tDead after pair with prob:  [0.9784541]\n",
            "Episode: 160\tAverage Reward: -120\tAverage steps: 66\t Total: 82166\n",
            "{}\n",
            "Evaluation. Timesteps: 82166 Episodes: 160\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -120. All rewards: [-119, -119, -119, -121, -121]\n",
            "\tDead after pair with prob:  [0.960606]\n",
            "\tDead after pair with prob:  [0.91616315]\n",
            "\tDead after pair with prob:  [0.95087093]\n",
            "\tDead after pair with prob:  [0.9856673]\n",
            "\tDead after pair with prob:  [0.9809281]\n",
            "{}\n",
            "Evaluation. Timesteps: 82533 Episodes: 165\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -120. All rewards: [-118, -119, -121, -121, -119]\n",
            "\tDead after pair with prob:  [0.93790853]\n",
            "\tDead after pair with prob:  [0.9078864]\n",
            "\tDead after pair with prob:  [0.8404712]\n",
            "\tDead after pair with prob:  [0.8823811]\n",
            "\tDead after pair with prob:  [0.933581]\n",
            "Episode: 170\tAverage Reward: -122\tAverage steps: 74\t Total: 82909\n",
            "{}\n",
            "Evaluation. Timesteps: 82909 Episodes: 170\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -129. All rewards: [-126, -129, -129, -133, -129]\n",
            "\tDead after pair with prob:  [0.9967637]\n",
            "\tDead after pair with prob:  [0.9218436]\n",
            "\tDead after pair with prob:  [0.9700992]\n",
            "\tStuck, ep reward -116.0275781736666\n",
            "\tDead after pair with prob:  [0.97229576]\n",
            "{}\n",
            "Evaluation. Timesteps: 84841 Episodes: 175\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -124. All rewards: [-124, -126, -124, -125, -121]\n",
            "\tDead after pair with prob:  [0.9878108]\n",
            "\tDead after pair with prob:  [0.98002166]\n",
            "\tDead after pair with prob:  [0.9327804]\n",
            "\tDead after pair with prob:  [0.9540217]\n",
            "\tDead after pair with prob:  [0.95881814]\n",
            "Episode: 180\tAverage Reward: -127\tAverage steps: 236\t Total: 85273\n",
            "{}\n",
            "Evaluation. Timesteps: 85273 Episodes: 180\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -123. All rewards: [-124, -126, -124, -119, -119]\n",
            "\tDead after pair with prob:  [0.96348375]\n",
            "\tDead after pair with prob:  [0.9598433]\n",
            "\tDead after pair with prob:  [0.9753567]\n",
            "\tDead after pair with prob:  [0.8180267]\n",
            "\tDead after pair with prob:  [0.95343065]\n",
            "{}\n",
            "Evaluation. Timesteps: 85700 Episodes: 185\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -127. All rewards: [-127, -127, -127, -128, -127]\n",
            "\tDead after pair with prob:  [0.9517531]\n",
            "\tDead after pair with prob:  [0.958311]\n",
            "\tDead after pair with prob:  [0.9745]\n",
            "\tDead after pair with prob:  [0.96463346]\n",
            "\tDead after pair with prob:  [0.961569]\n",
            "Episode: 190\tAverage Reward: -125\tAverage steps: 83\t Total: 86112\n",
            "{}\n",
            "Evaluation. Timesteps: 86112 Episodes: 190\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -122. All rewards: [-121, -123, -121, -122, -122]\n",
            "\tDead after pair with prob:  [0.95748717]\n",
            "\tDead after pair with prob:  [0.9780667]\n",
            "\tDead after pair with prob:  [0.98267853]\n",
            "\tDead after pair with prob:  [0.94993436]\n",
            "\tDead after pair with prob:  [0.865784]\n",
            "{}\n",
            "Evaluation. Timesteps: 86576 Episodes: 195\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -119. All rewards: [-120, -119, -119, -119, -120]\n",
            "\tDead after pair with prob:  [0.91643894]\n",
            "\tDead after pair with prob:  [0.94335806]\n",
            "\tDead after pair with prob:  [0.97830874]\n",
            "\tDead after pair with prob:  [0.9338382]\n",
            "\tDead after pair with prob:  [0.7614844]\n",
            "Episode: 200\tAverage Reward: -118\tAverage steps: 78\t Total: 86893\n",
            "{}\n",
            "Evaluation. Timesteps: 86893 Episodes: 200\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -115. All rewards: [-115, -115, -115, -116, -116]\n",
            "\tDead after pair with prob:  [0.8662128]\n",
            "\tDead after pair with prob:  [0.9572275]\n",
            "\tDead after pair with prob:  [0.9271612]\n",
            "\tDead after pair with prob:  [0.775828]\n",
            "\tDead after pair with prob:  [0.93630695]\n",
            "{}\n",
            "Evaluation. Timesteps: 87226 Episodes: 205\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -119. All rewards: [-120, -120, -118, -120, -118]\n",
            "\tDead after pair with prob:  [0.9606641]\n",
            "\tDead after pair with prob:  [0.8025574]\n",
            "\tDead after pair with prob:  [0.94641787]\n",
            "\tDead after pair with prob:  [0.9597172]\n",
            "\tDead after pair with prob:  [0.9692478]\n",
            "Episode: 210\tAverage Reward: -120\tAverage steps: 65\t Total: 87549\n",
            "{}\n",
            "Evaluation. Timesteps: 87549 Episodes: 210\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -115. All rewards: [-110, -118, -116, -117, -115]\n",
            "\tDead after pair with prob:  [0.9590616]\n",
            "\tDead after pair with prob:  [0.95500463]\n",
            "\tDead after pair with prob:  [0.80264926]\n",
            "\tDead after pair with prob:  [0.90188277]\n",
            "\tDead after pair with prob:  [0.9324589]\n",
            "{}\n",
            "Evaluation. Timesteps: 87771 Episodes: 215\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -120. All rewards: [-124, -114, -124, -116, -121]\n",
            "\tDead after pair with prob:  [0.88466376]\n",
            "\tDead after pair with prob:  [0.76494837]\n",
            "\tDead after pair with prob:  [0.97110087]\n",
            "\tDead after pair with prob:  [0.9588654]\n",
            "\tDead after pair with prob:  [0.93782383]\n",
            "Episode: 220\tAverage Reward: -115\tAverage steps: 47\t Total: 88027\n",
            "{}\n",
            "Evaluation. Timesteps: 88027 Episodes: 220\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -113. All rewards: [-113, -113, -113, -113, -113]\n",
            "\tDead after pair with prob:  [0.9169372]\n",
            "\tDead after pair with prob:  [0.9582429]\n",
            "\tDead after pair with prob:  [0.8322683]\n",
            "\tDead after pair with prob:  [0.8885731]\n",
            "\tDead after pair with prob:  [0.941816]\n",
            "{}\n",
            "Evaluation. Timesteps: 88272 Episodes: 225\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -123. All rewards: [-118, -119, -125, -125, -125]\n",
            "\tDead after pair with prob:  [0.9775645]\n",
            "\tDead after pair with prob:  [0.9482489]\n",
            "\tDead after pair with prob:  [0.91867775]\n",
            "\tDead after pair with prob:  [0.97394335]\n",
            "\tDead after pair with prob:  [0.8671338]\n",
            "Episode: 230\tAverage Reward: -120\tAverage steps: 54\t Total: 88574\n",
            "{}\n",
            "Evaluation. Timesteps: 88574 Episodes: 230\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -120. All rewards: [-120, -120, -120, -121, -121]\n",
            "\tDead after pair with prob:  [0.87598675]\n",
            "\tDead after pair with prob:  [0.86520356]\n",
            "\tDead after pair with prob:  [0.9392577]\n",
            "\tDead after pair with prob:  [0.90804195]\n",
            "\tDead after pair with prob:  [0.93574256]\n",
            "{}\n",
            "Evaluation. Timesteps: 88854 Episodes: 235\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -119. All rewards: [-109, -122, -122, -120, -122]\n",
            "\tDead after pair with prob:  [0.9601728]\n",
            "\tDead after pair with prob:  [0.9044051]\n",
            "\tDead after pair with prob:  [0.96726406]\n",
            "\tDead after pair with prob:  [0.9691684]\n",
            "\tDead after pair with prob:  [0.9790106]\n",
            "Episode: 240\tAverage Reward: -118\tAverage steps: 52\t Total: 89101\n",
            "{}\n",
            "Evaluation. Timesteps: 89101 Episodes: 240\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -125. All rewards: [-124, -126, -126, -126, -124]\n",
            "\tDead after pair with prob:  [0.9433134]\n",
            "\tDead after pair with prob:  [0.9227551]\n",
            "\tDead after pair with prob:  [0.913052]\n",
            "\tDead after pair with prob:  [0.962909]\n",
            "\tDead after pair with prob:  [0.7935377]\n",
            "{}\n",
            "Evaluation. Timesteps: 89370 Episodes: 245\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -121. All rewards: [-118, -117, -116, -127, -127]\n",
            "\tDead after pair with prob:  [0.9756815]\n",
            "\tDead after pair with prob:  [0.99166757]\n",
            "\tDead after pair with prob:  [0.8671141]\n",
            "\tDead after pair with prob:  [0.88579667]\n",
            "\tDead after pair with prob:  [0.88351536]\n",
            "Episode: 250\tAverage Reward: -122\tAverage steps: 56\t Total: 89669\n",
            "{}\n",
            "Evaluation. Timesteps: 89669 Episodes: 250\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -110. All rewards: [-114, -109, -109, -109, -109]\n",
            "\tDead after pair with prob:  [0.96818686]\n",
            "\tDead after pair with prob:  [0.9473611]\n",
            "\tDead after pair with prob:  [0.954211]\n",
            "\tDead after pair with prob:  [0.92943776]\n",
            "\tDead after pair with prob:  [0.9765351]\n",
            "{}\n",
            "Evaluation. Timesteps: 89887 Episodes: 255\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -123. All rewards: [-104, -106, -127, -127, -148]\n",
            "\tDead after pair with prob:  [0.8688421]\n",
            "\tDead after pair with prob:  [0.92323637]\n",
            "\tDead after pair with prob:  [0.95018494]\n",
            "\tDead after pair with prob:  [0.89950365]\n",
            "\tDead after pair with prob:  [0.9867754]\n",
            "Episode: 260\tAverage Reward: -114\tAverage steps: 52\t Total: 90193\n",
            "{}\n",
            "Evaluation. Timesteps: 90193 Episodes: 260\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -117. All rewards: [-117, -116, -119, -117, -116]\n",
            "\tDead after pair with prob:  [0.984313]\n",
            "\tDead after pair with prob:  [0.9571261]\n",
            "\tDead after pair with prob:  [0.784711]\n",
            "\tDead after pair with prob:  [0.9500785]\n",
            "\tDead after pair with prob:  [0.9166481]\n",
            "{}\n",
            "Evaluation. Timesteps: 90439 Episodes: 265\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -125. All rewards: [-125, -125, -125, -125, -125]\n",
            "\tDead after pair with prob:  [0.97176516]\n",
            "\tDead after pair with prob:  [0.92944807]\n",
            "\tDead after pair with prob:  [0.9440576]\n",
            "\tDead after pair with prob:  [0.8163703]\n",
            "\tDead after pair with prob:  [0.96246827]\n",
            "Episode: 270\tAverage Reward: -120\tAverage steps: 49\t Total: 90685\n",
            "{}\n",
            "Evaluation. Timesteps: 90685 Episodes: 270\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -110. All rewards: [-114, -108, -109, -108, -108]\n",
            "\tDead after pair with prob:  [0.90929556]\n",
            "\tDead after pair with prob:  [0.84238553]\n",
            "\tDead after pair with prob:  [0.94623876]\n",
            "\tDead after pair with prob:  [0.8959947]\n",
            "\tDead after pair with prob:  [0.99259937]\n",
            "{}\n",
            "Evaluation. Timesteps: 90877 Episodes: 275\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -113. All rewards: [-109, -108, -107, -125, -117]\n",
            "\tDead after pair with prob:  [0.9445238]\n",
            "\tDead after pair with prob:  [0.9860676]\n",
            "\tDead after pair with prob:  [0.9619871]\n",
            "\tDead after pair with prob:  [0.949878]\n",
            "\tDead after pair with prob:  [0.9621113]\n",
            "Episode: 280\tAverage Reward: -113\tAverage steps: 45\t Total: 91141\n",
            "{}\n",
            "Evaluation. Timesteps: 91141 Episodes: 280\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -116. All rewards: [-116, -116, -116, -116, -116]\n",
            "\tDead after pair with prob:  [0.93335104]\n",
            "\tDead after pair with prob:  [0.9466508]\n",
            "\tDead after pair with prob:  [0.9519285]\n",
            "\tDead after pair with prob:  [0.9636204]\n",
            "\tDead after pair with prob:  [0.94974214]\n",
            "{}\n",
            "Evaluation. Timesteps: 91373 Episodes: 285\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -115. All rewards: [-115, -115, -115, -115, -115]\n",
            "\tDead after pair with prob:  [0.96038824]\n",
            "\tDead after pair with prob:  [0.94133806]\n",
            "\tDead after pair with prob:  [0.93332016]\n",
            "\tDead after pair with prob:  [0.94814473]\n",
            "\tDead after pair with prob:  [0.98497134]\n",
            "Episode: 290\tAverage Reward: -116\tAverage steps: 46\t Total: 91603\n",
            "{}\n",
            "Evaluation. Timesteps: 91603 Episodes: 290\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -108. All rewards: [-116, -105, -107, -105, -105]\n",
            "\tDead after pair with prob:  [0.9777891]\n",
            "\tDead after pair with prob:  [0.95030606]\n",
            "\tDead after pair with prob:  [0.97747487]\n",
            "\tDead after pair with prob:  [0.9780924]\n",
            "\tDead after pair with prob:  [0.9804966]\n",
            "{}\n",
            "Evaluation. Timesteps: 91884 Episodes: 295\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -108. All rewards: [-104, -104, -103, -103, -125]\n",
            "\tDead after pair with prob:  [0.97742134]\n",
            "\tDead after pair with prob:  [0.91791713]\n",
            "\tDead after pair with prob:  [0.95967615]\n",
            "\tDead after pair with prob:  [0.9268246]\n",
            "\tDead after pair with prob:  [0.81412363]\n",
            "Episode: 300\tAverage Reward: -115\tAverage steps: 54\t Total: 92148\n",
            "{}\n",
            "Evaluation. Timesteps: 92148 Episodes: 300\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -114. All rewards: [-114, -114, -115, -115, -114]\n",
            "\tDead after pair with prob:  [0.9638835]\n",
            "\tDead after pair with prob:  [0.91394484]\n",
            "\tDead after pair with prob:  [0.9716005]\n",
            "\tDead after pair with prob:  [0.9795344]\n",
            "\tDead after pair with prob:  [0.8722789]\n",
            "{}\n",
            "Evaluation. Timesteps: 92375 Episodes: 305\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -117. All rewards: [-117, -118, -117, -117, -117]\n",
            "\tDead after pair with prob:  [0.97196406]\n",
            "\tDead after pair with prob:  [0.98436695]\n",
            "\tDead after pair with prob:  [0.9626417]\n",
            "\tDead after pair with prob:  [0.96435237]\n",
            "\tDead after pair with prob:  [0.9503982]\n",
            "Episode: 310\tAverage Reward: -118\tAverage steps: 49\t Total: 92647\n",
            "{}\n",
            "Evaluation. Timesteps: 92647 Episodes: 310\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -122. All rewards: [-118, -123, -123, -123, -123]\n",
            "\tDead after pair with prob:  [0.9485096]\n",
            "\tDead after pair with prob:  [0.9430622]\n",
            "\tDead after pair with prob:  [0.9716114]\n",
            "\tDead after pair with prob:  [0.86297864]\n",
            "\tDead after pair with prob:  [0.97342277]\n",
            "{}\n",
            "Evaluation. Timesteps: 92979 Episodes: 315\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -118. All rewards: [-116, -128, -114, -115, -117]\n",
            "\tDead after pair with prob:  [0.93653315]\n",
            "\tDead after pair with prob:  [0.99140036]\n",
            "\tDead after pair with prob:  [0.9749945]\n",
            "\tDead after pair with prob:  [0.9795766]\n",
            "\tDead after pair with prob:  [0.9807762]\n",
            "Episode: 320\tAverage Reward: -116\tAverage steps: 55\t Total: 93202\n",
            "{}\n",
            "Evaluation. Timesteps: 93202 Episodes: 320\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -115. All rewards: [-115, -115, -114, -115, -115]\n",
            "\tDead after pair with prob:  [0.8824614]\n",
            "\tDead after pair with prob:  [0.8513971]\n",
            "\tDead after pair with prob:  [0.9611247]\n",
            "\tDead after pair with prob:  [0.9485768]\n",
            "\tDead after pair with prob:  [0.9837691]\n",
            "{}\n",
            "Evaluation. Timesteps: 93437 Episodes: 325\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -116. All rewards: [-116, -116, -117, -116, -116]\n",
            "\tDead after pair with prob:  [0.9824985]\n",
            "\tDead after pair with prob:  [0.9704763]\n",
            "\tDead after pair with prob:  [0.95985854]\n",
            "\tDead after pair with prob:  [0.9561293]\n",
            "\tDead after pair with prob:  [0.9232489]\n",
            "Episode: 330\tAverage Reward: -115\tAverage steps: 45\t Total: 93657\n",
            "{}\n",
            "Evaluation. Timesteps: 93657 Episodes: 330\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -119. All rewards: [-115, -117, -118, -117, -130]\n",
            "\tDead after pair with prob:  [0.96516854]\n",
            "\tDead after pair with prob:  [0.9171848]\n",
            "\tDead after pair with prob:  [0.8388293]\n",
            "\tDead after pair with prob:  [0.74385214]\n",
            "\tDead after pair with prob:  [0.9680104]\n",
            "{}\n",
            "Evaluation. Timesteps: 93974 Episodes: 335\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -124. All rewards: [-128, -124, -123, -127, -116]\n",
            "\tDead after pair with prob:  [0.85521144]\n",
            "\tDead after pair with prob:  [0.9615484]\n",
            "\tDead after pair with prob:  [0.98974764]\n",
            "\tDead after pair with prob:  [0.98532575]\n",
            "\tDead after pair with prob:  [0.962537]\n",
            "Episode: 340\tAverage Reward: -115\tAverage steps: 58\t Total: 94240\n",
            "{}\n",
            "Evaluation. Timesteps: 94240 Episodes: 340\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -121. All rewards: [-122, -122, -115, -123, -122]\n",
            "\tDead after pair with prob:  [0.984421]\n",
            "\tDead after pair with prob:  [0.9300098]\n",
            "\tDead after pair with prob:  [0.829298]\n",
            "\tDead after pair with prob:  [0.98524183]\n",
            "\tDead after pair with prob:  [0.96814716]\n",
            "{}\n",
            "Evaluation. Timesteps: 94511 Episodes: 345\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -123. All rewards: [-123, -124, -124, -124, -123]\n",
            "\tDead after pair with prob:  [0.99395025]\n",
            "\tDead after pair with prob:  [0.83827704]\n",
            "\tDead after pair with prob:  [0.9514568]\n",
            "\tDead after pair with prob:  [0.8333825]\n",
            "\tDead after pair with prob:  [0.9732945]\n",
            "Episode: 350\tAverage Reward: -119\tAverage steps: 48\t Total: 94728\n",
            "{}\n",
            "Evaluation. Timesteps: 94728 Episodes: 350\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -113. All rewards: [-115, -110, -120, -109, -110]\n",
            "\tDead after pair with prob:  [0.8387226]\n",
            "\tDead after pair with prob:  [0.9938693]\n",
            "\tDead after pair with prob:  [0.96327686]\n",
            "\tDead after pair with prob:  [0.95554334]\n",
            "\tDead after pair with prob:  [0.97672766]\n",
            "{}\n",
            "Evaluation. Timesteps: 95104 Episodes: 355\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -129. All rewards: [-127, -129, -129, -129, -128]\n",
            "\tDead after pair with prob:  [0.97018313]\n",
            "\tDead after pair with prob:  [0.96540916]\n",
            "\tDead after pair with prob:  [0.96724063]\n",
            "\tDead after pair with prob:  [0.8906611]\n",
            "\tDead after pair with prob:  [0.95153946]\n",
            "Episode: 360\tAverage Reward: -120\tAverage steps: 66\t Total: 95394\n",
            "{}\n",
            "Evaluation. Timesteps: 95394 Episodes: 360\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -127. All rewards: [-127, -127, -127, -126, -126]\n",
            "\tDead after pair with prob:  [0.96426743]\n",
            "\tDead after pair with prob:  [0.9189245]\n",
            "\tDead after pair with prob:  [0.93081176]\n",
            "\tDead after pair with prob:  [0.9270436]\n",
            "\tDead after pair with prob:  [0.889812]\n",
            "{}\n",
            "Evaluation. Timesteps: 95688 Episodes: 365\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -123. All rewards: [-123, -123, -123, -123, -123]\n",
            "\tDead after pair with prob:  [0.96358114]\n",
            "\tDead after pair with prob:  [0.9723247]\n",
            "\tDead after pair with prob:  [0.98165864]\n",
            "\tDead after pair with prob:  [0.89408696]\n",
            "\tDead after pair with prob:  [0.9908449]\n",
            "Episode: 370\tAverage Reward: -125\tAverage steps: 63\t Total: 96025\n",
            "{}\n",
            "Evaluation. Timesteps: 96025 Episodes: 370\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -103. All rewards: [-114, -101, -100, -100, -102]\n",
            "\tDead after pair with prob:  [0.90130115]\n",
            "\tDead after pair with prob:  [0.9101599]\n",
            "\tDead after pair with prob:  [0.80057687]\n",
            "\tDead after pair with prob:  [0.93872565]\n",
            "\tDead after pair with prob:  [0.8870794]\n",
            "{}\n",
            "Evaluation. Timesteps: 96328 Episodes: 375\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -130. All rewards: [-131, -130, -133, -128, -131]\n",
            "\tDead after pair with prob:  [0.8444372]\n",
            "\tDead after pair with prob:  [0.9663745]\n",
            "\tDead after pair with prob:  [0.95534474]\n",
            "\tDead after pair with prob:  [0.93216544]\n",
            "\tDead after pair with prob:  [0.9642963]\n",
            "Episode: 380\tAverage Reward: -122\tAverage steps: 68\t Total: 96706\n",
            "{}\n",
            "Evaluation. Timesteps: 96706 Episodes: 380\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -119. All rewards: [-119, -119, -118, -119, -119]\n",
            "\tDead after pair with prob:  [0.98594475]\n",
            "\tDead after pair with prob:  [0.9910721]\n",
            "\tDead after pair with prob:  [0.9146348]\n",
            "\tDead after pair with prob:  [0.9401119]\n",
            "\tDead after pair with prob:  [0.97981524]\n",
            "{}\n",
            "Evaluation. Timesteps: 97035 Episodes: 385\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -127. All rewards: [-125, -135, -126, -125, -126]\n",
            "\tDead after pair with prob:  [0.98463506]\n",
            "\tDead after pair with prob:  [0.87168443]\n",
            "\tDead after pair with prob:  [0.8872843]\n",
            "\tDead after pair with prob:  [0.9624146]\n",
            "\tDead after pair with prob:  [0.98633295]\n",
            "Episode: 390\tAverage Reward: -129\tAverage steps: 67\t Total: 97379\n",
            "{}\n",
            "Evaluation. Timesteps: 97379 Episodes: 390\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -125. All rewards: [-116, -126, -128, -130, -128]\n",
            "\tDead after pair with prob:  [0.84744763]\n",
            "\tDead after pair with prob:  [0.90238154]\n",
            "\tDead after pair with prob:  [0.9342074]\n",
            "\tDead after pair with prob:  [0.87540895]\n",
            "\tDead after pair with prob:  [0.9863669]\n",
            "{}\n",
            "Evaluation. Timesteps: 97849 Episodes: 395\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -128. All rewards: [-128, -129, -131, -127, -124]\n",
            "\tDead after pair with prob:  [0.95147467]\n",
            "\tDead after pair with prob:  [0.9729494]\n",
            "\tDead after pair with prob:  [0.8440254]\n",
            "\tDead after pair with prob:  [0.00114968]\n",
            "\tDead after pair with prob:  [2.8065417e-09]\n",
            "Episode: 400\tAverage Reward: -132\tAverage steps: 110\t Total: 98485\n",
            "{}\n",
            "Evaluation. Timesteps: 98485 Episodes: 400\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tMean reward: -140. All rewards: [-135, -141, -144, -139, -140]\n",
            "\tDead after pair with prob:  [0.88484085]\n",
            "\tDead after pair with prob:  [0.9754807]\n",
            "\tDead after pair with prob:  [1.2865977e-06]\n",
            "\tDead after pair with prob:  [0.9889354]\n",
            "\tDead after pair with prob:  [0.98347616]\n",
            "{}\n",
            "Evaluation. Timesteps: 100099 Episodes: 405\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -152. All rewards: [-156, -157, -147, -153, -145]\n",
            "\tStuck, ep reward -146.34963093116647\n",
            "\tDead after pair with prob:  [0.8756921]\n",
            "\tStuck, ep reward -109.63537441063079\n",
            "\tStuck, ep reward -157.6946771354883\n",
            "\tStuck, ep reward -171.09041042219198\n",
            "Episode: 410\tAverage Reward: -147\tAverage steps: 817\t Total: 106657\n",
            "{}\n",
            "Evaluation. Timesteps: 106657 Episodes: 410\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -156. All rewards: [-153, -156, -157, -157, -155]\n",
            "\tStuck, ep reward -150.13937965331365\n",
            "\tStuck, ep reward -123.91978346248588\n",
            "\tStuck, ep reward -126.02558426014174\n",
            "\tStuck, ep reward -143.20242617868516\n",
            "\tStuck, ep reward -118.11642966975651\n",
            "{}\n",
            "Evaluation. Timesteps: 114652 Episodes: 415\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -131. All rewards: [-118, -130, -131, -139, -139]\n",
            "\tStuck, ep reward -128.14752859631358\n",
            "\tStuck, ep reward -121.542173461055\n",
            "\tStuck, ep reward -156.169242730329\n",
            "\tStuck, ep reward -137.9465963554804\n",
            "\tStuck, ep reward -136.725572743667\n",
            "Episode: 420\tAverage Reward: -134\tAverage steps: 1599\t Total: 122647\n",
            "{}\n",
            "Evaluation. Timesteps: 122647 Episodes: 420\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tMean reward: -156. All rewards: [-154, -156, -158, -156, -154]\n",
            "\tDead after pair with prob:  [1.2449001e-11]\n",
            "\tDead after pair with prob:  [0.8638684]\n",
            "\tDead after pair with prob:  [0.9330649]\n",
            "\tDead after pair with prob:  [0.5025201]\n",
            "\tStuck, ep reward -141.9806913560747\n",
            "{}\n",
            "Evaluation. Timesteps: 125938 Episodes: 425\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -137. All rewards: [-136, -135, -141, -133, -140]\n",
            "\tDead after pair with prob:  [0.90556556]\n",
            "\tDead after pair with prob:  [0.8937175]\n",
            "\tDead after pair with prob:  [0.9447467]\n",
            "\tDead after pair with prob:  [0.8443016]\n",
            "\tDead after pair with prob:  [0.6139705]\n",
            "Episode: 430\tAverage Reward: -150\tAverage steps: 389\t Total: 126546\n",
            "{}\n",
            "Evaluation. Timesteps: 126546 Episodes: 430\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -137. All rewards: [-131, -138, -137, -143, -137]\n",
            "\tDead after pair with prob:  [0.9436168]\n",
            "\tDead after pair with prob:  [0.95838785]\n",
            "\tDead after pair with prob:  [0.91253304]\n",
            "\tDead after pair with prob:  [0.90576005]\n",
            "\tDead after pair with prob:  [0.8746781]\n",
            "{}\n",
            "Evaluation. Timesteps: 127247 Episodes: 435\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -143. All rewards: [-141, -140, -150, -142, -141]\n",
            "\tDead after pair with prob:  [0.98085773]\n",
            "\tDead after pair with prob:  [0.87629324]\n",
            "\tDead after pair with prob:  [0.9137889]\n",
            "\tDead after pair with prob:  [2.7449937e-34]\n",
            "\tDead after pair with prob:  [0.84981704]\n",
            "Episode: 440\tAverage Reward: -141\tAverage steps: 143\t Total: 127982\n",
            "{}\n",
            "Evaluation. Timesteps: 127982 Episodes: 440\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -135. All rewards: [-139, -134, -134, -136, -133]\n",
            "\tDead after pair with prob:  [0.8789484]\n",
            "\tDead after pair with prob:  [0.9358961]\n",
            "\tDead after pair with prob:  [0.9014952]\n",
            "\tDead after pair with prob:  [0.97702426]\n",
            "\tDead after pair with prob:  [0.8294528]\n",
            "{}\n",
            "Evaluation. Timesteps: 128538 Episodes: 445\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -146. All rewards: [-137, -151, -157, -130, -154]\n",
            "\tDead after pair with prob:  [0.986761]\n",
            "\tDead after pair with prob:  [1.0773612e-18]\n",
            "\tDead after pair with prob:  [0.8072871]\n",
            "\tDead after pair with prob:  [1.5130437e-08]\n",
            "\tDead after pair with prob:  [0.9204596]\n",
            "Episode: 450\tAverage Reward: -139\tAverage steps: 154\t Total: 129524\n",
            "{}\n",
            "Evaluation. Timesteps: 129524 Episodes: 450\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -132. All rewards: [-128, -141, -130, -131, -131]\n",
            "\tDead after pair with prob:  [0.8337589]\n",
            "\tDead after pair with prob:  [0.92349577]\n",
            "\tDead after pair with prob:  [0.87122995]\n",
            "\tDead after pair with prob:  [0.88934314]\n",
            "\tDead after pair with prob:  [0.5814329]\n",
            "{}\n",
            "Evaluation. Timesteps: 130188 Episodes: 455\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -133. All rewards: [-137, -137, -138, -142, -111]\n",
            "\tDead after pair with prob:  [1.0456261e-09]\n",
            "\tDead after pair with prob:  [1.2814649e-05]\n",
            "\tStuck, ep reward -114.4717850580517\n",
            "\tStuck, ep reward -102.57258254034913\n",
            "\tStuck, ep reward -124.79101552800888\n",
            "Episode: 460\tAverage Reward: -135\tAverage steps: 634\t Total: 135869\n",
            "{}\n",
            "Evaluation. Timesteps: 135869 Episodes: 460\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -129. All rewards: [-119, -115, -160, -117, -135]\n",
            "\tStuck, ep reward -110.86113743574417\n",
            "\tDead after pair with prob:  [2.0395012e-06]\n",
            "\tStuck, ep reward -173.0012675392176\n",
            "\tStuck, ep reward -129.8878409409286\n",
            "\tDead after pair with prob:  [0.836]\n",
            "{}\n",
            "Evaluation. Timesteps: 141669 Episodes: 465\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -172. All rewards: [-172, -172, -172, -172, -171]\n",
            "\tStuck, ep reward -160.83410068194712\n",
            "\tStuck, ep reward -136.50453868451777\n",
            "\tStuck, ep reward -143.12509414318208\n",
            "\tStuck, ep reward -112.81283643999907\n",
            "\tStuck, ep reward -143.00935604150192\n",
            "Episode: 470\tAverage Reward: -143\tAverage steps: 1379\t Total: 149664\n",
            "{}\n",
            "Evaluation. Timesteps: 149664 Episodes: 470\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -122. All rewards: [-116, -124, -119, -126, -124]\n",
            "\tStuck, ep reward -110.03004554103282\n",
            "\tStuck, ep reward -72.28109291790221\n",
            "\tStuck, ep reward -103.55700465112085\n",
            "\tStuck, ep reward -90.34873238079618\n",
            "\tStuck, ep reward -158.52790640924124\n",
            "{}\n",
            "Evaluation. Timesteps: 157659 Episodes: 475\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -136. All rewards: [-133, -139, -135, -137, -137]\n",
            "\tStuck, ep reward -135.73021027318313\n",
            "\tStuck, ep reward -137.00608717040026\n",
            "\tStuck, ep reward -110.42072970137629\n",
            "\tStuck, ep reward -91.52538539478184\n",
            "\tStuck, ep reward -60.798682211718685\n",
            "Episode: 480\tAverage Reward: -107\tAverage steps: 1599\t Total: 165654\n",
            "{}\n",
            "Evaluation. Timesteps: 165654 Episodes: 480\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -59. All rewards: [-57, -64, -54, -63, -56]\n",
            "\tDead after pair with prob:  [0.93890214]\n",
            "\tStuck, ep reward -62.82967889387314\n",
            "\tStuck, ep reward -55.06908028474273\n",
            "\tStuck, ep reward -73.70118025057286\n",
            "\tStuck, ep reward -88.41942502693868\n",
            "{}\n",
            "Evaluation. Timesteps: 172156 Episodes: 485\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -131. All rewards: [-124, -135, -133, -135, -129]\n",
            "\tStuck, ep reward -126.63749197682567\n",
            "\tDead after pair with prob:  [0.9152275]\n",
            "\tStuck, ep reward -164.67329879782332\n",
            "\tStuck, ep reward -69.56856309426924\n",
            "\tStuck, ep reward -107.26105023028765\n",
            "Episode: 490\tAverage Reward: -96\tAverage steps: 1297\t Total: 178626\n",
            "{}\n",
            "Evaluation. Timesteps: 178626 Episodes: 490\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -131. All rewards: [-132, -132, -127, -133, -133]\n",
            "\tStuck, ep reward -129.0602915034213\n",
            "\tStuck, ep reward -98.96016943714737\n",
            "\tStuck, ep reward -92.87360798596707\n",
            "\tStuck, ep reward -43.57791724857022\n",
            "\tStuck, ep reward -100.60322985118884\n",
            "{}\n",
            "Evaluation. Timesteps: 186621 Episodes: 495\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -84. All rewards: [-57, -56, -72, -128, -108]\n",
            "\tStuck, ep reward -61.79088225198328\n",
            "\tStuck, ep reward -31.796361135086105\n",
            "\tStuck, ep reward -9.40906271095446\n",
            "\tStuck, ep reward -32.72992039353172\n",
            "\tStuck, ep reward -72.14519723725309\n",
            "Episode: 500\tAverage Reward: -67\tAverage steps: 1599\t Total: 194616\n",
            "{}\n",
            "Evaluation. Timesteps: 194616 Episodes: 500\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -81. All rewards: [-88, -77, -71, -91, -77]\n",
            "\tStuck, ep reward -83.37788743351928\n",
            "\tStuck, ep reward -83.73064678223113\n",
            "\tStuck, ep reward -81.9688226039636\n",
            "\tStuck, ep reward -72.43154432838621\n",
            "\tStuck, ep reward -97.49949840981778\n",
            "{}\n",
            "Evaluation. Timesteps: 202611 Episodes: 505\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -81. All rewards: [-78, -76, -81, -80, -88]\n",
            "\tStuck, ep reward -80.06441959872481\n",
            "\tStuck, ep reward -66.41956777948859\n",
            "\tStuck, ep reward -60.68982398440667\n",
            "\tStuck, ep reward -62.38461580572746\n",
            "\tStuck, ep reward -125.32016213959689\n",
            "Episode: 510\tAverage Reward: -81\tAverage steps: 1599\t Total: 210606\n",
            "{}\n",
            "Evaluation. Timesteps: 210606 Episodes: 510\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -64. All rewards: [-69, -44, -66, -70, -70]\n",
            "\tStuck, ep reward -70.87517754485205\n",
            "\tStuck, ep reward -61.19378343303554\n",
            "\tStuck, ep reward -110.79832393444774\n",
            "\tStuck, ep reward -112.31685566083017\n",
            "\tStuck, ep reward -97.14620807495416\n",
            "{}\n",
            "Evaluation. Timesteps: 218601 Episodes: 515\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -83. All rewards: [-84, -82, -82, -85, -81]\n",
            "\tStuck, ep reward -83.78356509814022\n",
            "\tStuck, ep reward -81.29891533022564\n",
            "\tStuck, ep reward -40.80236822939487\n",
            "\tStuck, ep reward -97.10731225151756\n",
            "\tStuck, ep reward -77.20934767247812\n",
            "Episode: 520\tAverage Reward: -83\tAverage steps: 1599\t Total: 226596\n",
            "{}\n",
            "Evaluation. Timesteps: 226596 Episodes: 520\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -61. All rewards: [-58, -61, -69, -59, -60]\n",
            "\tStuck, ep reward -56.67619356434497\n",
            "\tStuck, ep reward -74.55225311335518\n",
            "\tStuck, ep reward -75.95616255387671\n",
            "\tStuck, ep reward -69.74317078067324\n",
            "\tStuck, ep reward -81.87000480420488\n",
            "{}\n",
            "Evaluation. Timesteps: 234591 Episodes: 525\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -78. All rewards: [-89, -90, -94, -59, -60]\n",
            "\tStuck, ep reward -81.56606968601065\n",
            "\tStuck, ep reward -84.8274853280224\n",
            "\tStuck, ep reward -73.11392246768436\n",
            "\tStuck, ep reward -36.340761003884964\n",
            "\tStuck, ep reward -16.30446180176206\n",
            "Episode: 530\tAverage Reward: -65\tAverage steps: 1599\t Total: 242586\n",
            "{}\n",
            "Evaluation. Timesteps: 242586 Episodes: 530\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -60. All rewards: [-44, -39, -74, -67, -76]\n",
            "\tStuck, ep reward -40.484862879334536\n",
            "\tStuck, ep reward -27.812052324326892\n",
            "\tStuck, ep reward -37.40016729736852\n",
            "\tStuck, ep reward -74.48596631562474\n",
            "\tDead after pair with prob:  [0.9977157]\n",
            "{}\n",
            "Evaluation. Timesteps: 249077 Episodes: 535\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -106. All rewards: [-98, -126, -101, -102, -103]\n",
            "\tDead after pair with prob:  [0.9990895]\n",
            "\tDead after pair with prob:  [0.63883394]\n",
            "\tStuck, ep reward -40.442412542354695\n",
            "\tDead after pair with prob:  [0.917863]\n",
            "\tStuck, ep reward -39.195907909895524\n",
            "Episode: 540\tAverage Reward: -73\tAverage steps: 1137\t Total: 253957\n",
            "{}\n",
            "Evaluation. Timesteps: 253957 Episodes: 540\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -33. All rewards: [-32, -37, -35, -33, -30]\n",
            "\tStuck, ep reward -37.28994278792806\n",
            "\tStuck, ep reward -71.23896990461341\n",
            "\tStuck, ep reward -56.77233573588153\n",
            "\tStuck, ep reward -103.47537659566285\n",
            "\tStuck, ep reward -59.72833352988192\n",
            "{}\n",
            "Evaluation. Timesteps: 261952 Episodes: 545\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -60. All rewards: [-63, -60, -60, -62, -57]\n",
            "\tStuck, ep reward -71.0766905480872\n",
            "\tStuck, ep reward -84.38875277049775\n",
            "\tStuck, ep reward -115.96261674646516\n",
            "\tStuck, ep reward -91.10976384701344\n",
            "\tStuck, ep reward -55.809873475467036\n",
            "Episode: 550\tAverage Reward: -74\tAverage steps: 1599\t Total: 269947\n",
            "{}\n",
            "Evaluation. Timesteps: 269947 Episodes: 550\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -76. All rewards: [-95, -99, -72, -59, -57]\n",
            "\tStuck, ep reward -67.07368500633525\n",
            "\tStuck, ep reward -63.149253912233135\n",
            "\tStuck, ep reward -20.87262976209771\n",
            "\tStuck, ep reward -66.63712950711759\n",
            "\tStuck, ep reward -67.49007763296949\n",
            "{}\n",
            "Evaluation. Timesteps: 277942 Episodes: 555\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -71. All rewards: [-72, -72, -66, -80, -63]\n",
            "\tStuck, ep reward -64.7816029122673\n",
            "\tStuck, ep reward -74.96181510856165\n",
            "\tStuck, ep reward -58.08493778333508\n",
            "\tStuck, ep reward -60.28525363511766\n",
            "\tStuck, ep reward -30.392461682115076\n",
            "Episode: 560\tAverage Reward: -57\tAverage steps: 1599\t Total: 285937\n",
            "{}\n",
            "Evaluation. Timesteps: 285937 Episodes: 560\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -56. All rewards: [-47, -59, -57, -57, -59]\n",
            "\tStuck, ep reward -50.143810750230735\n",
            "\tStuck, ep reward -20.146031624120404\n",
            "\tStuck, ep reward -56.17687166431857\n",
            "\tStuck, ep reward -63.065111292839404\n",
            "\tStuck, ep reward -98.18246533125799\n",
            "{}\n",
            "Evaluation. Timesteps: 293932 Episodes: 565\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -55. All rewards: [-46, -100, -43, -44, -43]\n",
            "\tStuck, ep reward -48.68376160382089\n",
            "\tStuck, ep reward -60.03535934306727\n",
            "\tStuck, ep reward -96.16492770644795\n",
            "\tStuck, ep reward -114.11682390080355\n",
            "\tStuck, ep reward -56.67338509066927\n",
            "Episode: 570\tAverage Reward: -66\tAverage steps: 1599\t Total: 301927\n",
            "{}\n",
            "Evaluation. Timesteps: 301927 Episodes: 570\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -60. All rewards: [-48, -50, -106, -51, -43]\n",
            "\tStuck, ep reward -54.11085429805275\n",
            "\tStuck, ep reward -18.31871919899288\n",
            "\tStuck, ep reward -62.363902322111215\n",
            "\tStuck, ep reward -38.951484294933586\n",
            "\tStuck, ep reward -57.13816187488482\n",
            "{}\n",
            "Evaluation. Timesteps: 309922 Episodes: 575\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -66. All rewards: [-64, -66, -75, -61, -66]\n",
            "\tStuck, ep reward -60.23237457588756\n",
            "\tStuck, ep reward -30.146068152873017\n",
            "\tStuck, ep reward -69.50859327500363\n",
            "\tStuck, ep reward -53.164265762064744\n",
            "\tStuck, ep reward -45.071760241803794\n",
            "Episode: 580\tAverage Reward: -48\tAverage steps: 1599\t Total: 317917\n",
            "{}\n",
            "Evaluation. Timesteps: 317917 Episodes: 580\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -37. All rewards: [-37, -48, -49, -25, -24]\n",
            "\tStuck, ep reward -41.907940998603934\n",
            "\tStuck, ep reward -50.152822373315544\n",
            "\tStuck, ep reward -45.002969622274996\n",
            "\tStuck, ep reward -30.190656094787876\n",
            "\tStuck, ep reward 17.183392874954308\n",
            "{}\n",
            "Evaluation. Timesteps: 325912 Episodes: 585\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -29. All rewards: [-31, -24, -38, -18, -35]\n",
            "\tStuck, ep reward -35.9900257369521\n",
            "\tStuck, ep reward -99.8241334798403\n",
            "\tStuck, ep reward -10.014588386190734\n",
            "\tDead after pair with prob:  [1.]\n",
            "\tStuck, ep reward -11.235654917461392\n",
            "Episode: 590\tAverage Reward: -43\tAverage steps: 1444\t Total: 332362\n",
            "{}\n",
            "Evaluation. Timesteps: 332362 Episodes: 590\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -13. All rewards: [-13, -27, 53, -85, 5]\n",
            "\tStuck, ep reward 44.18605664133035\n",
            "\tStuck, ep reward -44.603220921120894\n",
            "\tStuck, ep reward 45.69179767654803\n",
            "\tStuck, ep reward -66.68824769727652\n",
            "\tStuck, ep reward -92.5623671572755\n",
            "{}\n",
            "Evaluation. Timesteps: 340357 Episodes: 595\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -103. All rewards: [-104, -104, -112, -104, -93]\n",
            "\tStuck, ep reward -40.18614308755163\n",
            "\tStuck, ep reward -51.12222421959682\n",
            "\tStuck, ep reward -64.45379119478629\n",
            "\tStuck, ep reward -58.08077738509792\n",
            "\tStuck, ep reward -54.96312911445485\n",
            "Episode: 600\tAverage Reward: -38\tAverage steps: 1599\t Total: 348352\n",
            "{}\n",
            "Evaluation. Timesteps: 348352 Episodes: 600\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -48. All rewards: [-49, -48, -46, -50, -50]\n",
            "\tStuck, ep reward -46.99588951503526\n",
            "\tStuck, ep reward -45.7666471729758\n",
            "\tStuck, ep reward -55.595679115579344\n",
            "\tStuck, ep reward -50.99194417029645\n",
            "\tStuck, ep reward -82.4360323409603\n",
            "{}\n",
            "Evaluation. Timesteps: 356347 Episodes: 605\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -62. All rewards: [-62, -68, -41, -69, -71]\n",
            "\tStuck, ep reward -61.113045918218404\n",
            "\tStuck, ep reward 52.74016647206735\n",
            "\tStuck, ep reward 42.13670867936516\n",
            "\tStuck, ep reward -33.78798623599253\n",
            "\tStuck, ep reward 60.43886925897867\n",
            "Episode: 610\tAverage Reward: -22\tAverage steps: 1599\t Total: 364342\n",
            "{}\n",
            "Evaluation. Timesteps: 364342 Episodes: 610\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: 4. All rewards: [0, 17, 4, 0, 1]\n",
            "\tStuck, ep reward 9.023885107418884\n",
            "\tStuck, ep reward -54.65645194760577\n",
            "\tStuck, ep reward 68.78294783063677\n",
            "\tStuck, ep reward 68.07745258513799\n",
            "\tStuck, ep reward 120.94181057401708\n",
            "{}\n",
            "Evaluation. Timesteps: 372337 Episodes: 615\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: 53. All rewards: [83, 80, 78, -57, 82]\n",
            "\tStuck, ep reward 74.97128611706668\n",
            "\tStuck, ep reward -5.111895665953847\n",
            "\tStuck, ep reward 58.97577920185384\n",
            "\tStuck, ep reward 48.561691996189914\n",
            "\tStuck, ep reward 24.790940545458604\n",
            "Episode: 620\tAverage Reward: 41\tAverage steps: 1599\t Total: 380332\n",
            "{}\n",
            "Evaluation. Timesteps: 380332 Episodes: 620\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: 4. All rewards: [-70, 55, 37, 65, -65]\n",
            "\tStuck, ep reward 59.43904569291666\n",
            "\tStuck, ep reward 104.78883836105648\n",
            "\tStuck, ep reward 79.57066862051438\n",
            "\tStuck, ep reward 50.89231984056547\n",
            "\tStuck, ep reward 32.20458643156018\n",
            "{}\n",
            "Evaluation. Timesteps: 388327 Episodes: 625\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: 24. All rewards: [10, 13, 32, 21, 43]\n",
            "\tStuck, ep reward 44.125070147734554\n",
            "\tStuck, ep reward 97.94855432399906\n",
            "\tStuck, ep reward 92.9505914915801\n",
            "\tStuck, ep reward 86.11238995560446\n",
            "\tStuck, ep reward 87.5941499258489\n",
            "Episode: 630\tAverage Reward: 73\tAverage steps: 1599\t Total: 396322\n",
            "{}\n",
            "Evaluation. Timesteps: 396322 Episodes: 630\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: 128. All rewards: [129, 116, 124, 137, 133]\n",
            "\tStuck, ep reward 123.47428158580438\n",
            "\tStuck, ep reward 87.45911639471528\n",
            "\tStuck, ep reward 113.61928948731601\n",
            "\tStuck, ep reward 155.79540582465185\n",
            "\tStuck, ep reward 67.9140987648154\n",
            "{}\n",
            "Evaluation. Timesteps: 404317 Episodes: 635\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: 150. All rewards: [157, 150, 149, 151, 144]\n",
            "\tStuck, ep reward 141.31192967415382\n",
            "\tStuck, ep reward 109.48715073056636\n",
            "\tStuck, ep reward 146.89724176325515\n",
            "\tStuck, ep reward 140.7394650296136\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}