{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TD3PG.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "61VeKABEm9_O",
        "geRE4MK65Wnb",
        "UH9aTMAwIeTf",
        "YnZxLGmrlAyw"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akterskii/RL/blob/master/Dead%20prediction/TD3PG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4Ovhw-h0cfa",
        "colab_type": "code",
        "outputId": "313facc4-9cc0-4834-861e-9a51bbca4d45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        }
      },
      "source": [
        "!pip install gym['box2d']"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.6/dist-packages (0.10.11)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.12.0)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (2.21.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.3.1)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.16.4)\n",
            "Collecting box2d-py>=2.3.5; extra == \"box2d\" (from gym[box2d])\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/bd/6cdc3fd994b0649dcf5d9bad85bd9e26172308bbe9a421bfc6fdbf5081a6/box2d_py-2.3.8-cp36-cp36m-manylinux1_x86_64.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 40.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[box2d]) (2019.6.16)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[box2d]) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[box2d]) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[box2d]) (2.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym[box2d]) (0.16.0)\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GuuZ4NNdAIa",
        "colab_type": "code",
        "outputId": "a35a0898-7a5a-49d0-a0fe-ff69793b2c88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "# Run this cell to mount your Google Drive.\n",
        "from google.colab import drive\n",
        "mount = '/content/drive'\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e81Vm0EmsKyN",
        "colab_type": "text"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAAepgnysM01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os \n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "#for RAdam\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "\n",
        "# for custom env\n",
        "from gym.wrappers import TimeLimit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXdG4uSSr9Kx",
        "colab_type": "text"
      },
      "source": [
        "## RAdam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TuAFDpGr8aX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RAdam(Optimizer):\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
        "        self.buffer = [[None, None, None] for ind in range(10)]\n",
        "        super(RAdam, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(RAdam, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data.float()\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
        "\n",
        "                p_data_fp32 = p.data.float()\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
        "                else:\n",
        "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "\n",
        "                state['step'] += 1\n",
        "                buffered = self.buffer[int(state['step'] % 10)]\n",
        "                if state['step'] == buffered[0]:\n",
        "                    N_sma, step_size = buffered[1], buffered[2]\n",
        "                else:\n",
        "                    buffered[0] = state['step']\n",
        "                    beta2_t = beta2 ** state['step']\n",
        "                    N_sma_max = 2 / (1 - beta2) - 1\n",
        "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
        "                    buffered[1] = N_sma\n",
        "                    if N_sma > 5:\n",
        "                        step_size = group['lr'] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
        "                    else:\n",
        "                        step_size = group['lr'] / (1 - beta1 ** state['step'])\n",
        "                    buffered[2] = step_size\n",
        "\n",
        "                if group['weight_decay'] != 0:\n",
        "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
        "\n",
        "                if N_sma > 5:                    \n",
        "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n",
        "                else:\n",
        "                    p_data_fp32.add_(-step_size, exp_avg)\n",
        "\n",
        "                p.data.copy_(p_data_fp32)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu-XBmsVIJ-p",
        "colab_type": "text"
      },
      "source": [
        "## Actor and Critic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92njCCoUIMxr",
        "colab_type": "code",
        "outputId": "d947004e-7d1c-45e7-c4ad-b6d152fc1c84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, max_action, name):\n",
        "        super(Actor, self).__init__()\n",
        "        \n",
        "        self.l1 = nn.Linear(state_dim, 400)\n",
        "        self.l2 = nn.Linear(400, 300)\n",
        "        self.l3 = nn.Linear(300, action_dim)\n",
        "        \n",
        "        self.max_action = max_action\n",
        "        \n",
        "        self.fname = name + '.pth'\n",
        "        \n",
        "    def forward(self, state):\n",
        "        a = F.relu(self.l1(state))\n",
        "        a = F.relu(self.l2(a))\n",
        "        a = torch.tanh(self.l3(a)) * self.max_action\n",
        "        return a\n",
        "    \n",
        "    \n",
        "        \n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, name, probability=False):\n",
        "        super(Critic, self).__init__()\n",
        "        \n",
        "        self.l1 = nn.Linear(state_dim + action_dim, 400)\n",
        "        self.l2 = nn.Linear(400, 300)\n",
        "        self.l3 = nn.Linear(300, 1)\n",
        "        \n",
        "        self.probability = probability\n",
        "        \n",
        "        self.name = name\n",
        "        self.fname = name + '.pth'\n",
        "        \n",
        "    def forward(self, state, action):\n",
        "        state_action = torch.cat([state, action], 1)\n",
        "        \n",
        "        q = F.relu(self.l1(state_action))\n",
        "        q = F.relu(self.l2(q))\n",
        "        q = self.l3(q)\n",
        "        if self.probability:\n",
        "            q = torch.sigmoid(q)\n",
        "        return q\n",
        "    "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHv86zi1IR8G",
        "colab_type": "text"
      },
      "source": [
        "## Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39B6n_Mq0kfK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TD3:\n",
        "    def __init__(self, lr, state_dim, action_dim, max_action, danger_threshold, directory, fname, epochs_for_danger):\n",
        "        opt_RAdam = True\n",
        "        \n",
        "        self.actor = Actor(state_dim, action_dim, max_action, 'actor').to(device)\n",
        "        self.actor_target = Actor(state_dim, action_dim, max_action, 'actor_target').to(device)\n",
        "        self.actor_perturbed = Actor(state_dim, action_dim, max_action, 'actor_target').to(device)\n",
        "        \n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "        \n",
        "        \n",
        "        if opt_RAdam:\n",
        "            self.actor_optimizer = RAdam(self.actor.parameters(), lr=lr)\n",
        "        else:\n",
        "            self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n",
        "        \n",
        "        \n",
        "        self.critic_1 = Critic(state_dim, action_dim, 'critic_1').to(device)\n",
        "        self.critic_1_target = Critic(state_dim, action_dim, 'critic_1_target').to(device)\n",
        "        self.critic_1_target.load_state_dict(self.critic_1.state_dict())\n",
        "        if opt_RAdam:\n",
        "            self.critic_1_optimizer = RAdam(self.critic_1.parameters(), lr=lr)\n",
        "        else:\n",
        "            self.critic_1_optimizer = optim.Adam(self.critic_1.parameters(), lr=lr)\n",
        "        \n",
        "        self.critic_2 = Critic(state_dim, action_dim, 'critic_2').to(device)\n",
        "        self.critic_2_target = Critic(state_dim, action_dim, 'critic_2_target').to(device)\n",
        "        self.critic_2_target.load_state_dict(self.critic_2.state_dict())\n",
        "        if opt_RAdam:\n",
        "            self.critic_2_optimizer = RAdam(self.critic_2.parameters(), lr=lr)\n",
        "        else:\n",
        "            self.critic_2_optimizer = optim.Adam(self.critic_2.parameters(), lr=lr)\n",
        "        \n",
        "        self.max_action = max_action\n",
        "        \n",
        "        #danger\n",
        "        self.actor_danger = Actor(state_dim, action_dim, max_action, 'actor_danger').to(device)\n",
        "        if opt_RAdam:\n",
        "            self.actor_danger_optimizer = RAdam(self.actor_danger.parameters(), lr=lr)\n",
        "        else:\n",
        "            self.actor_danger_optimizer = optim.Adam(self.actor_danger.parameters(), lr=lr)\n",
        "        \n",
        "        self.critic_danger = Critic(state_dim, action_dim, 'critic_danger', probability=True).to(device)\n",
        "        if opt_RAdam:\n",
        "            self.critic_danger_optimizer = RAdam(self.critic_danger.parameters(), lr=lr)\n",
        "        else:       \n",
        "            self.critic_danger_optimizer = optim.Adam(self.critic_danger.parameters(), lr=lr)\n",
        "        \n",
        "        self.threshold = danger_threshold\n",
        "        self.directory = directory\n",
        "        self.fname = fname\n",
        "        self.epochs_for_danger = epochs_for_danger\n",
        "        self.action_update = False\n",
        "          \n",
        "    \n",
        "    def select_action(self, state, danger=False, param_noise=False, debug=False):\n",
        "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)        \n",
        "        \n",
        "        if param_noise:\n",
        "            self.actor_perturbed.eval()\n",
        "            action = self.actor_perturbed(state)\n",
        "            self.actor_perturbed.train()\n",
        "        else:\n",
        "            self.actor.eval()\n",
        "            action = self.actor(state)\n",
        "            self.actor.train()\n",
        "            \n",
        "        if danger:\n",
        "            self.action_update = False\n",
        "            \n",
        "            # estimate probability of death\n",
        "            init_prob_danger = self.critic_danger(state, action).cpu().data.numpy().flatten()\n",
        "            if  init_prob_danger > self.threshold:\n",
        "                \n",
        "                # safe action in danger case\n",
        "                self.actor_danger.eval() # set evaluation mode\n",
        "                action = self.actor_danger(state)\n",
        "                self.actor_danger.train() # set vack train mode\n",
        "                \n",
        "                self.action_update = True\n",
        "                \n",
        "                if debug:\n",
        "                    new_prob_danger = self.critic_danger(state, action).cpu().data.numpy().flatten()\n",
        "                    print( \"\\t\\t\\tP before: {}. P after: {}\".format(init_prob_danger, new_prob_danger))\n",
        "        \n",
        "        return action.cpu().data.numpy().flatten()\n",
        "    \n",
        "    \n",
        "    def perturb_actor_parameters(self, param_noise):\n",
        "        \"\"\"Apply parameter noise to actor model, for exploration\"\"\"\n",
        "        hard_update(self.actor_perturbed, self.actor)\n",
        "        params = self.actor_perturbed.state_dict()\n",
        "        for name in params:\n",
        "            if 'ln' in name: \n",
        "                pass \n",
        "            param = params[name]\n",
        "            random = torch.randn(param.shape).to(device)\n",
        "            \n",
        "            param += random * param_noise.current_stddev\n",
        "        \n",
        "    \n",
        "    def update(self, replay_buffer, replay_buffer_danger, n_iter, batch_size, batch_size_danger, gamma, polyak, policy_noise, noise_clip, policy_delay):\n",
        "        \n",
        "        for i in range(n_iter):\n",
        "            # Sample a batch of transitions from replay buffer:\n",
        "            state, action_, reward, next_state, done = replay_buffer.sample(batch_size)            \n",
        "            state = torch.FloatTensor(state).to(device)\n",
        "            action = torch.FloatTensor(action_).to(device)\n",
        "            reward = torch.FloatTensor(reward).reshape((batch_size,1)).to(device)\n",
        "            next_state = torch.FloatTensor(next_state).to(device)\n",
        "            done = torch.FloatTensor(done).reshape((batch_size,1)).to(device)\n",
        "                                    \n",
        "            # Select next action according to target policy:\n",
        "            noise = torch.FloatTensor(action_).data.normal_(0, policy_noise).to(device)\n",
        "            noise = noise.clamp(-noise_clip, noise_clip)\n",
        "            next_action = (self.actor_target(next_state) + noise)\n",
        "            next_action = next_action.clamp(-self.max_action, self.max_action)\n",
        "            \n",
        "            # Compute target Q-value:\n",
        "            target_Q1 = self.critic_1_target(next_state, next_action)\n",
        "            target_Q2 = self.critic_2_target(next_state, next_action)\n",
        "            target_Q = torch.min(target_Q1, target_Q2)\n",
        "            target_Q = reward + ((1-done) * gamma * target_Q).detach()\n",
        "            \n",
        "                                    \n",
        "            # Optimize Critic 1:\n",
        "            current_Q1 = self.critic_1(state, action)\n",
        "            loss_Q1 = F.mse_loss(current_Q1, target_Q)\n",
        "            self.critic_1_optimizer.zero_grad()\n",
        "            loss_Q1.backward()\n",
        "            self.critic_1_optimizer.step()\n",
        "            \n",
        "            # Optimize Critic 2:\n",
        "            current_Q2 = self.critic_2(state, action)\n",
        "            loss_Q2 = F.mse_loss(current_Q2, target_Q)\n",
        "            self.critic_2_optimizer.zero_grad()\n",
        "            loss_Q2.backward()\n",
        "            self.critic_2_optimizer.step()\n",
        "            \n",
        "            \n",
        "            # Delayed policy updates:\n",
        "            if i % policy_delay == 0:\n",
        "                # Compute actor loss:\n",
        "                actor_loss = -self.critic_1(state, self.actor(state)).mean()\n",
        "                \n",
        "                # Optimize the actor\n",
        "                self.actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                self.actor_optimizer.step()                \n",
        "                \n",
        "                # Polyak averaging update:\n",
        "                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "                    target_param.data.copy_( (polyak * target_param.data) + ((1-polyak) * param.data))\n",
        "                \n",
        "                for param, target_param in zip(self.critic_1.parameters(), self.critic_1_target.parameters()):\n",
        "                    target_param.data.copy_( (polyak * target_param.data) + ((1-polyak) * param.data))\n",
        "                \n",
        "                for param, target_param in zip(self.critic_2.parameters(), self.critic_2_target.parameters()):\n",
        "                    target_param.data.copy_( (polyak * target_param.data) + ((1-polyak) * param.data))\n",
        "                  \n",
        "                  \n",
        "        batch_steps = max(1, replay_buffer_danger.size // batch_size_danger)\n",
        "        \n",
        "        if len(replay_buffer_danger.buffer) > 0:\n",
        "            for _ in range(self.epochs_for_danger):\n",
        "                for j in range(batch_steps):\n",
        "                    # Sample two batches of transitions: deadend and normals\n",
        "                    state_not_danger, action_not_danger, _, _, done_not_danger = replay_buffer.sample(batch_size_danger)            \n",
        "                    state_not_danger = torch.FloatTensor(state_not_danger).to(device)\n",
        "                    action_not_danger = torch.FloatTensor(action_not_danger).to(device)\n",
        "                    done_not_danger = torch.FloatTensor(done_not_danger).reshape((batch_size_danger, 1)).to(device)\n",
        "            \n",
        "                    state_danger, action_danger, _, _, done_danger = replay_buffer_danger.sample(batch_size_danger)            \n",
        "                    state_danger = torch.FloatTensor(state_danger).to(device)\n",
        "                    action_danger = torch.FloatTensor(action_danger).to(device)\n",
        "                    done_danger = torch.FloatTensor(done_danger).reshape((batch_size_danger, 1)).to(device)\n",
        "                  \n",
        "                    # Compute danger probabilities\n",
        "                    target_Q_not_danger = done_not_danger\n",
        "                    target_Q_danger = done_danger\n",
        "                    #pprint(\"dan_q\", target_Q_not_danger, target_Q_danger)\n",
        "                  \n",
        "                    # Optimize Critic Danger:\n",
        "                    current_Q_danger = self.critic_danger(state_danger, action_danger)\n",
        "                    current_Q_not_danger = self.critic_danger(state_not_danger, action_not_danger)\n",
        "                    loss_Q_danger = F.mse_loss(current_Q_danger, target_Q_danger)\n",
        "                    loss_Q_not_danger = F.mse_loss(current_Q_not_danger, target_Q_not_danger)\n",
        "                    loss_QD =(loss_Q_danger + loss_Q_not_danger)/2\n",
        "                    self.critic_danger_optimizer.zero_grad()\n",
        "                    loss_QD.backward()\n",
        "                    self.critic_danger_optimizer.step()\n",
        "                    \n",
        "                    if j % policy_delay == 0:\n",
        "                        actor_danger_loss = self.critic_danger(state_danger, self.actor_danger(state_danger)).mean()\n",
        "                    \n",
        "                        # Optimize the actor for danger\n",
        "                        self.actor_danger_optimizer.zero_grad()\n",
        "                        actor_danger_loss.backward()\n",
        "                        self.actor_danger_optimizer.step()           \n",
        "                     \n",
        "                \n",
        "    def save(self, directory=None, fname=None, optimizers = False, danger = False):\n",
        "        if directory is None:\n",
        "            directory = self.directory\n",
        "        if fname is None:\n",
        "            fname = self.fname\n",
        "            \n",
        "        base_path = \"%s/%s_\"% (directory, fname)\n",
        "        \n",
        "        torch.save(self.actor.state_dict(), base_path + self.actor.fname)\n",
        "        torch.save(self.actor_target.state_dict(), base_path + self.actor_target.fname)\n",
        "        \n",
        "        torch.save(self.critic_1.state_dict(), base_path + self.critic_1.fname)\n",
        "        torch.save(self.critic_1_target.state_dict(), base_path + self.critic_1_target.fname)\n",
        "        \n",
        "        torch.save(self.critic_2.state_dict(), base_path + self.critic_2.fname)\n",
        "        torch.save(self.critic_2_target.state_dict(), base_path + self.critic_2_target.fname)\n",
        "        \n",
        "        if danger:\n",
        "            torch.save(self.actor_danger.state_dict(),  base_path + self.actor_danger.fname)\n",
        "            torch.save(self.critic_danger.state_dict(), base_path + self.critic_danger.fname)\n",
        "        \n",
        "        if optimizers:\n",
        "            torch.save(self.actor_optimizer.state_dict(), '%s/%s_actor_optimizer.pth' % (directory, fname))\n",
        "            torch.save(self.critic_1_optimizer.state_dict(), '%s/%s_critic_1_optimizer.pth' % (directory, fname))\n",
        "            torch.save(self.critic_2_optimizer.state_dict(), '%s/%s_critic_2_optimizer.pth' % (directory, fname))\n",
        "            if danger:\n",
        "                torch.save(self.actor_danger_optimizer.state_dict(), '%s/%s_actor_danger_optimizer.pth' % (directory, fname))\n",
        "                torch.save(self.critic_danger_optimizer.state_dict(), '%s/%s_critic_danger_optimizer.pth' % (directory, fname))\n",
        "                \n",
        "        \n",
        "    def load(self, directory=None, fname=None, optimizers=False, danger = False):\n",
        "        if directory is None:\n",
        "            directory = self.directory\n",
        "        if fname is None:\n",
        "            fname = self.fname\n",
        "            \n",
        "        base_path = \"%s/%s_\"% (directory, fname)\n",
        "        \n",
        "        self.actor.load_state_dict(torch.load(base_path + self.actor.fname, map_location=lambda storage, loc: storage))\n",
        "        self.actor_target.load_state_dict(torch.load(base_path + self.actor_target.fname, map_location=lambda storage, loc: storage))\n",
        "        \n",
        "        self.critic_1.load_state_dict(torch.load(base_path + self.critic_1.fname, map_location=lambda storage, loc: storage))\n",
        "        self.critic_1_target.load_state_dict(torch.load(base_path + self.critic_1_target.fname, map_location=lambda storage, loc: storage))\n",
        "        \n",
        "        self.critic_2.load_state_dict(torch.load(base_path + self.critic_2.fname, map_location=lambda storage, loc: storage))\n",
        "        self.critic_2_target.load_state_dict(torch.load(base_path + self.critic_2_target.fname, map_location=lambda storage, loc: storage))\n",
        "        \n",
        "        if danger:\n",
        "            self.actor_danger.load_state_dict(torch.load('%s/%s_actor_danger.pth' % (directory, name), map_location=lambda storage, loc: storage))\n",
        "            self.critic_danger.load_state_dict(torch.load('%s/%s_critic_danger.pth' % (directory, name), map_location=lambda storage, loc: storage))\n",
        "        \n",
        "        if optimizers:\n",
        "            self.actor_optimizer.load_state_dict(torch.load( '%s/%s_actor_optimizer.pth' % (directory, name), map_location=lambda storage, loc: storage))\n",
        "            self.critic_1_optimizer.load_state_dict(torch.load('%s/%s_critic_1_optimizer.pth' % (directory, name), map_location=lambda storage, loc: storage))\n",
        "            self.critic_2_optimizer.load_state_dict(torch.load('%s/%s_critic_2_optimizer.pth' % (directory, name), map_location=lambda storage, loc: storage))\n",
        "            if danger:\n",
        "                self.actor_danger_optimizer.load_state_dict(torch.load(base_path + self.actor_danger.fname, map_location=lambda storage, loc: storage))\n",
        "                self.critic_danger_optimizer.load_state_dict(torch.load(base_path + self.critic_danger.fname, map_location=lambda storage, loc: storage))\n",
        "        \n",
        "        \n",
        "    def load_actor(self, directory=None, fname=None, danger=False):\n",
        "        if directory is None:\n",
        "            directory = self.directory\n",
        "        if fname is None:\n",
        "            fname = self.fname      \n",
        "      \n",
        "        base_path = \"%s/%s_\"% (directory, fname)\n",
        "        self.actor.load_state_dict(torch.load(base_path + self.actor.fname, map_location=lambda storage, loc: storage))\n",
        "        self.actor_target.load_state_dict(torch.load(base_path + self.actor_target.fname, map_location=lambda storage, loc: storage))\n",
        "        if danger:\n",
        "            self.actor_danger.load_state_dict(torch.load(base_path + self.actor_danger.fname, map_location=lambda storage, loc: storage))\n",
        "            self.critic_danger.load_state_dict(torch.load(base_path + self.critic_danger.fname, map_location=lambda storage, loc: storage))\n",
        "        \n",
        "        \n",
        "        \n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pr_4tXGr5Qzj",
        "colab_type": "text"
      },
      "source": [
        "##Custom Bipedal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQj8Rpz3yw2P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import warnings\n",
        "from gym.envs.box2d.bipedal_walker import *\n",
        "class CustomizableBipedalWalker(BipedalWalker):\n",
        "    def __init__(self):\n",
        "        self.default_params = {\n",
        "            'stump_height_low': 1,\n",
        "            'stump_height_high': 3,\n",
        "            'pit_depth': 4,\n",
        "            'pit_width_low': 3,\n",
        "            'pit_width_high': 5,\n",
        "            'stair_heights': [-.5, .5],\n",
        "            'stair_width_low': 4,\n",
        "            'stair_width_high': 5,\n",
        "            'stair_steps_low': 3,\n",
        "            'stair_steps_high': 5,\n",
        "            'states': [0],\n",
        "            'state_probs': None\n",
        "        }\n",
        "        self.params = {**self.default_params}\n",
        "        BipedalWalker.__init__(self)\n",
        "        \n",
        "    def _update_env_params(self, **kwargs):\n",
        "        # TODO: add kind of sanity check here\n",
        "        self.params = {**self.params, **kwargs}\n",
        "        _ = self.reset()\n",
        "        \n",
        "    def reset_env_params(self, hardcore=False):\n",
        "        params = {**self.default_params}\n",
        "        if hardcore:\n",
        "            params['states'] = np.arange(4)\n",
        "        self._update_env_params(**params)\n",
        "    \n",
        "    def set_env_states(self, state_mask, p=None):\n",
        "        \"\"\"\n",
        "        :param state_mask: np.array(,dtype=bool) that masks [\"GRASS\", \"STUMP\", \"STAIRS\", \"PIT\"].\n",
        "            Note that masking out \"GRASS\" takes no effect.\n",
        "        :param p: np.array or list of probabilities: [p_grass, p_stump, p_stairs, p_pit].\n",
        "            Probs corresponding to masked out states are ignored\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        states_ = np.arange(4)[state_mask]\n",
        "        p_ = None\n",
        "        if p is not None:\n",
        "            p_ = np.array(p)\n",
        "            if not np.all(p_ >= 0):\n",
        "                raise ValueError\n",
        "            p_ = p_[state_mask] / p_[state_mask].sum()\n",
        "        self._update_env_params(states=states_, state_probs=p_)\n",
        "    \n",
        "    def set_env_params(self, pit_width=None, stair_width=None, stair_steps=None, stump_height=None):\n",
        "        \"\"\"\n",
        "            NB: All params are integers or tuples of integers\n",
        "        \"\"\"\n",
        "        kwargs = {**locals()}\n",
        "        _ = kwargs.pop('self', None)\n",
        "        params = {}\n",
        "        for k,v in kwargs.items():\n",
        "            if type(v) is int:\n",
        "                params[k + '_low'] = v\n",
        "                params[k + '_high'] = v + 1\n",
        "            elif isinstance(v, (tuple, list)): \n",
        "                if v[1] - v[0] >= 1:\n",
        "                    params[k + '_low'] = v[0]\n",
        "                    params[k + '_high'] = v[1]\n",
        "                else:\n",
        "                    warnings.warn(f'{k} shoud be an integer. {k}[1] - {k}[0] < 1 '+\\\n",
        "                                  f'=> will set {k}_low = {v[0]}, {k}_high = {v[0]+1}')\n",
        "                    params[k + '_low'] = v[0]\n",
        "                    params[k + '_high'] = v[0] + 1\n",
        "        self._update_env_params(**params)\n",
        "        \n",
        "    def _generate_terrain(self, hardcore=True):\n",
        "        GRASS, STUMP, STAIRS, PIT, _STATES_ = range(5)\n",
        "        state    = GRASS\n",
        "        velocity = 0.0\n",
        "        y        = TERRAIN_HEIGHT\n",
        "        counter  = TERRAIN_STARTPAD\n",
        "        oneshot  = False\n",
        "        self.terrain   = []\n",
        "        self.terrain_x = []\n",
        "        self.terrain_y = []\n",
        "        for i in range(TERRAIN_LENGTH):\n",
        "            x = i*TERRAIN_STEP\n",
        "            self.terrain_x.append(x)\n",
        "\n",
        "            if state==GRASS and not oneshot:\n",
        "                velocity = 0.8*velocity + 0.01*np.sign(TERRAIN_HEIGHT - y)\n",
        "                if i > TERRAIN_STARTPAD: velocity += self.np_random.uniform(-1, 1)/SCALE   #1\n",
        "                y += velocity\n",
        "\n",
        "            elif state==PIT and oneshot:\n",
        "                counter = self.np_random.randint(self.params['pit_width_low'], \n",
        "                                                 self.params['pit_width_high'])\n",
        "                PIT_H = self.params['pit_depth']\n",
        "                poly = [\n",
        "                    (x,              y),\n",
        "                    (x+TERRAIN_STEP, y),\n",
        "                    (x+TERRAIN_STEP, y-PIT_H*TERRAIN_STEP),\n",
        "                    (x,              y-PIT_H*TERRAIN_STEP),\n",
        "                    ]\n",
        "                self.fd_polygon.shape.vertices=poly\n",
        "                t = self.world.CreateStaticBody(\n",
        "                    fixtures = self.fd_polygon)\n",
        "                t.color1, t.color2 = (1,1,1), (0.6,0.6,0.6)\n",
        "                self.terrain.append(t)\n",
        "\n",
        "                self.fd_polygon.shape.vertices=[(p[0]+TERRAIN_STEP*counter,p[1]) for p in poly]\n",
        "                t = self.world.CreateStaticBody(\n",
        "                    fixtures = self.fd_polygon)\n",
        "                t.color1, t.color2 = (1,1,1), (0.6,0.6,0.6)\n",
        "                self.terrain.append(t)\n",
        "                counter += 2\n",
        "                original_y = y\n",
        "\n",
        "            elif state==PIT and not oneshot:\n",
        "                y = original_y\n",
        "                if counter > 1:\n",
        "                    y -= PIT_H*TERRAIN_STEP\n",
        "\n",
        "            elif state==STUMP and oneshot:\n",
        "                counter = self.np_random.randint(self.params['stump_height_low'], self.params['stump_height_high'])\n",
        "                poly = [\n",
        "                    (x,                      y),\n",
        "                    (x+counter*TERRAIN_STEP, y),\n",
        "                    (x+counter*TERRAIN_STEP, y+counter*TERRAIN_STEP),\n",
        "                    (x,                      y+counter*TERRAIN_STEP),\n",
        "                    ]\n",
        "                self.fd_polygon.shape.vertices=poly\n",
        "                t = self.world.CreateStaticBody(\n",
        "                    fixtures = self.fd_polygon)\n",
        "                t.color1, t.color2 = (1,1,1), (0.6,0.6,0.6)\n",
        "                self.terrain.append(t)\n",
        "\n",
        "            elif state==STAIRS and oneshot:\n",
        "                stair_height = self.np_random.choice(self.params['stair_heights'])\n",
        "                stair_width = self.np_random.randint(self.params['stair_width_low'], \n",
        "                                                     self.params['stair_width_high'])\n",
        "                stair_steps = self.np_random.randint(self.params['stair_steps_low'], \n",
        "                                                     self.params['stair_steps_high'])\n",
        "                original_y = y\n",
        "                for s in range(stair_steps):\n",
        "                    poly = [\n",
        "                        (x+(    s*stair_width)*TERRAIN_STEP, y+(   s*stair_height)*TERRAIN_STEP),\n",
        "                        (x+((1+s)*stair_width)*TERRAIN_STEP, y+(   s*stair_height)*TERRAIN_STEP),\n",
        "                        (x+((1+s)*stair_width)*TERRAIN_STEP, y+(-1+s*stair_height)*TERRAIN_STEP),\n",
        "                        (x+(    s*stair_width)*TERRAIN_STEP, y+(-1+s*stair_height)*TERRAIN_STEP),\n",
        "                        ]\n",
        "                    self.fd_polygon.shape.vertices=poly\n",
        "                    t = self.world.CreateStaticBody(\n",
        "                        fixtures = self.fd_polygon)\n",
        "                    t.color1, t.color2 = (1,1,1), (0.6,0.6,0.6)\n",
        "                    self.terrain.append(t)\n",
        "                counter = stair_steps*stair_width\n",
        "\n",
        "            elif state==STAIRS and not oneshot:\n",
        "                s = stair_steps*stair_width - counter - stair_height\n",
        "                n = s/stair_width\n",
        "                y = original_y + (n*stair_height)*TERRAIN_STEP\n",
        "\n",
        "            oneshot = False\n",
        "            self.terrain_y.append(y)\n",
        "            counter -= 1\n",
        "            if counter==0:\n",
        "                counter = self.np_random.randint(TERRAIN_GRASS/2, TERRAIN_GRASS)\n",
        "                if state==GRASS:\n",
        "                    state = self.np_random.choice(self.params['states'], p=self.params['state_probs'])\n",
        "                    oneshot = True\n",
        "                else:\n",
        "                    state = GRASS\n",
        "                    oneshot = True\n",
        "\n",
        "        self.terrain_poly = []\n",
        "        for i in range(TERRAIN_LENGTH-1):\n",
        "            poly = [\n",
        "                (self.terrain_x[i],   self.terrain_y[i]),\n",
        "                (self.terrain_x[i+1], self.terrain_y[i+1])\n",
        "                ]\n",
        "            self.fd_edge.shape.vertices=poly\n",
        "            t = self.world.CreateStaticBody(\n",
        "                fixtures = self.fd_edge)\n",
        "            color = (0.3, 1.0 if i%2==0 else 0.8, 0.3)\n",
        "            t.color1 = color\n",
        "            t.color2 = color\n",
        "            self.terrain.append(t)\n",
        "            color = (0.4, 0.6, 0.3)\n",
        "            poly += [ (poly[1][0], 0), (poly[0][0], 0) ]\n",
        "            self.terrain_poly.append( (poly, color) )\n",
        "        self.terrain.reverse()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rqNDNCWf-Z3",
        "colab_type": "text"
      },
      "source": [
        "## Noise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_QrGdK9gC32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AdaptiveParamNoiseSpec(object):\n",
        "    def __init__(self, initial_stddev=0.1, desired_action_stddev=0.2, adaptation_coefficient=1.01):\n",
        "        \"\"\"\n",
        "        Note that initial_stddev and current_stddev refer to std of parameter noise, \n",
        "        but desired_action_stddev refers to (as name notes) desired std in action space\n",
        "        \"\"\"\n",
        "        self.initial_stddev = initial_stddev\n",
        "        self.desired_action_stddev = desired_action_stddev\n",
        "        self.adaptation_coefficient = adaptation_coefficient\n",
        "\n",
        "        self.current_stddev = initial_stddev\n",
        "\n",
        "    def adapt(self, sq_distance):\n",
        "        \"\"\"\n",
        "        Expaects \n",
        "        \"\"\"\n",
        "        \n",
        "        if sq_distance > self.desired_action_stddev ** 2:\n",
        "            # Decrease stddev.\n",
        "            self.current_stddev /= self.adaptation_coefficient\n",
        "        else:\n",
        "            # Increase stddev.\n",
        "            self.current_stddev *= self.adaptation_coefficient\n",
        "\n",
        "    def get_stats(self):\n",
        "        stats = {\n",
        "            'param_noise_stddev': self.current_stddev,\n",
        "        }\n",
        "        return stats\n",
        "\n",
        "    def __repr__(self):\n",
        "        fmt = 'AdaptiveParamNoiseSpec(initial_stddev={}, desired_action_stddev={}, adaptation_coefficient={})'\n",
        "        return fmt.format(self.initial_stddev, self.desired_action_stddev, self.adaptation_coefficient)\n",
        "\n",
        "def ddpg_sq_distance_metric(actions1, actions2):\n",
        "    \"\"\"\n",
        "    Compute SQUARE of the \"distance\" between actions taken by two policies at the same states\n",
        "    Expects numpy arrays\n",
        "    \"\"\"\n",
        "    diff = actions1-actions2\n",
        "    mean_diff = np.mean(np.square(diff), axis=0)\n",
        "    sq_dist = np.mean(mean_diff)\n",
        "    return sq_dist\n",
        "\n",
        "\n",
        "def hard_update(target, source):\n",
        "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
        "           target_param.data.copy_(param.data)\n",
        "            \n",
        "            \n",
        "class Noise:\n",
        "    def __init__(self, dim):\n",
        "        self.dim = dim\n",
        "    \n",
        "        #normal\n",
        "    \n",
        "        #ou\n",
        "    \n",
        "    \n",
        "    def get_noise(self, n_type='normal'):\n",
        "        if n_type == 'normal':\n",
        "            return \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oC4_CZH8IET9",
        "colab_type": "text"
      },
      "source": [
        "## Replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkalC0GW0zcf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size=1e6, base_path=None, name=None):\n",
        "        self.buffer = []\n",
        "        self.max_size = int(max_size)\n",
        "        self.size = 0\n",
        "        \n",
        "        if base_path is not None:\n",
        "            self.base_path = base_path\n",
        "        else:\n",
        "            self.base_path = ''\n",
        "            \n",
        "        if name is not None:\n",
        "            self.fname = name + '.pth'\n",
        "        else:\n",
        "            self.fname = 'buffer.pth'\n",
        "            \n",
        "        \n",
        "    \n",
        "    def add(self, transition):\n",
        "        self.size +=1\n",
        "        # transiton is tuple of (state, action, reward, next_state, done)\n",
        "        self.buffer.append(transition)\n",
        "       \n",
        "    \n",
        "    def save(self):\n",
        "        try:\n",
        "            with open(self.basename + self.fname , 'wb') as f:\n",
        "                pickle.dump(self.buffer, f)\n",
        "        except OSError:\n",
        "            print('Buffer is not saved!\\n\\n')\n",
        "            \n",
        "            \n",
        "    def load(self):\n",
        "        try:\n",
        "            with open(self.basename + self.fname , 'rb') as f:\n",
        "                self.buffer = pickle.load(f)\n",
        "        except OSError:\n",
        "            self.buffer = []\n",
        "            print('Buffer is not loaded!\\n\\n')\n",
        "    \n",
        "          \n",
        "    def sample(self, batch_size):\n",
        "        # delete 1/5th of the buffer when full\n",
        "        if self.size > self.max_size:\n",
        "            del self.buffer[0:int(self.size/5)]\n",
        "            self.size = len(self.buffer)\n",
        "        \n",
        "        indexes = np.random.randint(0, len(self.buffer), size=batch_size)\n",
        "        state, action, reward, next_state, done = [], [], [], [], []\n",
        "        \n",
        "        for i in indexes:\n",
        "            s, a, r, s_, d = self.buffer[i]\n",
        "            state.append(np.array(s, copy=False))\n",
        "            action.append(np.array(a, copy=False))\n",
        "            reward.append(np.array(r, copy=False))\n",
        "            next_state.append(np.array(s_, copy=False))\n",
        "            done.append(np.array(d, copy=False))\n",
        "        \n",
        "        return np.array(state), np.array(action), np.array(reward), np.array(next_state), np.array(done)\n",
        "     \n",
        "    def get_last(self, amount):\n",
        "        state, action, reward, next_state, done = [], [], [], [], []\n",
        "        if amount < len(self.buffer):\n",
        "            start = len(self.buffer) - amount\n",
        "        else:\n",
        "            start = 0\n",
        "        for i in range(start,len(self.buffer)):\n",
        "            s, a, r, s_, d = self.buffer[i]\n",
        "            state.append(np.array(s, copy=False))\n",
        "            action.append(np.array(a, copy=False))\n",
        "            reward.append(np.array(r, copy=False))\n",
        "            next_state.append(np.array(s_, copy=False))\n",
        "            done.append(np.array(d, copy=False))\n",
        "        return s, a, r, s_, d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61VeKABEm9_O",
        "colab_type": "text"
      },
      "source": [
        "## Trajectory buffer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CEo4_gcm-kh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import deque\n",
        "class TrajectoriesEndBuffer():\n",
        "    def __init__(self, max_trajectory_size=10, init_prob=0.7):\n",
        "        assert type(max_trajectory_size) is int\n",
        "        assert max_trajectory_size > 0\n",
        "        \n",
        "        self.max_trajectory_size = max_trajectory_size\n",
        "        self.buffer = deque()\n",
        "        \n",
        "        #linear probability params\n",
        "        assert type(init_prob) is float\n",
        "        assert 0 <= init_prob <= 1\n",
        "        self.min_prob = init_prob\n",
        "        \n",
        "        #TRY other cases\n",
        "        \n",
        "    def add(self, transition):\n",
        "        if len(self.buffer) >= self.max_trajectory_size:\n",
        "            self.buffer.popleft()    \n",
        "        self.buffer.append(transition)\n",
        "    \n",
        "    \n",
        "    def get_heruistc_probabilities(self):\n",
        "        cur_len = len(self.buffer)\n",
        "        if cur_len > 0 and self.max_trajectory_size == 1:\n",
        "            return [1]\n",
        "\n",
        "        if cur_len < self.max_trajectory_size:\n",
        "            shift = self.max_trajectory_size - cur_len\n",
        "        else:\n",
        "            shift = 0\n",
        "        prob = [0] * cur_len\n",
        "        \n",
        "        # linear from\n",
        "        min_prob = self.min_prob\n",
        "        for i in range(cur_len):\n",
        "            prob[i] = min_prob + (1 - min_prob) * ( (i + shift) / (self.max_trajectory_size - 1) )\n",
        "            \n",
        "        return prob\n",
        "    \n",
        "    def transfer_to_replay_buffer(self, replay_buffer):\n",
        "        if len(self.buffer) == 0: \n",
        "            print('Trajectories buffer is empty')\n",
        "            return\n",
        "        \n",
        "        if self.buffer[-1][4] == False:\n",
        "            print('Trajectory is not finished')\n",
        "            return\n",
        "          \n",
        "        cur_len = len(self.buffer)\n",
        "        prob = self.get_heruistc_probabilities()\n",
        "        \n",
        "        for i in range(cur_len):\n",
        "            s1, a, r, s2, done = self.buffer[i]\n",
        "            replay_buffer.add((s1, a, r, s2, float(prob[i])))\n",
        "        \n",
        "        self.buffer = deque()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEEchoyDrpAR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzz69TYi7f2q",
        "colab_type": "text"
      },
      "source": [
        "## Research class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqj3YyUT7ebB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random \n",
        "class Research:\n",
        "    def set_danger_batch_size(self):\n",
        "        self.batch_size_danger = self.batch_size // 2\n",
        "        \n",
        "    def update_params(self, params):\n",
        "        param_names = ['random_seed', \n",
        "                       'max_episodes', 'max_timesteps', 'batch_size', \n",
        "                       'lr', 'polyak', 'policy_delay', 'gamma', 'log_interval', \n",
        "                       'batch_size_danger', \n",
        "                       'policy_noise',\n",
        "                       'danger_enable', 'danger_threshold', 'max_trajectory_size',\n",
        "                       'directory','filename_load', 'filename_save', 'epochs_for_danger',\n",
        "                       'noise_random_enable', 'exploration_noise', 'noise_clip',\n",
        "                       'max_iter_danger', 'evaluate_after_episodes', 'evaluate_average_on_episodes', 'env_name_load', \n",
        "                       'env_name',\n",
        "                       'param_noise_enable','initial_stddev','desired_action_stddev','adaptation_coefficient']\n",
        "        \n",
        "        for param_name in param_names:\n",
        "            if param_name in params:\n",
        "                \n",
        "                setattr(self, param_name, params[param_name])\n",
        "                print(\"{} set to {}\".format(param_name, params[param_name]))\n",
        "        \n",
        "        \n",
        "    def __init__(self, params):\n",
        "        ###########################################\n",
        "        ###         default values\n",
        "        ###########################################\n",
        "        \n",
        "        self.random_seed = 1\n",
        "        \n",
        "        # environment\n",
        "        self.env_rewards = {'BipedalWalkerHardcore-v2':[-100,300], 'BipedalWalker-v2':[-100,300],\n",
        "                            'LunarLanderContinuous-v2':[-100,200],'Pendulum-v0':[-1000,200]}\n",
        "\n",
        "        self.env_name_load = 'BipedalWalker-v2'\n",
        "        self.env_name = 'BipedalWalker-v2'\n",
        "        \n",
        "        if self.env_name in self.env_rewards:\n",
        "            self.terminal_reward = self.env_rewards[self.env_name][0]\n",
        "            self.passed_reward   = self.env_rewards[self.env_name][1]\n",
        "        else:\n",
        "            self.terminal_reward = None\n",
        "            self.passed_reward   = None\n",
        "        \n",
        "        self.env = None\n",
        "        \n",
        "        ## training \n",
        "        self.max_episodes = 1000         # max num of episodes\n",
        "        self.max_timesteps = 2000        # max timesteps in one episode\n",
        "        self.batch_size = 100            # num of transitions sampled from replay buffer\n",
        "        self.lr = 0.001\n",
        "        self.polyak = 0.995              # target policy update parameter (1-tau)\n",
        "        self.policy_delay = 2            # delayed policy updates parameter\n",
        "        \n",
        "        ## model params\n",
        "        self.gamma = 0.99                # discount for future rewards\n",
        "        \n",
        "        ##############################   \n",
        "        ##   Noise params\n",
        "        ##############################   \n",
        "        \n",
        "        # Random noise\n",
        "        self.noise_random_enable = True\n",
        "        self.exploration_noise = 0.25\n",
        "        self.policy_noise = 0.2          # target policy smoothing noise\n",
        "        self.noise_clip = 0.5\n",
        "        \n",
        "        # Param noise\n",
        "        self.param_noise_enable = False\n",
        "        self.noise_distance_batch_size = 10 ** 4\n",
        "        self.initial_stddev=0.05\n",
        "        self.desired_action_stddev=0.45 \n",
        "        self.adaptation_coefficient=1.03\n",
        "        \n",
        "        ## Danger params\n",
        "        self.danger_enable = True               # enable dnager mode\n",
        "        self.danger_threshold = 0.7      # probability to perform safe action\n",
        "        self.epochs_for_danger = 3       # epochs to train danger\n",
        "        self.max_iter_danger = 5         # iteration per iteration\n",
        "        self.batch_size_danger = self.batch_size // 2\n",
        "        self.max_trajectory_size = 10    # length of trajectory before death\n",
        "        \n",
        "        ## evaluation \n",
        "        self.evaluate_average_on_episodes = 10\n",
        "        self.evaluate_after_episodes = 30\n",
        "        self.evaluate_after_timesteps = 10000\n",
        "        \n",
        "        ## InpOut params\n",
        "        self.log_interval = 10                                              # print avg reward after interval\n",
        "        self.directory = \"/content/drive/My Drive\"                          # save trained models\n",
        "        self.filename_load = \"TD3_{}_{}\".format(self.env_name_load, self.random_seed)\n",
        "        self.filename_save = \"TD3_{}_{}\".format(self.env_name, self.random_seed)\n",
        "        ###########################################\n",
        "        \n",
        "        ## Buffers\n",
        "        self.replay_buffer = None\n",
        "        self.replay_buffer_dead = None\n",
        "        self.trajectory_buffer = None\n",
        "        \n",
        "        \n",
        "        ## Policy\n",
        "        self.policy = None\n",
        "        \n",
        "        ###########################################\n",
        "        ## Update default params\n",
        "        ###########################################\n",
        "        self.update_params(params)\n",
        "        \n",
        "        \n",
        "    def load_model(self, env_name, random_seed):\n",
        "        pass\n",
        "        \n",
        "        \n",
        "    def init_env(self, params = None):\n",
        "        if params:\n",
        "            self.update_env(params)\n",
        "            \n",
        "        # create env\n",
        "        if self.env_name == \"BipedalWalkerHardcore-v2\":\n",
        "            \n",
        "            env = CustomizableBipedalWalker()\n",
        "            env.set_env_params(stump_height=1)\n",
        "            env.set_env_states(state_mask=np.array([1,1,0,0],dtype=bool), p=np.array([0.1,0.9,0.9,0.9]))\n",
        "            \n",
        "            self.env = TimeLimit(env, max_episode_steps=2000)\n",
        "        else:\n",
        "            self.env = gym.make(self.env_name)\n",
        "            \n",
        "        if '_max_episode_steps' in self.env.__dict__:\n",
        "            self.max_timesteps = self.env.__dict__['_max_episode_steps']\n",
        "        else:\n",
        "            self.max_timesteps = 10**7 ## \"inifinte\" value if nothing defined\n",
        "        \n",
        "        # env params\n",
        "        self.state_dim = self.env.observation_space.shape[0]\n",
        "        self.action_dim = self.env.action_space.shape[0]\n",
        "        self.max_action = float(self.env.action_space.high[0])\n",
        "        \n",
        "        #check symmetry of actions\n",
        "        assert  np.all((-self.env.action_space.low) == self.env.action_space.high)\n",
        "        \n",
        "        \n",
        "    def init_buffers_and_agent(self, load, solved):\n",
        "        \n",
        "        # Buffers\n",
        "        self.replay_buffer = ReplayBuffer()\n",
        "        self.replay_buffer_dead = ReplayBuffer()\n",
        "        self.trajectory_buffer = TrajectoriesEndBuffer(self.max_trajectory_size)\n",
        "    \n",
        "        # Policy\n",
        "        self.policy = TD3(self.lr, self.state_dim, self.action_dim, self.max_action, \n",
        "                          self.danger_threshold, self.directory, self.filename_save, self.epochs_for_danger)\n",
        "    \n",
        "    \n",
        "        # load\n",
        "        if load:\n",
        "            full_fname = self.filename_load\n",
        "            if solved:\n",
        "                full_fname += \"_solved\"\n",
        "            self.policy.load(self.directory,  full_fname)\n",
        "           \n",
        "        \n",
        "    def train(self, params, debug=False):\n",
        "        self.update_params(params)\n",
        "        self.first_finish_ep_num  = -1\n",
        "        assert self.policy is not None\n",
        "        assert self.max_timesteps is not None\n",
        "        \n",
        "        # Random seed\n",
        "        if self.random_seed is not None:\n",
        "            os.environ['PYTHONHASHSEED']=str(self.random_seed)\n",
        "            print(\"Random Seed: {}\".format(self.random_seed))\n",
        "            np.random.seed(self.random_seed)\n",
        "            self.env.seed(self.random_seed)\n",
        "            random.seed(self.random_seed)\n",
        "            torch.manual_seed(self.random_seed)\n",
        "         \n",
        "    \n",
        "        # logging variables:\n",
        "        avg_reward = 0\n",
        "        ep_reward = 0\n",
        "        tot_time_steps = 0\n",
        "        average_time_steps = 0\n",
        "    \n",
        "        log_f = open(\"log.txt\",\"w+\")\n",
        "    \n",
        "        # training procedure:\n",
        "        print('Start training')\n",
        "        reach_the_end = False\n",
        "        \n",
        "        param_noise = AdaptiveParamNoiseSpec(initial_stddev=0.05,desired_action_stddev=0.45, adaptation_coefficient=1.05)\n",
        "        # amount of updates in dangers\n",
        "        tot_updates_amount = 0\n",
        "        ep_updates_amount = 0\n",
        "        \n",
        "        for episode in range(1, self.max_episodes + 1):\n",
        "            \n",
        "            ep_updates_amount = 0\n",
        "            state = self.env.reset()        \n",
        "            \n",
        "            if  self.param_noise_enable:\n",
        "                self.policy.perturb_actor_parameters(param_noise)\n",
        "            \n",
        "            for t in range(self.max_timesteps):\n",
        "                # select action and add exploration noise:\n",
        "                action = self.policy.select_action(state, danger=self.danger_enable, param_noise=self.param_noise_enable)\n",
        "                if self.policy.action_update:\n",
        "                    ep_updates_amount += 1\n",
        "                \n",
        "                if self.noise_random_enable:\n",
        "                    action = action + np.random.normal(0, self.exploration_noise, size=self.env.action_space.shape[0])\n",
        "                action = action.clip(self.env.action_space.low, self.env.action_space.high)\n",
        "                \n",
        "                #print(\"action {}\".format(action))\n",
        "                # take action in env:\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "            \n",
        "                self.replay_buffer.add((state, action, reward, next_state, float(done)))\n",
        "                self.trajectory_buffer.add((state, action, reward, next_state, float(done)))\n",
        "            \n",
        "                if reward == self.terminal_reward:\n",
        "                    #replay_buffer_dead.add((state, action, reward, next_state, float(done)))\n",
        "                    self.trajectory_buffer.transfer_to_replay_buffer(self.replay_buffer_dead)                    \n",
        "                    \n",
        "                    if debug:\n",
        "                        tmp_st = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
        "                        tmp_a = torch.FloatTensor(action.reshape(1, -1)).to(device)\n",
        "                        print(\"\\tDead after pair with prob: \", self.policy.critic_danger(tmp_st, tmp_a).cpu().data.numpy().flatten())\n",
        "                    \n",
        "                state = next_state\n",
        "                avg_reward += reward\n",
        "                ep_reward += reward\n",
        "                \n",
        "                #if done and t < self.max_timesteps - 1 and reward != self.terminal_reward and reach_the_end==False:\n",
        "                #    print('Reach the end. Episode: {}. r={} t={}'.format(episode, ep_reward, tot_time_steps + t))\n",
        "                #    reach_the_end = True\n",
        "                #    self.first_finish_ep_num = episode\n",
        "                #    break\n",
        "                \n",
        "                if done or t==(self.max_timesteps-1): \n",
        "                    tot_time_steps += t\n",
        "                    average_time_steps += t\n",
        "                    tot_updates_amount += ep_updates_amount \n",
        "                    \n",
        "                    self.policy.update(self.replay_buffer, self.replay_buffer_dead, t, self.batch_size, self.batch_size_danger, self.gamma, self.polyak, self.policy_noise, self.noise_clip, self.policy_delay)\n",
        "                    if t==(self.max_timesteps - 1):                    \n",
        "                        print(\"\\tStuck, ep reward {}\".format(ep_reward))\n",
        "                    break\n",
        "            \n",
        "            # logging updates:\n",
        "            log_f.write('{},{}\\n'.format(episode, ep_reward))\n",
        "            log_f.flush()\n",
        "            ep_reward = 0\n",
        "            \n",
        "            # if avg reward > 300 then save and stop traning:\n",
        "            if (avg_reward / self.log_interval) >= 300:\n",
        "                print(\"########## Solved! ###########\")\n",
        "                name = self.filename_save + '_solved'\n",
        "                self.policy.save(self.directory, name, optimizers=True, danger=True)\n",
        "                log_f.close()\n",
        "                break\n",
        "            \n",
        "            # update perturbed actor\n",
        "            #print('start perturtbated update')\n",
        "            if self.param_noise_enable:\n",
        "                states, actions_perturbed, _, _, _ = self.replay_buffer.get_last(t)\n",
        "                actions_perturbed = np.array(actions_perturbed, copy=False)\n",
        "            \n",
        "                actions_clear = self.policy.select_action(states, danger=False, param_noise=False)\n",
        "                ddpg_sq_distance = ddpg_sq_distance_metric(actions_perturbed, actions_clear)\n",
        "                param_noise.adapt(ddpg_sq_distance)\n",
        "            #print('end perturtbated update t={}'.format(t))\n",
        "            \n",
        "            # save policy \n",
        "            if episode > 0 and episode % (5 * self.log_interval) == 0:\n",
        "                self.policy.save(self.directory, self.filename_save, optimizers=True, danger=True)\n",
        "            \n",
        "            # print avg reward every log interval:\n",
        "            if episode % self.log_interval == 0:\n",
        "                avg_reward = int(avg_reward / self.log_interval)\n",
        "                average_time_steps = int(average_time_steps / self.log_interval)\n",
        "                print(\"Episode: {}\\tAverage Reward: {}\\tAverage steps: {}\\t Total: {}\".format(episode, avg_reward, average_time_steps, tot_time_steps))\n",
        "                avg_reward = 0\n",
        "                average_time_steps = 0\n",
        "            \n",
        "            if episode % self.evaluate_after_episodes == 0 and episode >= 100:\n",
        "                reach_the_end = self.evaluate(params ={}, time_steps = tot_time_steps, episode = episode)\n",
        "                if reach_the_end:\n",
        "                    self.first_finish_ep_num = episode\n",
        "                    print('Reach the end. Episode: {}. r={:.1f} Steps={} Updated steps={}'.format(episode, ep_reward, tot_time_steps, tot_updates_amount))\n",
        "                    break\n",
        "    \n",
        "    def evaluate(self, params, time_steps = None, episode = None):\n",
        "        self.update_params(params)\n",
        "        print('Evaluation. Timesteps: {} Episodes: {}'.format(time_steps, episode))\n",
        "        reach_the_end = 0\n",
        "        rewards = np.zeros(self.evaluate_average_on_episodes)\n",
        "        for ep in range(self.evaluate_average_on_episodes):\n",
        "            ep_reward = 0\n",
        "            state = self.env.reset()\n",
        "            for t in range(self.max_timesteps):\n",
        "                action = self.policy.select_action(state, debug=False, danger=True)\n",
        "                state, reward, done, _ = self.env.step(action)\n",
        "                ep_reward += reward\n",
        "                if done:\n",
        "                    s = \"\\t\\tDead {}\\tUpdate {}\".format( reward == self.terminal_reward,  self.policy.action_update)\n",
        "                    \n",
        "                    if reward != self.terminal_reward and t !=self.max_timesteps-1:\n",
        "                        print(s+'\\tFinished')\n",
        "                        reach_the_end += 1\n",
        "                    else:\n",
        "                        print(s)\n",
        "                    break\n",
        "\n",
        "            rewards[ep] = ep_reward \n",
        "            \n",
        "        print('\\t\\tMean reward: {}. All rewards: {}'.format(int(np.mean(rewards)), list(map(int,rewards)) ))\n",
        "        return reach_the_end == self.evaluate_average_on_episodes\n",
        "    \n",
        "    \n",
        "    def is_passed(self, step_num, reward):\n",
        "        if reward != self.terminal_reward:\n",
        "            if step_num < self.terminal_reward:\n",
        "                return True\n",
        "        return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geRE4MK65Wnb",
        "colab_type": "text"
      },
      "source": [
        "## Test\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UPdKCCrdj6G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "\n",
        "def test(random_seed = None, env_name = \"BipedalWalker-v2\", solved = False):\n",
        "    \n",
        "    if random_seed is None:\n",
        "        random_seed = 1\n",
        "    n_episodes = 5\n",
        "    lr = 0.002\n",
        "    max_timesteps = 2000\n",
        "    render = False\n",
        "    save_gif = False\n",
        "    \n",
        "    filename = \"TD3_{}_{}\".format(env_name, random_seed)\n",
        "    if solved:\n",
        "        filename += '_solved'\n",
        "    directory = \"/content/drive/My Drive\" #\"./preTrained/{}\".format(env_name)\n",
        "    \n",
        "    env = gym.make(env_name)\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "    max_action = float(env.action_space.high[0])\n",
        "    \n",
        "    policy = TD3(lr, state_dim, action_dim, max_action, 2, directory, filename,epochs_for_danger=1)\n",
        "    \n",
        "    print(directory, filename)\n",
        "    policy.load_actor(directory, filename)\n",
        "    \n",
        "    for ep in range(1, n_episodes+1):\n",
        "        ep_reward = 0\n",
        "        state = env.reset()\n",
        "        for t in range(max_timesteps):\n",
        "            action = policy.select_action(state)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            ep_reward += reward\n",
        "            if render:\n",
        "                env.render()\n",
        "                if save_gif:\n",
        "                    img = env.render(mode = 'rgb_array')\n",
        "                    img = Image.fromarray(img)\n",
        "                    img.save('./gif/{}.jpg'.format(t))\n",
        "            if done:\n",
        "                break\n",
        "            \n",
        "        print('Episode: {}\\tReward: {}'.format(ep, int(ep_reward)))\n",
        "        ep_reward = 0\n",
        "        env.close()        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfhPK_HOoA5T",
        "colab_type": "text"
      },
      "source": [
        "## Eval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFl1m7oqoCpY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(policy, env, cur_ep_number, n_episodes =10, debug=False, terminal_reward=-100):\n",
        "    max_timesteps = 2000\n",
        "    rewards = np.zeros(n_episodes)\n",
        "    for ep in range( n_episodes):\n",
        "        ep_reward = 0\n",
        "        state = env.reset()\n",
        "        for t in range(max_timesteps):\n",
        "            action = policy.select_action(state, debug=True)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            ep_reward += reward\n",
        "            if done:\n",
        "                print(\"\\t\\tDead {}\\tUpdate {}\".format(reward==terminal_reward,  policy.action_update))\n",
        "                if reward != terminal_reward:\n",
        "                    pass\n",
        "                break\n",
        "        rewards[ep] = ep_reward \n",
        "            \n",
        "    print('\\t\\tMean reward: {}. All rewards: {}'.format(int(np.mean(rewards)),list(map(int,rewards)) ))\n",
        "    return   \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-HbMCjmKp-x",
        "colab_type": "text"
      },
      "source": [
        "##Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEVVIPdd3Dpp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(load = False, solved=False, env_name_old = \"BipedalWalker-v2\", \n",
        "          env_name_new = \"BipedalWalkerHardcore-v2\", terminal_reward=-100, \n",
        "          max_episodes=2511, random_seed_=1000000009,danger_threshold_=0.7):\n",
        "    ######### Hyperparameters #########\n",
        "    log_interval = 10           # print avg reward after interval\n",
        "    random_seed = random_seed_\n",
        "    gamma = 0.99                # discount for future rewards\n",
        "    batch_size = 100            # num of transitions sampled from replay buffer\n",
        "    batch_size_danger = batch_size // 2\n",
        "    lr = 0.001\n",
        "    exploration_noise = 0.25\n",
        "    polyak = 0.995              # target policy update parameter (1-tau)\n",
        "    policy_noise = 0.2          # target policy smoothing noise\n",
        "    noise_clip = 0.5\n",
        "    policy_delay = 2            # delayed policy updates parameter\n",
        "    max_episodes = max_episodes # max num of episodes\n",
        "    max_timesteps = 2000        # max timesteps in one episode\n",
        "    danger_threshold = danger_threshold_      # probability to perform safe action\n",
        "    directory = \"/content/drive/My Drive\"# \"./preTrained/{}\".format(env_name) # save trained models\n",
        "    filename_load = \"TD3_{}_{}\".format(env_name_old, random_seed)\n",
        "    filename_save = \"TD3_{}_{}\".format(env_name_new, random_seed)\n",
        "    epochs_for_danger = 3\n",
        "    max_iter_danger = 5\n",
        "    episodes_to_evaluate = 10\n",
        "    ###################################\n",
        "    \n",
        "    \n",
        "    if env_name_new == \"BipedalWalker-v2\":\n",
        "        env = gym.make(\"BipedalWalker-v2\")\n",
        "        max_env_t = 1600\n",
        "    elif env_name_new == \"BipedalWalkerHardcore-v2\":\n",
        "        env =CustomizableBipedalWalker()\n",
        "        env.set_env_params(stump_height=1)\n",
        "        env.set_env_states(state_mask=np.array([1,1,0,0],dtype=bool), p=np.array([0.1,0.9,0.9,0.9]))\n",
        "        env = TimeLimit(env, max_episode_steps=2000)\n",
        "        max_env_t = 1600\n",
        "    else:\n",
        "        env = gym.make(env_name_new)\n",
        "        max_env_t = 1000\n",
        "    \n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "    max_action = float(env.action_space.high[0])\n",
        "    \n",
        "    # Buffers\n",
        "    replay_buffer = ReplayBuffer()\n",
        "    replay_buffer_dead = ReplayBuffer()\n",
        "    trajectory_buffer = TrajectoriesEndBuffer()\n",
        "    \n",
        "    # Policy\n",
        "    policy = TD3(lr, state_dim, action_dim, max_action, danger_threshold, directory, filename_save, epochs_for_danger)\n",
        "    \n",
        "    \n",
        "    # load\n",
        "    if load:\n",
        "        full_fname = filename_load\n",
        "        if solved:\n",
        "            full_fname += \"_solved\"\n",
        "        policy.load(directory,  full_fname)\n",
        "    else:\n",
        "        policy.save(directory, filename_save)\n",
        "             \n",
        "    \n",
        "    # Random seed\n",
        "    if random_seed:\n",
        "        print(\"Random Seed: {}\".format(random_seed))\n",
        "        env.seed(random_seed)\n",
        "        torch.manual_seed(random_seed)\n",
        "        np.random.seed(random_seed)\n",
        "    \n",
        "    # logging variables:\n",
        "    avg_reward = 0\n",
        "    ep_reward = 0\n",
        "    tot_time_steps = 0\n",
        "    average_time_steps = 0\n",
        "    \n",
        "    log_f = open(\"log.txt\",\"w+\")\n",
        "    \n",
        "    # training procedure:\n",
        "    print('Start training')\n",
        "    reach_the_end = False\n",
        "    \n",
        "    for episode in range(1, max_episodes+1):\n",
        "            \n",
        "        state = env.reset()\n",
        "        for t in range(max_timesteps+1):\n",
        "            # select action and add exploration noise:\n",
        "            action = policy.select_action(state)\n",
        "            action = action + np.random.normal(0, exploration_noise, size=env.action_space.shape[0])\n",
        "            action = action.clip(env.action_space.low, env.action_space.high)\n",
        "            \n",
        "            # take action in env:\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            \n",
        "            \n",
        "            replay_buffer.add((state, action, reward, next_state, float(done)))\n",
        "            trajectory_buffer.add((state, action, reward, next_state, float(done)))\n",
        "            \n",
        "            if reward == terminal_reward:\n",
        "                tmp_st = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
        "                tmp_a = torch.FloatTensor(action.reshape(1, -1)).to(device)\n",
        "                print(\"\\tDead after pair with prob: \", policy.critic_danger(tmp_st, tmp_a).cpu().data.numpy().flatten())\n",
        "                #replay_buffer_dead.add((state, action, reward, next_state, float(done)))\n",
        "                trajectory_buffer.transfer_to_replay_buffer(replay_buffer_dead)\n",
        "            \n",
        "            state = next_state\n",
        "            \n",
        "            avg_reward += reward\n",
        "            ep_reward += reward\n",
        "            \n",
        "            if done and reward != terminal_reward and reach_the_end==False and t < max_env_t-1:\n",
        "                print('Finished. r={} t={}'.format(reward, t))\n",
        "                reach_the_end = True\n",
        "              \n",
        "                first_finish_ep_num = episode\n",
        "            # if episode is done then update policy:\n",
        "            if done:\n",
        "                print('last t num:{}'.format(t))\n",
        "            if t==(max_timesteps-1): \n",
        "                print('done: {}'.format(done))\n",
        "                \n",
        "            if done or t==(max_timesteps-1): \n",
        "                tot_time_steps += t\n",
        "                average_time_steps += t\n",
        "                if t==(max_timesteps):                    \n",
        "                    print(\"\\tStuck!\")\n",
        "                policy.update(replay_buffer,replay_buffer_dead, t, batch_size, batch_size_danger, gamma, polyak, policy_noise, noise_clip, policy_delay)\n",
        "                break\n",
        "        \n",
        "        # logging updates:\n",
        "        log_f.write('{},{}\\n'.format(episode, ep_reward))\n",
        "        log_f.flush()\n",
        "        ep_reward = 0\n",
        "        \n",
        "        # if avg reward > 300 then save and stop traning:\n",
        "        if (avg_reward/log_interval) >= 300:\n",
        "            print(\"########## Solved! ###########\")\n",
        "            name = filename_save + '_solved'\n",
        "            policy.save(directory, name, optimizers=True, danger=True)\n",
        "            log_f.close()\n",
        "            break\n",
        "        \n",
        "        if episode > 1 and episode % (5 * log_interval) == 0:\n",
        "            policy.save(directory, filename_save, optimizers=True, danger=True)\n",
        "            \n",
        "        \n",
        "        # print avg reward every log interval:\n",
        "        if episode % log_interval == 0:\n",
        "            avg_reward = int(avg_reward / log_interval)\n",
        "            average_time_steps = int(average_time_steps / log_interval)\n",
        "            print(\"Episode: {}\\tAverage Reward: {}\\tAverage steps: {}\\t Total: {}\".format(episode, avg_reward, average_time_steps, tot_time_steps))\n",
        "            avg_reward = 0\n",
        "            average_time_steps = 0\n",
        "            \n",
        "        if episode % episodes_to_evaluate == 0 and episode > 0:\n",
        "            evaluate(policy, env, debug=False)\n",
        "    #print('first finished: ',episode)\n",
        "    return policy, replay_buffer, replay_buffer_dead"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqRwJRyr0PZh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train(load=True, solved=True, env_name_old = \"BipedalWalker-v2\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLPaJkML2tox",
        "colab_type": "code",
        "outputId": "d9879fcc-08e1-4a0a-fa9d-63db60d926a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "test(1, \"BipedalWalker-v2\", solved=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive TD3_BipedalWalker-v2_1_solved\n",
            "Episode: 1\tReward: 302\n",
            "Episode: 2\tReward: 300\n",
            "Episode: 3\tReward: 303\n",
            "Episode: 4\tReward: 301\n",
            "Episode: 5\tReward: 302\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBzWGWj0235B",
        "colab_type": "code",
        "outputId": "b5257441-af0e-4dba-f24c-1ffe37e73904",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "test(1, \"BipedalWalkerHardcore-v2\", solved=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive TD3_BipedalWalkerHardcore-v2_1\n",
            "Episode: 1\tReward: -73\n",
            "Episode: 2\tReward: -111\n",
            "Episode: 3\tReward: -70\n",
            "Episode: 4\tReward: -140\n",
            "Episode: 5\tReward: -69\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC-3p-bFIcMg",
        "colab_type": "text"
      },
      "source": [
        "# Debug\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx4nESZqVJ9q",
        "colab_type": "text"
      },
      "source": [
        "## Test Research class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsFU93i6VOFo",
        "colab_type": "code",
        "outputId": "cd4864eb-9170-4258-feef-e8ee4675a06f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 906
        }
      },
      "source": [
        "params = {'env_name':\"BipedalWalker-v2\",'danger_enable':True}\n",
        "new_research = Research(params)\n",
        "\n",
        "new_research.init_env()\n",
        "new_research.init_buffers_and_agent(load=False, solved=False)\n",
        "# prime = { 14245223, 21192173, 33535573, 285781, 332447659, 19152527}\n",
        "#  'max_timesteps':10,\n",
        "new_research.train({'random_seed':19152527, \n",
        "                    'max_trajectory_size' : 10,\n",
        "                    'evaluate_after_episodes' : 5,\n",
        "                    'evaluate_average_on_episodes' : 5,\n",
        "                    'max_episodes' : 1000,}, \n",
        "                   debug=True)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "danger_enable set to True\n",
            "env_name set to BipedalWalker-v2\n",
            "random_seed set to 19152527\n",
            "max_episodes set to 1000\n",
            "max_trajectory_size set to 10\n",
            "evaluate_after_episodes set to 5\n",
            "evaluate_average_on_episodes set to 5\n",
            "Random Seed: 19152527\n",
            "Start training\n",
            "\tDead after pair with prob:  [0.49679735]\n",
            "\tDead after pair with prob:  [0.48567435]\n",
            "\tDead after pair with prob:  [0.4927879]\n",
            "\tDead after pair with prob:  [0.4864081]\n",
            "\tDead after pair with prob:  [0.4825896]\n",
            "\tDead after pair with prob:  [0.47822413]\n",
            "\tStuck, ep reward -133.50278979325105\n",
            "\tDead after pair with prob:  [0.46869397]\n",
            "\tDead after pair with prob:  [0.4680006]\n",
            "\tDead after pair with prob:  [0.47929052]\n",
            "Episode: 10\tAverage Reward: -106\tAverage steps: 216\t Total: 2163\n",
            "\tDead after pair with prob:  [0.4888375]\n",
            "\tDead after pair with prob:  [0.5150979]\n",
            "\tDead after pair with prob:  [0.55649835]\n",
            "\tDead after pair with prob:  [0.58449537]\n",
            "\tDead after pair with prob:  [0.61619157]\n",
            "\tDead after pair with prob:  [0.6609184]\n",
            "\tDead after pair with prob:  [0.5704515]\n",
            "\tDead after pair with prob:  [0.58003736]\n",
            "\tDead after pair with prob:  [0.6031625]\n",
            "\tDead after pair with prob:  [0.7099028]\n",
            "Episode: 20\tAverage Reward: -106\tAverage steps: 53\t Total: 2699\n",
            "\tDead after pair with prob:  [0.09579618]\n",
            "\tStuck, ep reward -143.84198573676707\n",
            "\tDead after pair with prob:  [0.36714113]\n",
            "\tDead after pair with prob:  [0.75960344]\n",
            "\tDead after pair with prob:  [0.553433]\n",
            "\tDead after pair with prob:  [0.6042889]\n",
            "\tDead after pair with prob:  [0.57584655]\n",
            "\tDead after pair with prob:  [0.6296405]\n",
            "\tDead after pair with prob:  [0.5159567]\n",
            "\tDead after pair with prob:  [0.55223405]\n",
            "Episode: 30\tAverage Reward: -118\tAverage steps: 226\t Total: 4959\n",
            "\tDead after pair with prob:  [0.7639457]\n",
            "\tDead after pair with prob:  [0.8379607]\n",
            "\tStuck, ep reward -139.36046367475674\n",
            "\tStuck, ep reward -99.14477704682528\n",
            "\tDead after pair with prob:  [0.93472266]\n",
            "\tDead after pair with prob:  [0.9552514]\n",
            "\tDead after pair with prob:  [0.94457996]\n",
            "\tDead after pair with prob:  [0.9635354]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UH9aTMAwIeTf",
        "colab_type": "text"
      },
      "source": [
        "## Test targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2LSHCIJd33H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "log_interval = 10           # print avg reward after interval\n",
        "random_seed = 1\n",
        "gamma = 0.99                # discount for future rewards\n",
        "batch_size = 100            # num of transitions sampled from replay buffer\n",
        "lr = 0.001\n",
        "exploration_noise = 0.15 \n",
        "polyak = 0.995              # target policy update parameter (1-tau)\n",
        "policy_noise = 0.2          # target policy smoothing noise\n",
        "noise_clip = 0.5\n",
        "policy_delay = 2            # delayed policy updates parameter\n",
        "max_episodes = 1500         # max num of episodes\n",
        "max_timesteps = 2000        # max timesteps in one episode\n",
        "directory = \"/content/drive/My Drive\"# \"./preTrained/{}\".format(env_name) # save trained models\n",
        "\n",
        "    \n",
        "env = CustomizableBipedalWalker()\n",
        "env.set_env_params(stump_height=2)\n",
        "    \n",
        "env.set_env_states(state_mask=np.array([1,1,0,0],dtype=bool), p=np.array([0.1,0.9,0.9,0.9]))\n",
        "    \n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])\n",
        "    \n",
        "policy = TD3(lr, state_dim, action_dim, max_action, danger_threshold = .6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pYSy_i8HsdB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = CustomizableBipedalWalker()\n",
        "env.set_env_params(stump_height=2)    \n",
        "env.set_env_states(state_mask=np.array([1,1,0,0],dtype=bool), p=np.array([0.1,0.9,0.9,0.9]))\n",
        "\n",
        "replay_buffer = ReplayBuffer()\n",
        "    \n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])\n",
        "\n",
        "\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    \n",
        "    action = policy.select_action(state)\n",
        "    action = action + np.random.normal(0, exploration_noise, size=env.action_space.shape[0])\n",
        "    action = action.clip(env.action_space.low, env.action_space.high)\n",
        "            \n",
        "            # take action in env:\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    replay_buffer.add((state, action, reward, next_state, float(done)))\n",
        "    state = next_state\n",
        "    print(reward, reward> -100)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4SulcblI_KC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a,b,c,d,e = replay_buffer.sample(10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gmf5wVAzLmZ_",
        "colab_type": "code",
        "outputId": "b9b5aa63-9cb8-4ce5-f246-4550f2f93f16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnZxLGmrlAyw",
        "colab_type": "text"
      },
      "source": [
        "## Model adjustments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3v1jERUXqF-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Critic2(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, name, probability=False):\n",
        "        super(Critic2, self).__init__()\n",
        "        \n",
        "        self.l1 = nn.Linear(state_dim + action_dim, 400)\n",
        "        self.b1 = nn.BatchNorm1d(400)\n",
        "        self.l2 = nn.Linear(400, 300)\n",
        "        self.l3 = nn.Linear(300, 1)\n",
        "        \n",
        "        self.probability = probability\n",
        "        \n",
        "        self.name = name\n",
        "        self.fname = name + '.pth'\n",
        "        \n",
        "    def forward(self, state, action):\n",
        "        state_action = torch.cat([state, action], 1)\n",
        "        \n",
        "        q = F.relu(self.b1(self.l1(state_action)))\n",
        "        \n",
        "        q = F.relu(self.l2(q))\n",
        "        q = self.l3(q)\n",
        "        if self.probability:\n",
        "            q = torch.sigmoid(q)\n",
        "        return q\n",
        "    \n",
        "cr = Critic2(state_dim=10, action_dim=5, name='k')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "131_b7n03jxn",
        "colab_type": "code",
        "outputId": "680a16e3-03fa-43a1-cc4e-72eec485b7e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "cr.state_dict()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('l1.weight',\n",
              "              tensor([[ 0.1534,  0.1447,  0.2514,  ...,  0.0659,  0.1171,  0.0379],\n",
              "                      [ 0.0543, -0.0819, -0.2025,  ...,  0.2515, -0.0691, -0.0846],\n",
              "                      [-0.0207, -0.2169, -0.0013,  ...,  0.0523, -0.0328,  0.2197],\n",
              "                      ...,\n",
              "                      [ 0.0685, -0.2310,  0.2321,  ...,  0.0113, -0.1875,  0.0675],\n",
              "                      [ 0.2493,  0.1893, -0.2360,  ...,  0.1646, -0.1145,  0.0965],\n",
              "                      [-0.1450,  0.1114, -0.0581,  ...,  0.2268, -0.0391,  0.2366]])),\n",
              "             ('l1.bias',\n",
              "              tensor([-0.2338,  0.0237,  0.0839, -0.1583, -0.0978,  0.1524,  0.1341, -0.0704,\n",
              "                       0.1951, -0.0658, -0.2274, -0.2453,  0.0346,  0.1386, -0.0211, -0.2211,\n",
              "                      -0.0214,  0.1355,  0.1305,  0.2324,  0.1730,  0.2405,  0.2109, -0.0625,\n",
              "                      -0.2516, -0.1561,  0.0171, -0.1175,  0.0174,  0.2412, -0.1111,  0.0387,\n",
              "                       0.2481,  0.1600, -0.0092, -0.0772,  0.2060,  0.0938,  0.1771,  0.0581,\n",
              "                       0.1694,  0.0633,  0.0563,  0.0288,  0.1543, -0.2491, -0.1058,  0.0023,\n",
              "                       0.2267,  0.0984,  0.1676, -0.2225,  0.0504,  0.0980, -0.0445,  0.1877,\n",
              "                      -0.2346, -0.1986,  0.0413, -0.0116,  0.1564,  0.1831,  0.2044, -0.0181,\n",
              "                       0.1326, -0.1045,  0.1450,  0.2059,  0.1900, -0.0395,  0.1660,  0.1010,\n",
              "                       0.1603, -0.1375, -0.0957,  0.0919, -0.0521, -0.0545, -0.2211,  0.2156,\n",
              "                       0.2216,  0.1434,  0.2479, -0.1793,  0.0331,  0.0610, -0.2048,  0.0563,\n",
              "                      -0.1936, -0.1886, -0.1165, -0.0956, -0.1146, -0.1637,  0.0650,  0.1979,\n",
              "                      -0.1270,  0.1641, -0.0888,  0.1014,  0.0834, -0.0538,  0.2511,  0.1444,\n",
              "                       0.0024,  0.0371,  0.2053,  0.1282, -0.0964,  0.0584, -0.0054, -0.1749,\n",
              "                      -0.2343, -0.2395,  0.0271, -0.0954,  0.1923, -0.1948, -0.2425, -0.0825,\n",
              "                      -0.0914, -0.1183, -0.2315,  0.1067, -0.0410,  0.1421, -0.2128, -0.1870,\n",
              "                       0.0837,  0.1330,  0.2459, -0.2557, -0.0141,  0.1095,  0.2322,  0.1287,\n",
              "                       0.0555, -0.1621,  0.1026, -0.0018,  0.1413,  0.1035,  0.0851,  0.0895,\n",
              "                       0.0077,  0.2427,  0.1157,  0.1137,  0.0767, -0.2424,  0.0760, -0.1180,\n",
              "                       0.1306, -0.1289,  0.2311,  0.2062,  0.2130, -0.1239, -0.2014, -0.1256,\n",
              "                      -0.0094, -0.0848, -0.0898,  0.2125, -0.1772, -0.0776, -0.1330,  0.1035,\n",
              "                       0.1390, -0.0717,  0.0696,  0.0556,  0.1280, -0.0448, -0.2342,  0.1281,\n",
              "                      -0.0050, -0.1255, -0.1786,  0.0846,  0.2477,  0.1197, -0.1570, -0.0569,\n",
              "                      -0.2533,  0.1863,  0.1995,  0.0145, -0.2484, -0.0231,  0.2353, -0.1632,\n",
              "                       0.1768, -0.1140, -0.2295,  0.0763, -0.2371, -0.1945,  0.0706,  0.2175,\n",
              "                      -0.1352,  0.1340,  0.0237,  0.0141,  0.2496,  0.0571,  0.1681,  0.1133,\n",
              "                      -0.0987,  0.1491, -0.0949, -0.1864,  0.2224,  0.1175,  0.2371, -0.0089,\n",
              "                      -0.1926,  0.1416,  0.1506, -0.0244,  0.0479, -0.1025,  0.1287,  0.2344,\n",
              "                       0.2465, -0.2113,  0.1384,  0.1045, -0.1799,  0.0581, -0.1235, -0.0328,\n",
              "                       0.0097,  0.0925,  0.2095,  0.0791, -0.2379, -0.1459, -0.2538,  0.2511,\n",
              "                       0.0248, -0.0883, -0.0629,  0.1348,  0.2547,  0.0191,  0.0907, -0.0892,\n",
              "                       0.2062,  0.1453, -0.1588, -0.2397,  0.0552,  0.1582, -0.0625, -0.0685,\n",
              "                      -0.0992,  0.1000, -0.0714,  0.0596, -0.1449, -0.0469, -0.0065,  0.0237,\n",
              "                       0.0176,  0.0140,  0.2361, -0.2137, -0.2371, -0.1665,  0.0397,  0.0538,\n",
              "                      -0.1650, -0.1596,  0.2204,  0.2021,  0.2453, -0.1254, -0.0318, -0.1344,\n",
              "                       0.0786, -0.1069, -0.1018, -0.0321,  0.1130,  0.1329,  0.0249,  0.0797,\n",
              "                      -0.0428,  0.1753,  0.0179, -0.1056, -0.0325, -0.0383, -0.0867,  0.0607,\n",
              "                       0.1695,  0.1502, -0.0337, -0.0563, -0.2501,  0.1808,  0.2449, -0.1717,\n",
              "                      -0.2379,  0.1770, -0.0659,  0.1080,  0.0856, -0.2181, -0.0031,  0.0334,\n",
              "                       0.1552, -0.0696,  0.0874, -0.1527, -0.2321, -0.1458, -0.1841,  0.2550,\n",
              "                      -0.0139, -0.2156, -0.1057, -0.1072,  0.0905,  0.0040, -0.1931,  0.0452,\n",
              "                      -0.2148, -0.0469,  0.1960,  0.0802, -0.1209, -0.1841,  0.0540,  0.1879,\n",
              "                       0.2244, -0.2296,  0.1272, -0.1355, -0.1351,  0.1164, -0.1570, -0.1261,\n",
              "                      -0.0600, -0.0407, -0.2335,  0.2109, -0.0907,  0.2511,  0.2507, -0.1519,\n",
              "                      -0.1166,  0.2090, -0.1435,  0.0551, -0.1080,  0.0319, -0.0274,  0.0215,\n",
              "                      -0.0321,  0.1811,  0.1975,  0.1525, -0.0137, -0.1706,  0.0662, -0.1557,\n",
              "                       0.1001, -0.0669,  0.1969, -0.0260, -0.1433, -0.0820, -0.1280, -0.1745,\n",
              "                      -0.2356, -0.2534,  0.1742, -0.0316,  0.1841, -0.1074, -0.0346, -0.1061,\n",
              "                      -0.0957, -0.0111,  0.0193,  0.0499, -0.1285,  0.1530,  0.2072, -0.1744,\n",
              "                       0.1136,  0.0569, -0.2537, -0.2104, -0.0857,  0.2213,  0.1024,  0.0902])),\n",
              "             ('b1.weight',\n",
              "              tensor([0.9538, 0.8469, 0.3684, 0.0516, 0.2853, 0.5387, 0.8240, 0.1130, 0.0931,\n",
              "                      0.7766, 0.7227, 0.3183, 0.2632, 0.5598, 0.2317, 0.7675, 0.3617, 0.2734,\n",
              "                      0.8948, 0.7464, 0.2372, 0.8165, 0.9386, 0.8297, 0.9291, 0.6298, 0.6084,\n",
              "                      0.6166, 0.7634, 0.7635, 0.4470, 0.3940, 0.8681, 0.2688, 0.1170, 0.3851,\n",
              "                      0.6711, 0.1076, 0.1851, 0.5735, 0.3113, 0.9832, 0.4009, 0.7250, 0.7498,\n",
              "                      0.7089, 0.2685, 0.4593, 0.8173, 0.0483, 0.3375, 0.3593, 0.9748, 0.7867,\n",
              "                      0.7258, 0.0232, 0.8737, 0.1113, 0.5691, 0.9058, 0.7432, 0.0619, 0.6603,\n",
              "                      0.9287, 0.8237, 0.5134, 0.0804, 0.8820, 0.3614, 0.4045, 0.0564, 0.7739,\n",
              "                      0.2975, 0.5507, 0.4026, 0.6011, 0.4218, 0.8417, 0.5234, 0.4045, 0.7989,\n",
              "                      0.1946, 0.1280, 0.3241, 0.8482, 0.9484, 0.5341, 0.5442, 0.5543, 0.4207,\n",
              "                      0.5593, 0.4354, 0.1094, 0.3197, 0.7490, 0.0557, 0.0958, 0.4990, 0.2908,\n",
              "                      0.6419, 0.3359, 0.3614, 0.3830, 0.9920, 0.8833, 0.9785, 0.3256, 0.2951,\n",
              "                      0.4427, 0.7159, 0.7650, 0.2712, 0.2099, 0.8588, 0.4365, 0.3802, 0.8729,\n",
              "                      0.1938, 0.4062, 0.2227, 0.8940, 0.6747, 0.9729, 0.3063, 0.0205, 0.5632,\n",
              "                      0.4683, 0.5889, 0.5493, 0.6316, 0.7973, 0.0333, 0.9175, 0.7525, 0.5177,\n",
              "                      0.8441, 0.2992, 0.1088, 0.6304, 0.6492, 0.8191, 0.0961, 0.0347, 0.6558,\n",
              "                      0.2075, 0.3179, 0.9810, 0.7801, 0.4008, 0.2515, 0.9550, 0.3359, 0.9439,\n",
              "                      0.6865, 0.9645, 0.2591, 0.0699, 0.9845, 0.4614, 0.7200, 0.9612, 0.7330,\n",
              "                      0.3962, 0.2059, 0.6280, 0.9201, 0.2564, 0.9909, 0.7730, 0.9287, 0.8761,\n",
              "                      0.0761, 0.5636, 0.9985, 0.6386, 0.8462, 0.4386, 0.7788, 0.1927, 0.4111,\n",
              "                      0.8777, 0.6856, 0.4242, 0.4298, 0.7458, 0.8293, 0.6282, 0.0937, 0.3354,\n",
              "                      0.7144, 0.8193, 0.4826, 0.6962, 0.0312, 0.3170, 0.6691, 0.3978, 0.7465,\n",
              "                      0.9796, 0.9651, 0.7245, 0.1771, 0.3708, 0.6129, 0.4219, 0.1707, 0.9250,\n",
              "                      0.6863, 0.5243, 0.0349, 0.5353, 0.2150, 0.3098, 0.0917, 0.5793, 0.4061,\n",
              "                      0.5789, 0.0618, 0.6661, 0.0588, 0.0102, 0.3679, 0.8075, 0.6921, 0.0920,\n",
              "                      0.9303, 0.6484, 0.0361, 0.4556, 0.0099, 0.7341, 0.6827, 0.0068, 0.1992,\n",
              "                      0.0306, 0.3027, 0.9274, 0.4683, 0.9748, 0.6064, 0.2204, 0.8227, 0.4090,\n",
              "                      0.9722, 0.1651, 0.9824, 0.1469, 0.1568, 0.2772, 0.1727, 0.2584, 0.1480,\n",
              "                      0.6253, 0.9480, 0.6628, 0.6772, 0.3393, 0.0713, 0.6439, 0.0409, 0.6389,\n",
              "                      0.1423, 0.8209, 0.7851, 0.7257, 0.5028, 0.7752, 0.2682, 0.1252, 0.0582,\n",
              "                      0.4246, 0.2433, 0.6281, 0.6332, 0.0402, 0.0429, 0.9196, 0.1057, 0.3500,\n",
              "                      0.4296, 0.8851, 0.2093, 0.6051, 0.3897, 0.4797, 0.9860, 0.1029, 0.2046,\n",
              "                      0.6425, 0.6267, 0.9046, 0.8786, 0.9889, 0.5697, 0.0147, 0.9105, 0.8099,\n",
              "                      0.4436, 0.2954, 0.4699, 0.0284, 0.2308, 0.1052, 0.1102, 0.4040, 0.1166,\n",
              "                      0.1245, 0.1946, 0.1651, 0.1218, 0.8334, 0.0806, 0.1218, 0.5480, 0.3955,\n",
              "                      0.0314, 0.4546, 0.2322, 0.6881, 0.9646, 0.8138, 0.9263, 0.9573, 0.0558,\n",
              "                      0.2380, 0.5128, 0.4089, 0.3477, 0.9550, 0.1999, 0.7534, 0.6414, 0.9506,\n",
              "                      0.4313, 0.5220, 0.6274, 0.6056, 0.8299, 0.2179, 0.9560, 0.8316, 0.6503,\n",
              "                      0.0769, 0.8002, 0.6833, 0.5709, 0.5371, 0.4455, 0.5370, 0.4713, 0.5511,\n",
              "                      0.0731, 0.0965, 0.6626, 0.6013, 0.4926, 0.5172, 0.9485, 0.1468, 0.3385,\n",
              "                      0.2102, 0.0293, 0.6496, 0.1044, 0.0152, 0.5605, 0.9114, 0.9220, 0.1038,\n",
              "                      0.7515, 0.9930, 0.7838, 0.2658, 0.8746, 0.5818, 0.7491, 0.9825, 0.8927,\n",
              "                      0.9136, 0.9028, 0.0373, 0.1167, 0.0222, 0.6258, 0.3775, 0.0887, 0.6433,\n",
              "                      0.4150, 0.5342, 0.4937, 0.6847, 0.9014, 0.5530, 0.5671, 0.0292, 0.5740,\n",
              "                      0.0603, 0.8678, 0.5340, 0.4392])),\n",
              "             ('b1.bias',\n",
              "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
              "             ('b1.running_mean',\n",
              "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
              "             ('b1.running_var',\n",
              "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "                      1., 1., 1., 1.])),\n",
              "             ('b1.num_batches_tracked', tensor(0)),\n",
              "             ('l2.weight',\n",
              "              tensor([[-0.0114,  0.0431, -0.0091,  ..., -0.0470,  0.0431,  0.0149],\n",
              "                      [ 0.0492, -0.0311, -0.0271,  ...,  0.0307,  0.0354, -0.0047],\n",
              "                      [ 0.0173,  0.0055,  0.0009,  ...,  0.0243,  0.0243, -0.0074],\n",
              "                      ...,\n",
              "                      [ 0.0424,  0.0117, -0.0271,  ..., -0.0331, -0.0282,  0.0135],\n",
              "                      [-0.0268,  0.0386,  0.0181,  ..., -0.0283, -0.0363, -0.0312],\n",
              "                      [ 0.0329, -0.0264, -0.0010,  ..., -0.0232,  0.0078, -0.0261]])),\n",
              "             ('l2.bias',\n",
              "              tensor([ 0.0344, -0.0494, -0.0199, -0.0133, -0.0250,  0.0088, -0.0116,  0.0007,\n",
              "                       0.0461, -0.0145,  0.0313, -0.0053,  0.0065, -0.0222, -0.0163, -0.0296,\n",
              "                      -0.0345, -0.0125, -0.0378, -0.0127, -0.0379, -0.0187,  0.0416,  0.0235,\n",
              "                      -0.0441, -0.0355,  0.0153, -0.0033, -0.0430, -0.0352, -0.0436, -0.0360,\n",
              "                       0.0498,  0.0188, -0.0269, -0.0091,  0.0439,  0.0097,  0.0149, -0.0383,\n",
              "                      -0.0454, -0.0249,  0.0241, -0.0464,  0.0394, -0.0417, -0.0363,  0.0452,\n",
              "                      -0.0064,  0.0071, -0.0419, -0.0476,  0.0239,  0.0235, -0.0281,  0.0048,\n",
              "                      -0.0018,  0.0372,  0.0432, -0.0219,  0.0154,  0.0369, -0.0468, -0.0306,\n",
              "                      -0.0215,  0.0461,  0.0357,  0.0012, -0.0238,  0.0097,  0.0209, -0.0499,\n",
              "                       0.0476,  0.0086,  0.0294, -0.0340, -0.0142, -0.0439, -0.0042, -0.0336,\n",
              "                      -0.0398,  0.0384, -0.0341,  0.0266, -0.0329, -0.0164,  0.0095, -0.0341,\n",
              "                       0.0193,  0.0309, -0.0460, -0.0011,  0.0389,  0.0321,  0.0010,  0.0370,\n",
              "                      -0.0166, -0.0336, -0.0047,  0.0438,  0.0128, -0.0494, -0.0256, -0.0198,\n",
              "                       0.0124,  0.0444, -0.0455,  0.0406,  0.0453, -0.0225, -0.0422,  0.0492,\n",
              "                       0.0184,  0.0127,  0.0076, -0.0045, -0.0178,  0.0211,  0.0447, -0.0117,\n",
              "                       0.0433, -0.0099, -0.0360, -0.0224,  0.0022, -0.0440, -0.0422, -0.0355,\n",
              "                      -0.0375,  0.0441,  0.0450,  0.0294,  0.0262, -0.0284,  0.0366, -0.0376,\n",
              "                       0.0417, -0.0256, -0.0407, -0.0446,  0.0338, -0.0280,  0.0320,  0.0259,\n",
              "                       0.0072,  0.0282,  0.0299,  0.0121,  0.0357,  0.0293, -0.0002, -0.0027,\n",
              "                       0.0482, -0.0155,  0.0184, -0.0327,  0.0218, -0.0351,  0.0404,  0.0470,\n",
              "                      -0.0168, -0.0269,  0.0034,  0.0133, -0.0160,  0.0491,  0.0155, -0.0439,\n",
              "                       0.0084,  0.0247, -0.0333,  0.0260, -0.0363, -0.0424,  0.0009, -0.0192,\n",
              "                       0.0363, -0.0068,  0.0376,  0.0457,  0.0399, -0.0452,  0.0305,  0.0072,\n",
              "                       0.0473,  0.0277,  0.0425, -0.0008, -0.0037, -0.0359,  0.0276, -0.0177,\n",
              "                       0.0172, -0.0292, -0.0147,  0.0004,  0.0009,  0.0445,  0.0432, -0.0029,\n",
              "                       0.0171, -0.0339,  0.0138, -0.0330, -0.0040, -0.0210,  0.0342, -0.0105,\n",
              "                      -0.0093, -0.0416,  0.0016,  0.0383,  0.0416, -0.0062,  0.0103,  0.0312,\n",
              "                       0.0070, -0.0098, -0.0410, -0.0467, -0.0497, -0.0307,  0.0072,  0.0448,\n",
              "                      -0.0053, -0.0329, -0.0478,  0.0204, -0.0065,  0.0323,  0.0373,  0.0226,\n",
              "                       0.0095,  0.0148,  0.0101, -0.0152,  0.0150, -0.0110, -0.0150,  0.0270,\n",
              "                      -0.0185,  0.0269, -0.0117, -0.0084, -0.0458,  0.0317,  0.0383, -0.0366,\n",
              "                       0.0424, -0.0111, -0.0215, -0.0131,  0.0119, -0.0179,  0.0282, -0.0442,\n",
              "                       0.0334, -0.0387, -0.0066, -0.0186, -0.0364, -0.0109,  0.0026,  0.0320,\n",
              "                       0.0490, -0.0247, -0.0315, -0.0092,  0.0296,  0.0077,  0.0222, -0.0029,\n",
              "                       0.0337,  0.0445, -0.0259, -0.0008, -0.0468,  0.0490,  0.0457,  0.0405,\n",
              "                      -0.0493, -0.0130, -0.0307, -0.0313, -0.0338,  0.0365, -0.0123, -0.0449,\n",
              "                       0.0032,  0.0350, -0.0151, -0.0285,  0.0311, -0.0396,  0.0110,  0.0008,\n",
              "                       0.0127, -0.0243, -0.0471,  0.0327])),\n",
              "             ('l3.weight',\n",
              "              tensor([[ 4.0003e-02,  2.8999e-02,  3.7307e-02, -1.7120e-02, -4.3580e-02,\n",
              "                       -5.7468e-03,  3.6493e-02,  4.1658e-02,  8.9037e-03,  3.4343e-02,\n",
              "                       -1.4765e-02,  5.5696e-02,  1.5661e-02,  4.6526e-02, -3.1963e-04,\n",
              "                        2.1118e-03,  2.6795e-03, -5.2999e-02,  6.0482e-03,  3.0449e-02,\n",
              "                       -2.4869e-02, -1.4783e-02, -3.4898e-02, -3.5551e-02,  3.0629e-02,\n",
              "                       -1.2992e-02,  3.1895e-03,  4.3717e-02, -3.2322e-02,  5.4715e-02,\n",
              "                       -4.8273e-03, -1.1154e-02, -1.8354e-02, -5.4498e-03,  4.8660e-02,\n",
              "                        3.6394e-03, -4.5131e-02, -3.3307e-02, -3.0729e-02, -2.8811e-02,\n",
              "                       -3.9097e-03,  3.8076e-02,  1.4747e-02,  3.8494e-02,  2.3080e-02,\n",
              "                       -4.4291e-02,  1.1672e-02, -4.5736e-02,  2.7750e-02,  1.9523e-02,\n",
              "                        7.0139e-03, -1.4717e-03,  1.5059e-04, -4.1421e-02, -1.2189e-02,\n",
              "                        2.9203e-02,  2.7009e-02, -3.6032e-02, -2.1366e-02, -4.1366e-03,\n",
              "                       -4.7032e-02,  8.3328e-03, -4.3820e-02, -2.6821e-02,  5.2131e-02,\n",
              "                        4.1222e-02, -2.8115e-02, -2.1079e-02,  8.6574e-03,  3.6493e-02,\n",
              "                       -3.6140e-02, -5.5259e-02,  3.5853e-02,  2.6765e-02,  7.4475e-03,\n",
              "                       -4.5732e-03,  2.6923e-03,  3.0416e-02,  4.6165e-02, -4.9407e-02,\n",
              "                       -2.0648e-02,  2.1045e-02, -2.9328e-02, -4.3588e-02,  1.3989e-02,\n",
              "                        1.5406e-02,  6.9110e-04, -7.3825e-03, -5.7461e-02, -2.5688e-02,\n",
              "                        5.1978e-02,  4.5663e-02,  1.4650e-02, -7.6501e-03,  4.0150e-02,\n",
              "                       -3.3499e-02, -7.5487e-04, -2.8449e-03, -3.8236e-02, -2.2162e-02,\n",
              "                        1.2287e-03,  5.0953e-02, -4.2527e-02,  2.7617e-02,  1.7675e-02,\n",
              "                       -3.0218e-02, -4.8621e-02, -4.0180e-02, -1.6337e-02,  1.9210e-02,\n",
              "                        5.4510e-02,  2.5459e-02, -2.3928e-02, -8.7290e-03,  3.0898e-02,\n",
              "                       -4.1889e-02, -2.1400e-02, -5.1275e-02, -1.8820e-02, -3.9430e-02,\n",
              "                        2.3358e-02, -3.5720e-02,  9.0405e-03, -5.7518e-02,  5.1298e-02,\n",
              "                       -4.6056e-02, -4.5022e-02,  4.2768e-02,  1.5864e-02,  1.9915e-02,\n",
              "                        1.8788e-02,  1.9270e-02, -2.2783e-02,  4.1961e-02,  5.0143e-02,\n",
              "                       -2.6405e-02,  1.8288e-02, -2.8470e-02,  1.8648e-02,  3.3110e-02,\n",
              "                       -5.5638e-02, -3.7580e-02, -8.0319e-03,  4.7015e-02, -1.4432e-02,\n",
              "                       -4.9314e-02, -3.7820e-02, -5.5114e-02,  2.8157e-02, -2.8183e-02,\n",
              "                        4.9977e-02,  3.1925e-02, -2.6941e-02,  5.2050e-02, -1.3091e-02,\n",
              "                       -4.4222e-02, -3.8454e-02,  4.0696e-02,  4.2301e-03, -5.8988e-03,\n",
              "                       -3.9147e-02,  3.7908e-02,  3.3944e-02,  3.7778e-02,  3.6242e-02,\n",
              "                       -4.8831e-02, -2.7950e-02, -3.7422e-02,  2.7896e-02,  2.3262e-02,\n",
              "                        4.7661e-02, -3.4648e-02, -1.8945e-02, -3.8817e-03,  3.9117e-02,\n",
              "                        1.1293e-02, -2.0755e-03, -4.7125e-02,  3.1860e-02, -4.0651e-03,\n",
              "                        2.1535e-02,  3.8230e-03, -1.5239e-02,  5.6763e-02, -9.4293e-03,\n",
              "                       -3.9609e-02,  3.8611e-02,  5.2564e-02, -2.9113e-05,  5.7594e-02,\n",
              "                       -4.5421e-02,  5.2959e-02,  5.1693e-02,  2.8700e-02,  3.7412e-02,\n",
              "                        5.6299e-02, -2.6359e-02, -2.1126e-02,  3.7423e-02, -1.6393e-02,\n",
              "                       -2.4395e-02, -2.1167e-02,  3.1092e-02,  5.7288e-02, -3.7654e-03,\n",
              "                       -4.1114e-02,  5.1653e-02, -3.6631e-02,  4.2640e-02, -5.5733e-02,\n",
              "                        4.8582e-02,  2.0465e-03, -2.9906e-02, -1.2590e-03, -1.9644e-02,\n",
              "                        5.3706e-03, -2.8931e-02,  4.6774e-02, -4.5480e-02, -4.5542e-02,\n",
              "                       -1.5156e-02, -2.4982e-02,  5.5501e-02, -2.2886e-02,  5.6912e-02,\n",
              "                       -3.7229e-02,  5.6627e-02,  1.8268e-03,  4.5648e-03,  1.5884e-02,\n",
              "                       -5.2869e-02, -9.1390e-04, -5.7711e-02, -4.1657e-02, -5.3576e-02,\n",
              "                       -3.8822e-02, -3.3921e-02, -5.3412e-03,  1.2653e-02, -5.1347e-02,\n",
              "                       -4.3406e-02, -5.6655e-02, -4.7290e-02, -3.9653e-03, -1.1462e-02,\n",
              "                       -6.5478e-03, -5.5676e-02,  4.7583e-02,  9.6492e-03,  3.1433e-02,\n",
              "                        5.1516e-02,  3.7380e-02,  1.2177e-02,  5.6635e-02,  4.0107e-02,\n",
              "                       -2.4882e-02, -3.5886e-02,  4.7631e-02, -3.0554e-02,  4.1912e-02,\n",
              "                       -3.7046e-02, -9.8500e-03,  2.8482e-02,  2.6179e-02,  1.9503e-02,\n",
              "                       -4.8761e-02,  1.2789e-02, -2.7760e-02, -4.2436e-02, -2.3775e-02,\n",
              "                       -5.6980e-02,  1.7828e-02, -5.4826e-02, -5.3418e-02,  2.7840e-02,\n",
              "                        4.9408e-03,  3.4826e-02, -5.1402e-02,  4.0505e-02, -1.1778e-02,\n",
              "                       -1.7693e-02, -4.0341e-02, -5.4232e-02,  4.7568e-02, -9.5092e-03,\n",
              "                       -6.3146e-03, -3.7830e-02, -4.8610e-02, -5.1909e-02, -2.3422e-02,\n",
              "                        3.7433e-02,  5.3604e-02,  2.2095e-02,  2.0053e-02, -2.0502e-02,\n",
              "                        3.4971e-02,  2.3078e-02, -3.2497e-02,  4.8092e-02,  8.4046e-04]])),\n",
              "             ('l3.bias', tensor([0.0530]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4SkaheA3w_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeVJRRcZlEfU",
        "colab_type": "text"
      },
      "source": [
        "## First Radam Test "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7C4OZFkJlQ5K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train(load=False, solved=False, env_name_load = \"Pendulum-v0\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWN2mdTMl1CI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AA:\n",
        "    def __init__(self, params):\n",
        "        names = ['ka', 'b']\n",
        "        for attr in names:\n",
        "            if attr in params:\n",
        "                setattr(self, attr, params[attr])\n",
        "    \n",
        "    def print(self, num):\n",
        "        if num == 3:\n",
        "            print(self.ka, self.b)\n",
        "        elif num == 2:\n",
        "            print(self.ka)\n",
        "        elif num == 1:\n",
        "            print(self.b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQL7wUU1DyiP",
        "colab_type": "code",
        "outputId": "bb2820c8-6bd7-4285-f8ec-b9411e831f63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "params = {'ka' : 1, 'b':2}\n",
        "a = AA(params)\n",
        "\n",
        "a.print(3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KN4hkm0cEG_3",
        "colab_type": "code",
        "outputId": "d8e167ba-6425-4a29-a7aa-dbec0811fa5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import gym\n",
        "env = gym.make('Pendulum-v0')\n",
        "try:\n",
        "    getattr(env, '_max_episode_steps9')\n",
        "except:\n",
        "    print('l')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "l\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6e3td5ujWSf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train(load= True, solved = False, env_name_old = 'BipedalWalkerHardcore-v2',env_name_new = 'BipedalWalkerHardcore-v2',random_seed_=1000000009, \n",
        "      max_episodes = 20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctSbuXcgn2-x",
        "colab_type": "code",
        "outputId": "1139750c-a63a-40b0-c28b-bee769b6884b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "TimeLimit"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "gym.wrappers.time_limit.TimeLimit"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5ZBiHAu3Urb",
        "colab_type": "code",
        "outputId": "3913dc27-aeca-45a7-b6e1-a41aa97c5e84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "env = gym.make(\"BipedalWalker-v2\")\n",
        "env.action_space.shape[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vF3-FH665SB5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = torch.Tensor((2,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hw4H-4cP7naJ",
        "colab_type": "code",
        "outputId": "b518f54d-2bd0-43f2-cf18-0fa61cdc1e5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        }
      },
      "source": [
        "t.dot(np.array([1,3]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-101-c34dcfeec2bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: dot(): argument 'tensor' (position 1) must be Tensor, not numpy.ndarray"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzTjHNUH7rlv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.tanh() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReUeh2el0MRA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "arr = []\n",
        "for _ in range(10):\n",
        "    tmp = tuple(np.random.rand(5))\n",
        "    arr.append(tmp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nm-JGOMt0aEc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a,b,_,_,_ = zip(*np.array(arr[1:4]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYfVe-xeWSu4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Ctmp:\n",
        "    def __init__(self):\n",
        "        self.a = 10\n",
        "    def train(self):\n",
        "        for i in range(10**20):\n",
        "            if i % 10**7 == 0:\n",
        "                self.a += 1\n",
        "                print(self.a)\n",
        "                \n",
        "vv = Ctmp()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNS1NxxTv0xM",
        "colab_type": "text"
      },
      "source": [
        "## Test TrajBuf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3mdBranwcMC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3e850d91-04f8-48d1-8b53-15ac0fe8c9a6"
      },
      "source": [
        "tt = TrajectoriesEndBuffer(1, init_prob=0.9)\n",
        "for i in range(10,100):\n",
        "    tt.add(i)\n",
        "arr = tt.get_heruistc_probabilities()\n",
        "print(len(arr),arr)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 [1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spYtJGi9zKcH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}