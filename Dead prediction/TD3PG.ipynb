{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TD3PG.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "61VeKABEm9_O",
        "geRE4MK65Wnb",
        "UH9aTMAwIeTf",
        "YnZxLGmrlAyw"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akterskii/RL/blob/master/Dead%20prediction/TD3PG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4Ovhw-h0cfa",
        "colab_type": "code",
        "outputId": "b2d0cee7-3437-448c-9ba0-81bb75fcbc51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        }
      },
      "source": [
        "!pip install gym['box2d']"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.6/dist-packages (0.15.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (3.4.7.28)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.17.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.3.2)\n",
            "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.3.2)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.12.0)\n",
            "Collecting box2d-py~=2.3.5; extra == \"box2d\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/bd/6cdc3fd994b0649dcf5d9bad85bd9e26172308bbe9a421bfc6fdbf5081a6/box2d_py-2.3.8-cp36-cp36m-manylinux1_x86_64.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 9.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym[box2d]) (0.16.0)\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GuuZ4NNdAIa",
        "colab_type": "code",
        "outputId": "032a3421-91b3-4e59-8963-92de4c2cd04c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "# Run this cell to mount your Google Drive.\n",
        "from google.colab import drive\n",
        "mount = '/content/drive'\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e81Vm0EmsKyN",
        "colab_type": "text"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAAepgnysM01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os \n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "#for RAdam\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "\n",
        "# for custom env\n",
        "from gym.wrappers import TimeLimit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXdG4uSSr9Kx",
        "colab_type": "text"
      },
      "source": [
        "## RAdam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TuAFDpGr8aX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RAdam(Optimizer):\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
        "        self.buffer = [[None, None, None] for ind in range(10)]\n",
        "        super(RAdam, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(RAdam, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data.float()\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
        "\n",
        "                p_data_fp32 = p.data.float()\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
        "                else:\n",
        "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "\n",
        "                state['step'] += 1\n",
        "                buffered = self.buffer[int(state['step'] % 10)]\n",
        "                if state['step'] == buffered[0]:\n",
        "                    N_sma, step_size = buffered[1], buffered[2]\n",
        "                else:\n",
        "                    buffered[0] = state['step']\n",
        "                    beta2_t = beta2 ** state['step']\n",
        "                    N_sma_max = 2 / (1 - beta2) - 1\n",
        "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
        "                    buffered[1] = N_sma\n",
        "                    if N_sma > 5:\n",
        "                        step_size = group['lr'] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
        "                    else:\n",
        "                        step_size = group['lr'] / (1 - beta1 ** state['step'])\n",
        "                    buffered[2] = step_size\n",
        "\n",
        "                if group['weight_decay'] != 0:\n",
        "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
        "\n",
        "                if N_sma > 5:                    \n",
        "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n",
        "                else:\n",
        "                    p_data_fp32.add_(-step_size, exp_avg)\n",
        "\n",
        "                p.data.copy_(p_data_fp32)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu-XBmsVIJ-p",
        "colab_type": "text"
      },
      "source": [
        "## Actor and Critic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92njCCoUIMxr",
        "colab_type": "code",
        "outputId": "ff1a07ae-7520-4285-c4fa-e8acc6f408ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, max_action, name):\n",
        "        super(Actor, self).__init__()\n",
        "        \n",
        "        self.l1 = nn.Linear(state_dim, 400)\n",
        "        self.l2 = nn.Linear(400, 300)\n",
        "        self.l3 = nn.Linear(300, action_dim)\n",
        "        \n",
        "        self.max_action = max_action\n",
        "        \n",
        "        self.fname = name + '.pth'\n",
        "        \n",
        "    def forward(self, state):\n",
        "        a = F.relu(self.l1(state))\n",
        "        a = F.relu(self.l2(a))\n",
        "        a = torch.tanh(self.l3(a)) * self.max_action\n",
        "        return a\n",
        "    \n",
        "    \n",
        "        \n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, name, probability=False):\n",
        "        super(Critic, self).__init__()\n",
        "        \n",
        "        self.l1 = nn.Linear(state_dim + action_dim, 400)\n",
        "        self.l2 = nn.Linear(400, 300)\n",
        "        self.l3 = nn.Linear(300, 1)\n",
        "        \n",
        "        self.probability = probability\n",
        "        \n",
        "        self.name = name\n",
        "        self.fname = name + '.pth'\n",
        "        \n",
        "    def forward(self, state, action):\n",
        "        state_action = torch.cat([state, action], 1)\n",
        "        \n",
        "        q = F.relu(self.l1(state_action))\n",
        "        q = F.relu(self.l2(q))\n",
        "        q = self.l3(q)\n",
        "        if self.probability:\n",
        "            q = torch.sigmoid(q)\n",
        "        return q\n",
        "    "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHv86zi1IR8G",
        "colab_type": "text"
      },
      "source": [
        "## Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39B6n_Mq0kfK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TD3:\n",
        "    def __init__(self, lr, state_dim, action_dim, max_action, danger, danger_threshold, directory, fname, epochs_for_danger):\n",
        "        opt_RAdam = True\n",
        "        \n",
        "        self.actor = Actor(state_dim, action_dim, max_action, 'actor').to(device)\n",
        "        self.actor_target = Actor(state_dim, action_dim, max_action, 'actor_target').to(device)\n",
        "        self.actor_perturbed = Actor(state_dim, action_dim, max_action, 'actor_target').to(device)\n",
        "        \n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "        \n",
        "        \n",
        "        if opt_RAdam:\n",
        "            self.actor_optimizer = RAdam(self.actor.parameters(), lr=lr)\n",
        "        else:\n",
        "            self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n",
        "        \n",
        "        \n",
        "        self.critic_1 = Critic(state_dim, action_dim, 'critic_1').to(device)\n",
        "        self.critic_1_target = Critic(state_dim, action_dim, 'critic_1_target').to(device)\n",
        "        self.critic_1_target.load_state_dict(self.critic_1.state_dict())\n",
        "        if opt_RAdam:\n",
        "            self.critic_1_optimizer = RAdam(self.critic_1.parameters(), lr=lr)\n",
        "        else:\n",
        "            self.critic_1_optimizer = optim.Adam(self.critic_1.parameters(), lr=lr)\n",
        "        \n",
        "        self.critic_2 = Critic(state_dim, action_dim, 'critic_2').to(device)\n",
        "        self.critic_2_target = Critic(state_dim, action_dim, 'critic_2_target').to(device)\n",
        "        self.critic_2_target.load_state_dict(self.critic_2.state_dict())\n",
        "        if opt_RAdam:\n",
        "            self.critic_2_optimizer = RAdam(self.critic_2.parameters(), lr=lr)\n",
        "        else:\n",
        "            self.critic_2_optimizer = optim.Adam(self.critic_2.parameters(), lr=lr)\n",
        "        \n",
        "        self.max_action = max_action\n",
        "        \n",
        "        #danger\n",
        "        self.danger = danger\n",
        "        self.actor_danger = Actor(state_dim, action_dim, max_action, 'actor_danger').to(device)\n",
        "        if opt_RAdam:\n",
        "            self.actor_danger_optimizer = RAdam(self.actor_danger.parameters(), lr=lr)\n",
        "        else:\n",
        "            self.actor_danger_optimizer = optim.Adam(self.actor_danger.parameters(), lr=lr)\n",
        "        \n",
        "        self.critic_danger = Critic(state_dim, action_dim, 'critic_danger', probability=True).to(device)\n",
        "        if opt_RAdam:\n",
        "            self.critic_danger_optimizer = RAdam(self.critic_danger.parameters(), lr=lr)\n",
        "        else:       \n",
        "            self.critic_danger_optimizer = optim.Adam(self.critic_danger.parameters(), lr=lr)\n",
        "        \n",
        "        self.threshold = danger_threshold\n",
        "        self.directory = directory\n",
        "        self.fname = fname\n",
        "        self.epochs_for_danger = epochs_for_danger\n",
        "        self.action_update = False\n",
        "          \n",
        "    \n",
        "    def select_action(self, state, danger=False, param_noise=False, debug=False):\n",
        "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)        \n",
        "        \n",
        "        if param_noise:\n",
        "            self.actor_perturbed.eval()\n",
        "            action = self.actor_perturbed(state)\n",
        "            self.actor_perturbed.train()\n",
        "        else:\n",
        "            self.actor.eval()\n",
        "            action = self.actor(state)\n",
        "            self.actor.train()\n",
        "            \n",
        "        if danger:\n",
        "            self.action_update = False\n",
        "            \n",
        "            # estimate probability of death\n",
        "            init_prob_danger = self.critic_danger(state, action).cpu().data.numpy().flatten()\n",
        "            if  init_prob_danger > self.threshold:\n",
        "                \n",
        "                # safe action in danger case\n",
        "                self.actor_danger.eval() # set evaluation mode\n",
        "                action = self.actor_danger(state)\n",
        "                self.actor_danger.train() # set vack train mode\n",
        "                \n",
        "                self.action_update = True\n",
        "                \n",
        "                if debug:\n",
        "                    new_prob_danger = self.critic_danger(state, action).cpu().data.numpy().flatten()\n",
        "                    print( \"\\t\\t\\tP before: {}. P after: {}\".format(init_prob_danger, new_prob_danger))\n",
        "        \n",
        "        return action.cpu().data.numpy().flatten()\n",
        "    \n",
        "    \n",
        "    def perturb_actor_parameters(self, param_noise):\n",
        "        \"\"\"Apply parameter noise to actor model, for exploration\"\"\"\n",
        "        hard_update(self.actor_perturbed, self.actor)\n",
        "        params = self.actor_perturbed.state_dict()\n",
        "        for name in params:\n",
        "            if 'ln' in name: \n",
        "                pass \n",
        "            param = params[name]\n",
        "            random = torch.randn(param.shape).to(device)\n",
        "            \n",
        "            param += random * param_noise.current_stddev\n",
        "        \n",
        "    \n",
        "    def update(self, replay_buffer, replay_buffer_danger, n_iter, batch_size, batch_size_danger, gamma, polyak, policy_noise, noise_clip, policy_delay):\n",
        "        \n",
        "        for i in range(n_iter):\n",
        "            # Sample a batch of transitions from replay buffer:\n",
        "            state, action_, reward, next_state, done = replay_buffer.sample(batch_size)            \n",
        "            state = torch.FloatTensor(state).to(device)\n",
        "            action = torch.FloatTensor(action_).to(device)\n",
        "            reward = torch.FloatTensor(reward).reshape((batch_size,1)).to(device)\n",
        "            next_state = torch.FloatTensor(next_state).to(device)\n",
        "            done = torch.FloatTensor(done).reshape((batch_size,1)).to(device)\n",
        "                                    \n",
        "            # Select next action according to target policy:\n",
        "            noise = torch.FloatTensor(action_).data.normal_(0, policy_noise).to(device)\n",
        "            noise = noise.clamp(-noise_clip, noise_clip)\n",
        "            next_action = (self.actor_target(next_state) + noise)\n",
        "            next_action = next_action.clamp(-self.max_action, self.max_action)\n",
        "            \n",
        "            # Compute target Q-value:\n",
        "            target_Q1 = self.critic_1_target(next_state, next_action)\n",
        "            target_Q2 = self.critic_2_target(next_state, next_action)\n",
        "            target_Q = torch.min(target_Q1, target_Q2)\n",
        "            target_Q = reward + ((1-done) * gamma * target_Q).detach()\n",
        "            \n",
        "                                    \n",
        "            # Optimize Critic 1:\n",
        "            current_Q1 = self.critic_1(state, action)\n",
        "            loss_Q1 = F.mse_loss(current_Q1, target_Q)\n",
        "            self.critic_1_optimizer.zero_grad()\n",
        "            loss_Q1.backward()\n",
        "            self.critic_1_optimizer.step()\n",
        "            \n",
        "            # Optimize Critic 2:\n",
        "            current_Q2 = self.critic_2(state, action)\n",
        "            loss_Q2 = F.mse_loss(current_Q2, target_Q)\n",
        "            self.critic_2_optimizer.zero_grad()\n",
        "            loss_Q2.backward()\n",
        "            self.critic_2_optimizer.step()\n",
        "            \n",
        "            \n",
        "            # Delayed policy updates:\n",
        "            if i % policy_delay == 0:\n",
        "                # Compute actor loss:\n",
        "                actor_loss = -self.critic_1(state, self.actor(state)).mean()\n",
        "                \n",
        "                # Optimize the actor\n",
        "                self.actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                self.actor_optimizer.step()                \n",
        "                \n",
        "                # Polyak averaging update:\n",
        "                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "                    target_param.data.copy_( (polyak * target_param.data) + ((1-polyak) * param.data))\n",
        "                \n",
        "                for param, target_param in zip(self.critic_1.parameters(), self.critic_1_target.parameters()):\n",
        "                    target_param.data.copy_( (polyak * target_param.data) + ((1-polyak) * param.data))\n",
        "                \n",
        "                for param, target_param in zip(self.critic_2.parameters(), self.critic_2_target.parameters()):\n",
        "                    target_param.data.copy_( (polyak * target_param.data) + ((1-polyak) * param.data))\n",
        "                  \n",
        "                  \n",
        "        \n",
        "        \n",
        "        if self.danger and len(replay_buffer_danger.buffer) > 0:\n",
        "            batch_steps = max(1, replay_buffer_danger.size // batch_size_danger)\n",
        "            for _ in range(self.epochs_for_danger):\n",
        "                for j in range(batch_steps):\n",
        "                    # Sample two batches of transitions: deadend and normals\n",
        "                    state_not_danger, action_not_danger, _, _, done_not_danger = replay_buffer.sample(batch_size_danger)            \n",
        "                    state_not_danger = torch.FloatTensor(state_not_danger).to(device)\n",
        "                    action_not_danger = torch.FloatTensor(action_not_danger).to(device)\n",
        "                    done_not_danger = torch.FloatTensor(done_not_danger).reshape((batch_size_danger, 1)).to(device)\n",
        "            \n",
        "                    state_danger, action_danger, _, _, done_danger = replay_buffer_danger.sample(batch_size_danger)            \n",
        "                    state_danger = torch.FloatTensor(state_danger).to(device)\n",
        "                    action_danger = torch.FloatTensor(action_danger).to(device)\n",
        "                    done_danger = torch.FloatTensor(done_danger).reshape((batch_size_danger, 1)).to(device)\n",
        "                  \n",
        "                    # Compute danger probabilities\n",
        "                    target_Q_not_danger = done_not_danger\n",
        "                    target_Q_danger = done_danger\n",
        "                    #pprint(\"dan_q\", target_Q_not_danger, target_Q_danger)\n",
        "                  \n",
        "                    # Optimize Critic Danger:\n",
        "                    current_Q_danger = self.critic_danger(state_danger, action_danger)\n",
        "                    current_Q_not_danger = self.critic_danger(state_not_danger, action_not_danger)\n",
        "                    loss_Q_danger = F.mse_loss(current_Q_danger, target_Q_danger)\n",
        "                    loss_Q_not_danger = F.mse_loss(current_Q_not_danger, target_Q_not_danger)\n",
        "                    loss_QD =(loss_Q_danger + loss_Q_not_danger)/2\n",
        "                    self.critic_danger_optimizer.zero_grad()\n",
        "                    loss_QD.backward()\n",
        "                    self.critic_danger_optimizer.step()\n",
        "                    \n",
        "                    if j % policy_delay == 0:\n",
        "                        actor_danger_loss = self.critic_danger(state_danger, self.actor_danger(state_danger)).mean()\n",
        "                    \n",
        "                        # Optimize the actor for danger\n",
        "                        self.actor_danger_optimizer.zero_grad()\n",
        "                        actor_danger_loss.backward()\n",
        "                        self.actor_danger_optimizer.step()           \n",
        "                     \n",
        "                \n",
        "    def save(self, directory=None, fname=None, optimizers = False, danger = False):\n",
        "        if directory is None:\n",
        "            directory = self.directory\n",
        "        if fname is None:\n",
        "            fname = self.fname\n",
        "            \n",
        "        base_path = \"%s/%s_\"% (directory, fname)\n",
        "        \n",
        "        torch.save(self.actor.state_dict(), base_path + self.actor.fname)\n",
        "        torch.save(self.actor_target.state_dict(), base_path + self.actor_target.fname)\n",
        "        \n",
        "        torch.save(self.critic_1.state_dict(), base_path + self.critic_1.fname)\n",
        "        torch.save(self.critic_1_target.state_dict(), base_path + self.critic_1_target.fname)\n",
        "        \n",
        "        torch.save(self.critic_2.state_dict(), base_path + self.critic_2.fname)\n",
        "        torch.save(self.critic_2_target.state_dict(), base_path + self.critic_2_target.fname)\n",
        "        \n",
        "        if danger:\n",
        "            torch.save(self.actor_danger.state_dict(),  base_path + self.actor_danger.fname)\n",
        "            torch.save(self.critic_danger.state_dict(), base_path + self.critic_danger.fname)\n",
        "        \n",
        "        if optimizers:\n",
        "            torch.save(self.actor_optimizer.state_dict(), '%s/%s_actor_optimizer.pth' % (directory, fname))\n",
        "            torch.save(self.critic_1_optimizer.state_dict(), '%s/%s_critic_1_optimizer.pth' % (directory, fname))\n",
        "            torch.save(self.critic_2_optimizer.state_dict(), '%s/%s_critic_2_optimizer.pth' % (directory, fname))\n",
        "            if danger:\n",
        "                torch.save(self.actor_danger_optimizer.state_dict(), '%s/%s_actor_danger_optimizer.pth' % (directory, fname))\n",
        "                torch.save(self.critic_danger_optimizer.state_dict(), '%s/%s_critic_danger_optimizer.pth' % (directory, fname))\n",
        "                \n",
        "        \n",
        "    def load(self, directory=None, fname=None, optimizers=False, danger = False):\n",
        "        if directory is None:\n",
        "            directory = self.directory\n",
        "        if fname is None:\n",
        "            fname = self.fname\n",
        "            \n",
        "        base_path = \"%s/%s_\"% (directory, fname)\n",
        "        \n",
        "        self.actor.load_state_dict(torch.load(base_path + self.actor.fname, map_location=lambda storage, loc: storage))\n",
        "        self.actor_target.load_state_dict(torch.load(base_path + self.actor_target.fname, map_location=lambda storage, loc: storage))\n",
        "        \n",
        "        self.critic_1.load_state_dict(torch.load(base_path + self.critic_1.fname, map_location=lambda storage, loc: storage))\n",
        "        self.critic_1_target.load_state_dict(torch.load(base_path + self.critic_1_target.fname, map_location=lambda storage, loc: storage))\n",
        "        \n",
        "        self.critic_2.load_state_dict(torch.load(base_path + self.critic_2.fname, map_location=lambda storage, loc: storage))\n",
        "        self.critic_2_target.load_state_dict(torch.load(base_path + self.critic_2_target.fname, map_location=lambda storage, loc: storage))\n",
        "        \n",
        "        if danger:\n",
        "            self.actor_danger.load_state_dict(torch.load('%s/%s_actor_danger.pth' % (directory, name), map_location=lambda storage, loc: storage))\n",
        "            self.critic_danger.load_state_dict(torch.load('%s/%s_critic_danger.pth' % (directory, name), map_location=lambda storage, loc: storage))\n",
        "        \n",
        "        if optimizers:\n",
        "            self.actor_optimizer.load_state_dict(torch.load( '%s/%s_actor_optimizer.pth' % (directory, name), map_location=lambda storage, loc: storage))\n",
        "            self.critic_1_optimizer.load_state_dict(torch.load('%s/%s_critic_1_optimizer.pth' % (directory, name), map_location=lambda storage, loc: storage))\n",
        "            self.critic_2_optimizer.load_state_dict(torch.load('%s/%s_critic_2_optimizer.pth' % (directory, name), map_location=lambda storage, loc: storage))\n",
        "            if danger:\n",
        "                self.actor_danger_optimizer.load_state_dict(torch.load(base_path + self.actor_danger.fname, map_location=lambda storage, loc: storage))\n",
        "                self.critic_danger_optimizer.load_state_dict(torch.load(base_path + self.critic_danger.fname, map_location=lambda storage, loc: storage))\n",
        "        \n",
        "        \n",
        "    def load_actor(self, directory=None, fname=None, danger=False):\n",
        "        if directory is None:\n",
        "            directory = self.directory\n",
        "        if fname is None:\n",
        "            fname = self.fname      \n",
        "      \n",
        "        base_path = \"%s/%s_\"% (directory, fname)\n",
        "        self.actor.load_state_dict(torch.load(base_path + self.actor.fname, map_location=lambda storage, loc: storage))\n",
        "        self.actor_target.load_state_dict(torch.load(base_path + self.actor_target.fname, map_location=lambda storage, loc: storage))\n",
        "        if danger:\n",
        "            self.actor_danger.load_state_dict(torch.load(base_path + self.actor_danger.fname, map_location=lambda storage, loc: storage))\n",
        "            self.critic_danger.load_state_dict(torch.load(base_path + self.critic_danger.fname, map_location=lambda storage, loc: storage))\n",
        "        \n",
        "        \n",
        "        \n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pr_4tXGr5Qzj",
        "colab_type": "text"
      },
      "source": [
        "##Custom Bipedal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQj8Rpz3yw2P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import warnings\n",
        "from gym.envs.box2d.bipedal_walker import *\n",
        "class CustomizableBipedalWalker(BipedalWalker):\n",
        "    def __init__(self):\n",
        "        self.default_params = {\n",
        "            'stump_height_low': 1,\n",
        "            'stump_height_high': 3,\n",
        "            'pit_depth': 4,\n",
        "            'pit_width_low': 3,\n",
        "            'pit_width_high': 5,\n",
        "            'stair_heights': [-.5, .5],\n",
        "            'stair_width_low': 4,\n",
        "            'stair_width_high': 5,\n",
        "            'stair_steps_low': 3,\n",
        "            'stair_steps_high': 5,\n",
        "            'states': [0],\n",
        "            'state_probs': None\n",
        "        }\n",
        "        self.params = {**self.default_params}\n",
        "        BipedalWalker.__init__(self)\n",
        "        \n",
        "    def _update_env_params(self, **kwargs):\n",
        "        # TODO: add kind of sanity check here\n",
        "        self.params = {**self.params, **kwargs}\n",
        "        _ = self.reset()\n",
        "        \n",
        "    def reset_env_params(self, hardcore=False):\n",
        "        params = {**self.default_params}\n",
        "        if hardcore:\n",
        "            params['states'] = np.arange(4)\n",
        "        self._update_env_params(**params)\n",
        "    \n",
        "    def set_env_states(self, state_mask, p=None):\n",
        "        \"\"\"\n",
        "        :param state_mask: np.array(,dtype=bool) that masks [\"GRASS\", \"STUMP\", \"STAIRS\", \"PIT\"].\n",
        "            Note that masking out \"GRASS\" takes no effect.\n",
        "        :param p: np.array or list of probabilities: [p_grass, p_stump, p_stairs, p_pit].\n",
        "            Probs corresponding to masked out states are ignored\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        states_ = np.arange(4)[state_mask]\n",
        "        p_ = None\n",
        "        if p is not None:\n",
        "            p_ = np.array(p)\n",
        "            if not np.all(p_ >= 0):\n",
        "                raise ValueError\n",
        "            p_ = p_[state_mask] / p_[state_mask].sum()\n",
        "        self._update_env_params(states=states_, state_probs=p_)\n",
        "    \n",
        "    def set_env_params(self, pit_width=None, stair_width=None, stair_steps=None, stump_height=None):\n",
        "        \"\"\"\n",
        "            NB: All params are integers or tuples of integers\n",
        "        \"\"\"\n",
        "        kwargs = {**locals()}\n",
        "        _ = kwargs.pop('self', None)\n",
        "        params = {}\n",
        "        for k,v in kwargs.items():\n",
        "            if type(v) is int:\n",
        "                params[k + '_low'] = v\n",
        "                params[k + '_high'] = v + 1\n",
        "            elif isinstance(v, (tuple, list)): \n",
        "                if v[1] - v[0] >= 1:\n",
        "                    params[k + '_low'] = v[0]\n",
        "                    params[k + '_high'] = v[1]\n",
        "                else:\n",
        "                    warnings.warn(f'{k} shoud be an integer. {k}[1] - {k}[0] < 1 '+\\\n",
        "                                  f'=> will set {k}_low = {v[0]}, {k}_high = {v[0]+1}')\n",
        "                    params[k + '_low'] = v[0]\n",
        "                    params[k + '_high'] = v[0] + 1\n",
        "        self._update_env_params(**params)\n",
        "        \n",
        "    def _generate_terrain(self, hardcore=True):\n",
        "        GRASS, STUMP, STAIRS, PIT, _STATES_ = range(5)\n",
        "        state    = GRASS\n",
        "        velocity = 0.0\n",
        "        y        = TERRAIN_HEIGHT\n",
        "        counter  = TERRAIN_STARTPAD\n",
        "        oneshot  = False\n",
        "        self.terrain   = []\n",
        "        self.terrain_x = []\n",
        "        self.terrain_y = []\n",
        "        for i in range(TERRAIN_LENGTH):\n",
        "            x = i*TERRAIN_STEP\n",
        "            self.terrain_x.append(x)\n",
        "\n",
        "            if state==GRASS and not oneshot:\n",
        "                velocity = 0.8*velocity + 0.01*np.sign(TERRAIN_HEIGHT - y)\n",
        "                if i > TERRAIN_STARTPAD: velocity += self.np_random.uniform(-1, 1)/SCALE   #1\n",
        "                y += velocity\n",
        "\n",
        "            elif state==PIT and oneshot:\n",
        "                counter = self.np_random.randint(self.params['pit_width_low'], \n",
        "                                                 self.params['pit_width_high'])\n",
        "                PIT_H = self.params['pit_depth']\n",
        "                poly = [\n",
        "                    (x,              y),\n",
        "                    (x+TERRAIN_STEP, y),\n",
        "                    (x+TERRAIN_STEP, y-PIT_H*TERRAIN_STEP),\n",
        "                    (x,              y-PIT_H*TERRAIN_STEP),\n",
        "                    ]\n",
        "                self.fd_polygon.shape.vertices=poly\n",
        "                t = self.world.CreateStaticBody(\n",
        "                    fixtures = self.fd_polygon)\n",
        "                t.color1, t.color2 = (1,1,1), (0.6,0.6,0.6)\n",
        "                self.terrain.append(t)\n",
        "\n",
        "                self.fd_polygon.shape.vertices=[(p[0]+TERRAIN_STEP*counter,p[1]) for p in poly]\n",
        "                t = self.world.CreateStaticBody(\n",
        "                    fixtures = self.fd_polygon)\n",
        "                t.color1, t.color2 = (1,1,1), (0.6,0.6,0.6)\n",
        "                self.terrain.append(t)\n",
        "                counter += 2\n",
        "                original_y = y\n",
        "\n",
        "            elif state==PIT and not oneshot:\n",
        "                y = original_y\n",
        "                if counter > 1:\n",
        "                    y -= PIT_H*TERRAIN_STEP\n",
        "\n",
        "            elif state==STUMP and oneshot:\n",
        "                counter = self.np_random.randint(self.params['stump_height_low'], self.params['stump_height_high'])\n",
        "                poly = [\n",
        "                    (x,                      y),\n",
        "                    (x+counter*TERRAIN_STEP, y),\n",
        "                    (x+counter*TERRAIN_STEP, y+counter*TERRAIN_STEP),\n",
        "                    (x,                      y+counter*TERRAIN_STEP),\n",
        "                    ]\n",
        "                self.fd_polygon.shape.vertices=poly\n",
        "                t = self.world.CreateStaticBody(\n",
        "                    fixtures = self.fd_polygon)\n",
        "                t.color1, t.color2 = (1,1,1), (0.6,0.6,0.6)\n",
        "                self.terrain.append(t)\n",
        "\n",
        "            elif state==STAIRS and oneshot:\n",
        "                stair_height = self.np_random.choice(self.params['stair_heights'])\n",
        "                stair_width = self.np_random.randint(self.params['stair_width_low'], \n",
        "                                                     self.params['stair_width_high'])\n",
        "                stair_steps = self.np_random.randint(self.params['stair_steps_low'], \n",
        "                                                     self.params['stair_steps_high'])\n",
        "                original_y = y\n",
        "                for s in range(stair_steps):\n",
        "                    poly = [\n",
        "                        (x+(    s*stair_width)*TERRAIN_STEP, y+(   s*stair_height)*TERRAIN_STEP),\n",
        "                        (x+((1+s)*stair_width)*TERRAIN_STEP, y+(   s*stair_height)*TERRAIN_STEP),\n",
        "                        (x+((1+s)*stair_width)*TERRAIN_STEP, y+(-1+s*stair_height)*TERRAIN_STEP),\n",
        "                        (x+(    s*stair_width)*TERRAIN_STEP, y+(-1+s*stair_height)*TERRAIN_STEP),\n",
        "                        ]\n",
        "                    self.fd_polygon.shape.vertices=poly\n",
        "                    t = self.world.CreateStaticBody(\n",
        "                        fixtures = self.fd_polygon)\n",
        "                    t.color1, t.color2 = (1,1,1), (0.6,0.6,0.6)\n",
        "                    self.terrain.append(t)\n",
        "                counter = stair_steps*stair_width\n",
        "\n",
        "            elif state==STAIRS and not oneshot:\n",
        "                s = stair_steps*stair_width - counter - stair_height\n",
        "                n = s/stair_width\n",
        "                y = original_y + (n*stair_height)*TERRAIN_STEP\n",
        "\n",
        "            oneshot = False\n",
        "            self.terrain_y.append(y)\n",
        "            counter -= 1\n",
        "            if counter==0:\n",
        "                counter = self.np_random.randint(TERRAIN_GRASS/2, TERRAIN_GRASS)\n",
        "                if state==GRASS:\n",
        "                    state = self.np_random.choice(self.params['states'], p=self.params['state_probs'])\n",
        "                    oneshot = True\n",
        "                else:\n",
        "                    state = GRASS\n",
        "                    oneshot = True\n",
        "\n",
        "        self.terrain_poly = []\n",
        "        for i in range(TERRAIN_LENGTH-1):\n",
        "            poly = [\n",
        "                (self.terrain_x[i],   self.terrain_y[i]),\n",
        "                (self.terrain_x[i+1], self.terrain_y[i+1])\n",
        "                ]\n",
        "            self.fd_edge.shape.vertices=poly\n",
        "            t = self.world.CreateStaticBody(\n",
        "                fixtures = self.fd_edge)\n",
        "            color = (0.3, 1.0 if i%2==0 else 0.8, 0.3)\n",
        "            t.color1 = color\n",
        "            t.color2 = color\n",
        "            self.terrain.append(t)\n",
        "            color = (0.4, 0.6, 0.3)\n",
        "            poly += [ (poly[1][0], 0), (poly[0][0], 0) ]\n",
        "            self.terrain_poly.append( (poly, color) )\n",
        "        self.terrain.reverse()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rqNDNCWf-Z3",
        "colab_type": "text"
      },
      "source": [
        "## Noise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_QrGdK9gC32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AdaptiveParamNoiseSpec(object):\n",
        "    def __init__(self, initial_stddev=0.1, desired_action_stddev=0.2, adaptation_coefficient=1.01):\n",
        "        \"\"\"\n",
        "        Note that initial_stddev and current_stddev refer to std of parameter noise, \n",
        "        but desired_action_stddev refers to (as name notes) desired std in action space\n",
        "        \"\"\"\n",
        "        self.initial_stddev = initial_stddev\n",
        "        self.desired_action_stddev = desired_action_stddev\n",
        "        self.adaptation_coefficient = adaptation_coefficient\n",
        "\n",
        "        self.current_stddev = initial_stddev\n",
        "\n",
        "    def adapt(self, sq_distance):\n",
        "        \"\"\"\n",
        "        Expaects \n",
        "        \"\"\"\n",
        "        \n",
        "        if sq_distance > self.desired_action_stddev ** 2:\n",
        "            # Decrease stddev.\n",
        "            self.current_stddev /= self.adaptation_coefficient\n",
        "        else:\n",
        "            # Increase stddev.\n",
        "            self.current_stddev *= self.adaptation_coefficient\n",
        "\n",
        "    def get_stats(self):\n",
        "        stats = {\n",
        "            'param_noise_stddev': self.current_stddev,\n",
        "        }\n",
        "        return stats\n",
        "\n",
        "    def __repr__(self):\n",
        "        fmt = 'AdaptiveParamNoiseSpec(initial_stddev={}, desired_action_stddev={}, adaptation_coefficient={})'\n",
        "        return fmt.format(self.initial_stddev, self.desired_action_stddev, self.adaptation_coefficient)\n",
        "\n",
        "def ddpg_sq_distance_metric(actions1, actions2):\n",
        "    \"\"\"\n",
        "    Compute SQUARE of the \"distance\" between actions taken by two policies at the same states\n",
        "    Expects numpy arrays\n",
        "    \"\"\"\n",
        "    diff = actions1-actions2\n",
        "    mean_diff = np.mean(np.square(diff), axis=0)\n",
        "    sq_dist = np.mean(mean_diff)\n",
        "    return sq_dist\n",
        "\n",
        "\n",
        "def hard_update(target, source):\n",
        "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
        "           target_param.data.copy_(param.data)\n",
        "            \n",
        "            \n",
        "class Noise:\n",
        "    def __init__(self, dim):\n",
        "        self.dim = dim\n",
        "    \n",
        "        #normal\n",
        "    \n",
        "        #ou\n",
        "    \n",
        "    \n",
        "    def get_noise(self, n_type='normal'):\n",
        "        if n_type == 'normal':\n",
        "            return \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oC4_CZH8IET9",
        "colab_type": "text"
      },
      "source": [
        "## Replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkalC0GW0zcf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size=1e6, base_path=None, name=None):\n",
        "        self.buffer = []\n",
        "        self.max_size = int(max_size)\n",
        "        self.size = 0\n",
        "        \n",
        "        if base_path is not None:\n",
        "            self.base_path = base_path\n",
        "        else:\n",
        "            self.base_path = ''\n",
        "            \n",
        "        if name is not None:\n",
        "            self.fname = name + '.pth'\n",
        "        else:\n",
        "            self.fname = 'buffer.pth'\n",
        "            \n",
        "        \n",
        "    \n",
        "    def add(self, transition):\n",
        "        self.size +=1\n",
        "        # transiton is tuple of (state, action, reward, next_state, done)\n",
        "        self.buffer.append(transition)\n",
        "       \n",
        "    \n",
        "    def save(self):\n",
        "        try:\n",
        "            with open(self.basename + self.fname , 'wb') as f:\n",
        "                pickle.dump(self.buffer, f)\n",
        "        except OSError:\n",
        "            print('Buffer is not saved!\\n\\n')\n",
        "            \n",
        "            \n",
        "    def load(self):\n",
        "        try:\n",
        "            with open(self.basename + self.fname , 'rb') as f:\n",
        "                self.buffer = pickle.load(f)\n",
        "        except OSError:\n",
        "            self.buffer = []\n",
        "            print('Buffer is not loaded!\\n\\n')\n",
        "    \n",
        "          \n",
        "    def sample(self, batch_size):\n",
        "        # delete 1/5th of the buffer when full\n",
        "        if self.size > self.max_size:\n",
        "            del self.buffer[0:int(self.size/5)]\n",
        "            self.size = len(self.buffer)\n",
        "        \n",
        "        indexes = np.random.randint(0, len(self.buffer), size=batch_size)\n",
        "        state, action, reward, next_state, done = [], [], [], [], []\n",
        "        \n",
        "        for i in indexes:\n",
        "            s, a, r, s_, d = self.buffer[i]\n",
        "            state.append(np.array(s, copy=False))\n",
        "            action.append(np.array(a, copy=False))\n",
        "            reward.append(np.array(r, copy=False))\n",
        "            next_state.append(np.array(s_, copy=False))\n",
        "            done.append(np.array(d, copy=False))\n",
        "        \n",
        "        return np.array(state), np.array(action), np.array(reward), np.array(next_state), np.array(done)\n",
        "     \n",
        "    def get_last(self, amount):\n",
        "        state, action, reward, next_state, done = [], [], [], [], []\n",
        "        if amount < len(self.buffer):\n",
        "            start = len(self.buffer) - amount\n",
        "        else:\n",
        "            start = 0\n",
        "        for i in range(start,len(self.buffer)):\n",
        "            s, a, r, s_, d = self.buffer[i]\n",
        "            state.append(np.array(s, copy=False))\n",
        "            action.append(np.array(a, copy=False))\n",
        "            reward.append(np.array(r, copy=False))\n",
        "            next_state.append(np.array(s_, copy=False))\n",
        "            done.append(np.array(d, copy=False))\n",
        "        return s, a, r, s_, d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61VeKABEm9_O",
        "colab_type": "text"
      },
      "source": [
        "## Trajectory buffer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CEo4_gcm-kh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import deque\n",
        "class TrajectoriesEndBuffer():\n",
        "    def __init__(self, max_trajectory_size=10, init_prob=0.7):\n",
        "        assert type(max_trajectory_size) is int\n",
        "        assert max_trajectory_size > 0\n",
        "        \n",
        "        self.max_trajectory_size = max_trajectory_size\n",
        "        self.buffer = deque()\n",
        "        \n",
        "        #linear probability params\n",
        "        assert type(init_prob) is float\n",
        "        assert 0 <= init_prob <= 1\n",
        "        self.min_prob = init_prob\n",
        "        \n",
        "        #TRY other cases\n",
        "        \n",
        "    def add(self, transition):\n",
        "        if len(self.buffer) >= self.max_trajectory_size:\n",
        "            self.buffer.popleft()    \n",
        "        self.buffer.append(transition)\n",
        "    \n",
        "    \n",
        "    def get_heruistc_probabilities(self):\n",
        "        cur_len = len(self.buffer)\n",
        "        if cur_len > 0 and self.max_trajectory_size == 1:\n",
        "            return [1]\n",
        "\n",
        "        if cur_len < self.max_trajectory_size:\n",
        "            shift = self.max_trajectory_size - cur_len\n",
        "        else:\n",
        "            shift = 0\n",
        "        prob = [0] * cur_len\n",
        "        \n",
        "        # linear from\n",
        "        min_prob = self.min_prob\n",
        "        for i in range(cur_len):\n",
        "            prob[i] = min_prob + (1 - min_prob) * ( (i + shift) / (self.max_trajectory_size - 1) )\n",
        "            \n",
        "        return prob\n",
        "    \n",
        "    def transfer_to_replay_buffer(self, replay_buffer):\n",
        "        if len(self.buffer) == 0: \n",
        "            print('Trajectories buffer is empty')\n",
        "            return\n",
        "        \n",
        "        if self.buffer[-1][4] == False:\n",
        "            print('Trajectory is not finished')\n",
        "            return\n",
        "          \n",
        "        cur_len = len(self.buffer)\n",
        "        prob = self.get_heruistc_probabilities()\n",
        "        \n",
        "        for i in range(cur_len):\n",
        "            s1, a, r, s2, done = self.buffer[i]\n",
        "            replay_buffer.add((s1, a, r, s2, float(prob[i])))\n",
        "        \n",
        "        self.buffer = deque()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEEchoyDrpAR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzz69TYi7f2q",
        "colab_type": "text"
      },
      "source": [
        "## Research class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqj3YyUT7ebB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random \n",
        "class Research:\n",
        "    def set_danger_batch_size(self):\n",
        "        self.batch_size_danger = self.batch_size // 2\n",
        "        \n",
        "    def update_params(self, params):\n",
        "        #print(params)\n",
        "        param_names = ['random_seed', \n",
        "                       'max_episodes', 'max_timesteps', 'batch_size', \n",
        "                       'lr', 'polyak', 'policy_delay', 'gamma', 'log_interval', \n",
        "                       'batch_size_danger', \n",
        "                       'policy_noise',\n",
        "                       'danger_enable', 'danger_threshold', 'max_trajectory_size',\n",
        "                       'directory','filename_load', 'filename_save', 'epochs_for_danger',\n",
        "                       'noise_random_enable', 'exploration_noise', 'noise_clip',\n",
        "                       'max_iter_danger', 'evaluate_after_episodes', 'evaluate_average_on_episodes', 'env_name_load', \n",
        "                       'env_name',\n",
        "                       'param_noise_enable','initial_stddev','desired_action_stddev','adaptation_coefficient']\n",
        "        \n",
        "        for param_name in param_names:\n",
        "            if param_name in params:\n",
        "                \n",
        "                setattr(self, param_name, params[param_name])\n",
        "                print(\"{} set to {}\".format(param_name, params[param_name]))\n",
        "        \n",
        "        \n",
        "    def __init__(self, params):\n",
        "        ###########################################\n",
        "        ###         default values\n",
        "        ###########################################\n",
        "        \n",
        "        self.random_seed = 1\n",
        "        \n",
        "        # environment\n",
        "        self.env_rewards = {'BipedalWalkerHardcore-v2':[-100,300], 'BipedalWalker-v2':[-100,300],\n",
        "                            'LunarLanderContinuous-v2':[-100,200],'Pendulum-v0':[-1000,200]}\n",
        "\n",
        "        self.env_name_load = 'BipedalWalker-v2'\n",
        "        self.env_name = 'BipedalWalker-v2'\n",
        "        \n",
        "        if self.env_name in self.env_rewards:\n",
        "            self.terminal_reward = self.env_rewards[self.env_name][0]\n",
        "            self.passed_reward   = self.env_rewards[self.env_name][1]\n",
        "        else:\n",
        "            self.terminal_reward = None\n",
        "            self.passed_reward   = None\n",
        "        \n",
        "        self.env = None\n",
        "        \n",
        "        ## training \n",
        "        self.max_episodes = 1000         # max num of episodes\n",
        "        self.max_timesteps = 2000        # max timesteps in one episode\n",
        "        self.batch_size = 100            # num of transitions sampled from replay buffer\n",
        "        self.lr = 0.001\n",
        "        self.polyak = 0.995              # target policy update parameter (1-tau)\n",
        "        self.policy_delay = 2            # delayed policy updates parameter\n",
        "        \n",
        "        ## model params\n",
        "        self.gamma = 0.99                # discount for future rewards\n",
        "        self.warmup_steps = 10**4\n",
        "        \n",
        "        ##############################   \n",
        "        ##   Noise params\n",
        "        ##############################   \n",
        "        \n",
        "        # Random noise\n",
        "        self.noise_random_enable = True\n",
        "        self.exploration_noise = 0.1\n",
        "        self.policy_noise = 0.2          # target policy smoothing noise\n",
        "        self.noise_clip = 0.5\n",
        "        \n",
        "        # Param noise\n",
        "        self.param_noise_enable = False\n",
        "        self.noise_distance_batch_size = 10 ** 4\n",
        "        self.initial_stddev=0.05\n",
        "        self.desired_action_stddev=0.45 \n",
        "        self.adaptation_coefficient=1.03\n",
        "        \n",
        "        ## Danger params\n",
        "        self.danger_enable = True               # enable dnager mode\n",
        "        self.danger_threshold = 0.7      # probability to perform safe action\n",
        "        self.epochs_for_danger = 3       # epochs to train danger\n",
        "        self.max_iter_danger = 5         # iteration per iteration\n",
        "        self.batch_size_danger = self.batch_size // 2\n",
        "        self.max_trajectory_size = 10    # length of trajectory before death\n",
        "        self.action_updates = 0\n",
        "        \n",
        "        ## evaluation \n",
        "        self.evaluate_average_on_episodes = 10\n",
        "        self.evaluate_after_episodes = 30\n",
        "        self.evaluate_after_timesteps = 10000\n",
        "        \n",
        "        ## InpOut params\n",
        "        self.log_interval = 10                                              # print avg reward after interval\n",
        "        self.directory = \"/content/drive/My Drive\"                          # save trained models\n",
        "        self.filename_load = \"TD3_{}_{}\".format(self.env_name_load, self.random_seed)\n",
        "        self.filename_save = \"TD3_{}_{}\".format(self.env_name, self.random_seed)\n",
        "        ###########################################\n",
        "        \n",
        "        ## Buffers\n",
        "        self.replay_buffer = None\n",
        "        self.replay_buffer_dead = None\n",
        "        self.trajectory_buffer = None\n",
        "        \n",
        "        \n",
        "        ## Policy\n",
        "        self.policy = None\n",
        "        \n",
        "        ###########################################\n",
        "        ## Update default params\n",
        "        ###########################################\n",
        "        self.update_params(params)\n",
        "        \n",
        "        \n",
        "    def load_model(self, env_name, random_seed):\n",
        "        pass\n",
        "        \n",
        "        \n",
        "    def init_env(self, params = None):\n",
        "        if params:\n",
        "            self.update_env(params)\n",
        "            \n",
        "        # create env\n",
        "        if self.env_name == \"BipedalWalkerHardcore-v2\":\n",
        "            \n",
        "            env = CustomizableBipedalWalker()\n",
        "            env.set_env_params(stump_height=1)\n",
        "            env.set_env_states(state_mask=np.array([1,1,0,0],dtype=bool), p=np.array([0.1,0.9,0.9,0.9]))\n",
        "            \n",
        "            self.env = TimeLimit(env, max_episode_steps=2000)\n",
        "        else:\n",
        "            self.env = gym.make(self.env_name)\n",
        "            \n",
        "        if '_max_episode_steps' in self.env.__dict__:\n",
        "            self.max_timesteps = self.env.__dict__['_max_episode_steps']\n",
        "        else:\n",
        "            self.max_timesteps = 10**7 ## \"inifinte\" value if nothing defined\n",
        "        \n",
        "        # env params\n",
        "        self.state_dim = self.env.observation_space.shape[0]\n",
        "        self.action_dim = self.env.action_space.shape[0]\n",
        "        self.max_action = float(self.env.action_space.high[0])\n",
        "        \n",
        "        #check symmetry of actions\n",
        "        assert  np.all((-self.env.action_space.low) == self.env.action_space.high)\n",
        "        \n",
        "        \n",
        "    def init_buffers_and_agent(self, load, solved):\n",
        "        \n",
        "        # Buffers\n",
        "        self.replay_buffer = ReplayBuffer()\n",
        "        if self.danger_enable:\n",
        "            self.replay_buffer_dead = ReplayBuffer()\n",
        "            self.trajectory_buffer = TrajectoriesEndBuffer(self.max_trajectory_size)\n",
        "    \n",
        "        # Policy\n",
        "        self.policy = TD3(self.lr, self.state_dim, self.action_dim, self.max_action,  self.danger_enable,\n",
        "                          self.danger_threshold, self.directory, self.filename_save, self.epochs_for_danger)\n",
        "    \n",
        "    \n",
        "        # load\n",
        "        if load:\n",
        "            full_fname = self.filename_load\n",
        "            if solved:\n",
        "                full_fname += \"_solved\"\n",
        "            self.policy.load(self.directory,  full_fname)\n",
        "           \n",
        "        \n",
        "    def train(self, params, debug=False):\n",
        "        self.update_params(params)\n",
        "        self.first_finish_ep_num  = -1\n",
        "        assert self.policy is not None\n",
        "        assert self.max_timesteps is not None\n",
        "        \n",
        "        # Random seed\n",
        "        if self.random_seed is not None:\n",
        "            os.environ['PYTHONHASHSEED']=str(self.random_seed)\n",
        "            print(\"Random Seed: {}\".format(self.random_seed))\n",
        "            np.random.seed(self.random_seed)\n",
        "            self.env.seed(self.random_seed)\n",
        "            random.seed(self.random_seed)\n",
        "            torch.manual_seed(self.random_seed)\n",
        "         \n",
        "    \n",
        "        # logging variables:\n",
        "        avg_reward = 0\n",
        "        ep_reward = 0\n",
        "        tot_time_steps = 0\n",
        "        average_time_steps = 0\n",
        "    \n",
        "        log_f = open(\"log.txt\",\"w+\")\n",
        "    \n",
        "        # training procedure:\n",
        "        print('Start training')\n",
        "        reach_the_end = False\n",
        "        \n",
        "        if self.param_noise_enable:\n",
        "            param_noise = AdaptiveParamNoiseSpec(initial_stddev=0.05,desired_action_stddev=0.45, adaptation_coefficient=1.05)\n",
        "        # amount of updates in dangers\n",
        "        tot_updates_amount = 0\n",
        "        self.action_updates = 0\n",
        "        ep_updates_amount = 0\n",
        "\n",
        "        \n",
        "        \n",
        "        for episode in range(1, self.max_episodes + 1):\n",
        "            \n",
        "            ep_updates_amount = 0\n",
        "            state = self.env.reset()        \n",
        "            \n",
        "            if  self.param_noise_enable:\n",
        "                self.policy.perturb_actor_parameters(param_noise)\n",
        "            \n",
        "            for t in range(self.max_timesteps):\n",
        "                # select action and add exploration noise:\n",
        "                \n",
        "                if tot_time_steps + t <= self.warmup_steps:\n",
        "                    action = self.env.action_space.sample()\n",
        "                else:\n",
        "                    action = self.policy.select_action(state, danger=self.danger_enable, param_noise=self.param_noise_enable)\n",
        "                    if self.policy.action_update:\n",
        "                        ep_updates_amount += 1\n",
        "                    \n",
        "                    if self.noise_random_enable:\n",
        "                        action = action + np.random.normal(0, self.exploration_noise, size=self.env.action_space.shape[0])\n",
        "                    action = action.clip(self.env.action_space.low, self.env.action_space.high)\n",
        "                    \n",
        "                #print(\"action {}\".format(action))\n",
        "                # take action in env:\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "            \n",
        "                self.replay_buffer.add((state, action, reward, next_state, float(done)))\n",
        "                if self.danger_enable:\n",
        "                    self.trajectory_buffer.add((state, action, reward, next_state, float(done)))\n",
        "            \n",
        "                if reward == self.terminal_reward:\n",
        "\n",
        "                    if self.danger_enable:\n",
        "                        self.trajectory_buffer.transfer_to_replay_buffer(self.replay_buffer_dead)                    \n",
        "                    \n",
        "                        if debug:\n",
        "                            tmp_st = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
        "                            tmp_a = torch.FloatTensor(action.reshape(1, -1)).to(device)\n",
        "                            print(\"\\tDead after pair with prob: \", self.policy.critic_danger(tmp_st, tmp_a).cpu().data.numpy().flatten())\n",
        "                    \n",
        "                state = next_state\n",
        "                avg_reward += reward\n",
        "                ep_reward += reward\n",
        "                \n",
        "                #if done and t < self.max_timesteps - 1 and reward != self.terminal_reward and reach_the_end==False:\n",
        "                #    print('Reach the end. Episode: {}. r={} t={}'.format(episode, ep_reward, tot_time_steps + t))\n",
        "                #    reach_the_end = True\n",
        "                #    self.first_finish_ep_num = episode\n",
        "                #    break\n",
        "                \n",
        "                if done or t==(self.max_timesteps-1): \n",
        "                    tot_time_steps += t\n",
        "                    average_time_steps += t\n",
        "                    tot_updates_amount += ep_updates_amount \n",
        "                    self.action_updates += ep_updates_amount\n",
        "\n",
        "                    self.policy.update(self.replay_buffer, self.replay_buffer_dead, t, self.batch_size, self.batch_size_danger, self.gamma, self.polyak, self.policy_noise, self.noise_clip, self.policy_delay)\n",
        "                    if t==(self.max_timesteps - 1):                    \n",
        "                        print(\"\\tStuck, ep reward {}\".format(ep_reward))\n",
        "                    break\n",
        "            \n",
        "            # logging updates:\n",
        "            log_f.write('{},{}\\n'.format(episode, ep_reward))\n",
        "            log_f.flush()\n",
        "            ep_reward = 0\n",
        "            \n",
        "            # if avg reward > 300 then save and stop traning:\n",
        "            if (avg_reward / self.log_interval) >= 300:\n",
        "                print(\"########## Solved! ###########\")\n",
        "                name = self.filename_save + '_solved'\n",
        "                self.policy.save(self.directory, name, optimizers=True, danger=True)\n",
        "                log_f.close()\n",
        "                break\n",
        "            \n",
        "            # update perturbed actor\n",
        "            #print('start perturtbated update')\n",
        "            if self.param_noise_enable:\n",
        "                states, actions_perturbed, _, _, _ = self.replay_buffer.get_last(t)\n",
        "                actions_perturbed = np.array(actions_perturbed, copy=False)\n",
        "            \n",
        "                actions_clear = self.policy.select_action(states, danger=False, param_noise=False)\n",
        "                ddpg_sq_distance = ddpg_sq_distance_metric(actions_perturbed, actions_clear)\n",
        "                param_noise.adapt(ddpg_sq_distance)\n",
        "            #print('end perturtbated update t={}'.format(t))\n",
        "            \n",
        "            # save policy \n",
        "            if episode > 0 and episode % (5 * self.log_interval) == 0:\n",
        "                self.policy.save(self.directory, self.filename_save, optimizers=True, danger=True)\n",
        "            \n",
        "            # print avg reward every log interval:\n",
        "            if episode % self.log_interval == 0:\n",
        "                avg_reward = int(avg_reward / self.log_interval)\n",
        "                average_time_steps = int(average_time_steps / self.log_interval)\n",
        "                print(\"Episode: {}\\tAverage Reward: {}\\tAverage steps: {}\\t Total: {}\".format(episode, avg_reward, average_time_steps, tot_time_steps))\n",
        "                avg_reward = 0\n",
        "                average_time_steps = 0\n",
        "            \n",
        "            if episode % self.evaluate_after_episodes == 0 and episode >= 100:\n",
        "                reach_the_end, mean_reward = self.evaluate(params ={}, time_steps = tot_time_steps, episode = episode)\n",
        "                if reach_the_end:\n",
        "                    self.first_finish_ep_num = episode\n",
        "                    print('Reach the end')\n",
        "                    print('Updates\\tSteps\\tReward\\tEpisode\\tSeed')\n",
        "                    print('{}\\t{}\\t{}\\t{}\\t{}'.format(self.action_updates, tot_time_steps, mean_reward, episode, self.random_seed))\n",
        "                    break\n",
        "    \n",
        "    def evaluate(self, params, time_steps = None, episode = None):\n",
        "        self.update_params(params)\n",
        "        print('Evaluation. Timesteps: {} Episodes: {}'.format(time_steps, episode))\n",
        "        reach_the_end = 0\n",
        "        rewards = np.zeros(self.evaluate_average_on_episodes)\n",
        "        for ep in range(self.evaluate_average_on_episodes):\n",
        "            ep_reward = 0\n",
        "            state = self.env.reset()\n",
        "            for t in range(self.max_timesteps):\n",
        "                action = self.policy.select_action(state, debug=False, danger=True)\n",
        "                state, reward, done, _ = self.env.step(action)\n",
        "                ep_reward += reward\n",
        "                if done:\n",
        "                    s = \"\\t\\tDead {}\\tUpdate {}\".format( reward == self.terminal_reward,  self.policy.action_update)\n",
        "                    \n",
        "                    if reward != self.terminal_reward and t !=self.max_timesteps-1:\n",
        "                        print(s+'\\tFinished')\n",
        "                        reach_the_end += 1\n",
        "                    else:\n",
        "                        print(s)\n",
        "                    break\n",
        "\n",
        "            rewards[ep] = ep_reward \n",
        "            \n",
        "        print('\\t\\tMean reward: {}. All rewards: {}'.format(int(np.mean(rewards)), list(map(int,rewards)) ))\n",
        "        return reach_the_end == self.evaluate_average_on_episodes, int(np.mean(rewards))\n",
        "    \n",
        "    \n",
        "    def is_passed(self, step_num, reward):\n",
        "        if reward != self.terminal_reward:\n",
        "            if step_num < self.terminal_reward:\n",
        "                return True\n",
        "\n",
        "        return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC-3p-bFIcMg",
        "colab_type": "text"
      },
      "source": [
        "# Debug\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx4nESZqVJ9q",
        "colab_type": "text"
      },
      "source": [
        "## Test Research class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsFU93i6VOFo",
        "colab_type": "code",
        "outputId": "86926716-1a5a-4e9e-dce1-20ca48967ee7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import datetime\n",
        "params = {'env_name': \"LunarLanderContinuous-v2\",\n",
        "          'random_seed':332447659, \n",
        "          'danger_enable':True,\n",
        "          'max_trajectory_size' : 20,\n",
        "          'evaluate_after_episodes' : 5,\n",
        "          'evaluate_average_on_episodes' : 5,\n",
        "          #'max_timesteps':1000,\n",
        "          'exploration_noise':0.3,\n",
        "          'max_episodes' : 1000,}\n",
        "\n",
        "\n",
        "print(datetime.datetime.now())\n",
        "\n",
        "new_research = Research(params)\n",
        "\n",
        "new_research.init_env()\n",
        "new_research.init_buffers_and_agent(load=False, solved=False)\n",
        "# prime = {19152527, 21192173,  33535573,  14245223,    285781, \n",
        "#          90109337, 634600381, 892392491, 48473791903, 7238744783}\n",
        "#  'max_timesteps':10,\n",
        "print(new_research.max_timesteps)\n",
        "new_research.train({}, debug=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-11-20 22:06:12.178057\n",
            "random_seed set to 332447659\n",
            "max_episodes set to 1000\n",
            "danger_enable set to True\n",
            "max_trajectory_size set to 20\n",
            "exploration_noise set to 0.3\n",
            "evaluate_after_episodes set to 5\n",
            "evaluate_average_on_episodes set to 5\n",
            "env_name set to LunarLanderContinuous-v2\n",
            "1000\n",
            "Random Seed: 332447659\n",
            "Start training\n",
            "\tDead after pair with prob:  [0.5016838]\n",
            "\tDead after pair with prob:  [0.5011838]\n",
            "\tDead after pair with prob:  [0.54413456]\n",
            "\tDead after pair with prob:  [0.498179]\n",
            "\tDead after pair with prob:  [0.5008072]\n",
            "\tDead after pair with prob:  [0.5101867]\n",
            "\tDead after pair with prob:  [0.5067465]\n",
            "\tDead after pair with prob:  [0.5376061]\n",
            "\tDead after pair with prob:  [0.4780513]\n",
            "\tDead after pair with prob:  [0.617012]\n",
            "Episode: 10\tAverage Reward: -195\tAverage steps: 113\t Total: 1130\n",
            "\tDead after pair with prob:  [0.7569107]\n",
            "\tDead after pair with prob:  [0.74205565]\n",
            "\tDead after pair with prob:  [0.47140846]\n",
            "\tDead after pair with prob:  [0.80857617]\n",
            "\tDead after pair with prob:  [0.8641012]\n",
            "\tDead after pair with prob:  [0.8044194]\n",
            "\tDead after pair with prob:  [0.71250653]\n",
            "\tDead after pair with prob:  [0.24730816]\n",
            "\tDead after pair with prob:  [0.89568233]\n",
            "\tDead after pair with prob:  [0.0911987]\n",
            "Episode: 20\tAverage Reward: -240\tAverage steps: 112\t Total: 2254\n",
            "\tDead after pair with prob:  [0.97764164]\n",
            "\tDead after pair with prob:  [0.8812357]\n",
            "\tDead after pair with prob:  [0.93135774]\n",
            "\tDead after pair with prob:  [0.50619835]\n",
            "\tDead after pair with prob:  [0.82497394]\n",
            "\tDead after pair with prob:  [0.13259396]\n",
            "\tDead after pair with prob:  [0.9976808]\n",
            "\tDead after pair with prob:  [0.9555713]\n",
            "\tDead after pair with prob:  [0.89429814]\n",
            "\tDead after pair with prob:  [0.8734665]\n",
            "Episode: 30\tAverage Reward: -218\tAverage steps: 104\t Total: 3295\n",
            "\tDead after pair with prob:  [0.8412418]\n",
            "\tDead after pair with prob:  [0.85318476]\n",
            "\tDead after pair with prob:  [0.9167431]\n",
            "\tDead after pair with prob:  [0.9159846]\n",
            "\tDead after pair with prob:  [0.94480157]\n",
            "\tDead after pair with prob:  [0.9738998]\n",
            "\tDead after pair with prob:  [0.8274661]\n",
            "\tDead after pair with prob:  [0.87266076]\n",
            "\tDead after pair with prob:  [0.86427313]\n",
            "\tDead after pair with prob:  [0.9131694]\n",
            "Episode: 40\tAverage Reward: -174\tAverage steps: 104\t Total: 4342\n",
            "\tDead after pair with prob:  [0.9715109]\n",
            "\tDead after pair with prob:  [0.8932612]\n",
            "\tDead after pair with prob:  [0.85190594]\n",
            "\tDead after pair with prob:  [0.9555033]\n",
            "\tDead after pair with prob:  [0.9150191]\n",
            "\tDead after pair with prob:  [0.95290214]\n",
            "\tDead after pair with prob:  [0.88170123]\n",
            "\tDead after pair with prob:  [0.8930301]\n",
            "\tDead after pair with prob:  [0.87119484]\n",
            "\tDead after pair with prob:  [0.98214716]\n",
            "Episode: 50\tAverage Reward: -217\tAverage steps: 106\t Total: 5410\n",
            "\tDead after pair with prob:  [0.8879304]\n",
            "\tDead after pair with prob:  [0.9981839]\n",
            "\tDead after pair with prob:  [0.76755404]\n",
            "\tDead after pair with prob:  [0.8597953]\n",
            "\tDead after pair with prob:  [0.92688316]\n",
            "\tDead after pair with prob:  [0.85943735]\n",
            "\tDead after pair with prob:  [0.8917206]\n",
            "\tDead after pair with prob:  [0.9484641]\n",
            "\tDead after pair with prob:  [0.9299809]\n",
            "\tDead after pair with prob:  [0.92135656]\n",
            "Episode: 60\tAverage Reward: -298\tAverage steps: 117\t Total: 6582\n",
            "\tDead after pair with prob:  [0.8751165]\n",
            "\tDead after pair with prob:  [0.9326657]\n",
            "\tDead after pair with prob:  [0.95007926]\n",
            "\tDead after pair with prob:  [0.90772027]\n",
            "\tDead after pair with prob:  [0.8221293]\n",
            "\tDead after pair with prob:  [0.9023504]\n",
            "\tDead after pair with prob:  [0.8521076]\n",
            "\tDead after pair with prob:  [0.6863737]\n",
            "\tDead after pair with prob:  [0.22463268]\n",
            "\tDead after pair with prob:  [0.8910525]\n",
            "Episode: 70\tAverage Reward: -223\tAverage steps: 108\t Total: 7662\n",
            "\tDead after pair with prob:  [0.50386846]\n",
            "\tDead after pair with prob:  [0.8914663]\n",
            "\tDead after pair with prob:  [0.7349564]\n",
            "\tDead after pair with prob:  [0.834191]\n",
            "\tDead after pair with prob:  [0.90428597]\n",
            "\tDead after pair with prob:  [0.7751213]\n",
            "\tDead after pair with prob:  [0.87714595]\n",
            "\tDead after pair with prob:  [0.81528544]\n",
            "\tDead after pair with prob:  [0.88482565]\n",
            "\tDead after pair with prob:  [0.8954517]\n",
            "Episode: 80\tAverage Reward: -217\tAverage steps: 100\t Total: 8668\n",
            "\tDead after pair with prob:  [0.9739135]\n",
            "\tDead after pair with prob:  [0.8273414]\n",
            "\tDead after pair with prob:  [0.9141464]\n",
            "\tDead after pair with prob:  [0.85530037]\n",
            "\tDead after pair with prob:  [0.6078859]\n",
            "\tDead after pair with prob:  [0.9534842]\n",
            "\tDead after pair with prob:  [0.8869269]\n",
            "\tDead after pair with prob:  [0.9841861]\n",
            "\tDead after pair with prob:  [0.98389906]\n",
            "\tDead after pair with prob:  [0.9746343]\n",
            "Episode: 90\tAverage Reward: -296\tAverage steps: 109\t Total: 9758\n",
            "\tDead after pair with prob:  [0.85785264]\n",
            "\tDead after pair with prob:  [0.8767519]\n",
            "\tDead after pair with prob:  [0.01287731]\n",
            "\tDead after pair with prob:  [0.96572787]\n",
            "\tDead after pair with prob:  [0.9545467]\n",
            "\tDead after pair with prob:  [0.75694853]\n",
            "\tStuck, ep reward -100.77944119887705\n",
            "\tStuck, ep reward -101.8230057781021\n",
            "\tStuck, ep reward -179.95703967965122\n",
            "\tDead after pair with prob:  [0.8611915]\n",
            "Episode: 100\tAverage Reward: -231\tAverage steps: 479\t Total: 14551\n",
            "Evaluation. Timesteps: 14551 Episodes: 100\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -54. All rewards: [-83, -56, -46, -38, -45]\n",
            "\tStuck, ep reward -50.901801275295654\n",
            "\tStuck, ep reward -55.488886319879754\n",
            "\tDead after pair with prob:  [0.7183461]\n",
            "\tDead after pair with prob:  [0.32366273]\n",
            "\tStuck, ep reward -162.26298309039692\n",
            "Evaluation. Timesteps: 17907 Episodes: 105\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -143. All rewards: [-229, -81, -192, -96, -116]\n",
            "\tStuck, ep reward -92.53802386694674\n",
            "\tStuck, ep reward -141.95469475266816\n",
            "\tStuck, ep reward -26.55404289791631\n",
            "\tStuck, ep reward -40.72074155413971\n",
            "\tStuck, ep reward -19.848662115444046\n",
            "Episode: 110\tAverage Reward: -101\tAverage steps: 835\t Total: 22902\n",
            "Evaluation. Timesteps: 22902 Episodes: 110\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -10. All rewards: [-7, -53, -2, 2, 7]\n",
            "\tStuck, ep reward -52.969064498508764\n",
            "\tDead after pair with prob:  [0.02166479]\n",
            "\tDead after pair with prob:  [0.43400723]\n",
            "\tStuck, ep reward -69.07572436735198\n",
            "\tStuck, ep reward -41.13537711195683\n",
            "Evaluation. Timesteps: 26424 Episodes: 115\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -84. All rewards: [-84, -69, -55, -124, -87]\n",
            "\tDead after pair with prob:  [0.8269777]\n",
            "\tStuck, ep reward -50.262072528625616\n",
            "\tDead after pair with prob:  [0.57030886]\n",
            "\tDead after pair with prob:  [0.73439944]\n",
            "\tDead after pair with prob:  [0.8926409]\n",
            "Episode: 120\tAverage Reward: -115\tAverage steps: 574\t Total: 28643\n",
            "Evaluation. Timesteps: 28643 Episodes: 120\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -168. All rewards: [-169, -43, -265, -219, -144]\n",
            "\tStuck, ep reward -68.0890664833185\n",
            "\tDead after pair with prob:  [0.30150047]\n",
            "\tDead after pair with prob:  [0.61307603]\n",
            "\tDead after pair with prob:  [0.39337468]\n",
            "\tDead after pair with prob:  [0.7325291]\n",
            "Evaluation. Timesteps: 32274 Episodes: 125\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tMean reward: -120. All rewards: [-82, -74, -161, -187, -95]\n",
            "\tDead after pair with prob:  [0.82372737]\n",
            "\tDead after pair with prob:  [0.87028146]\n",
            "\tDead after pair with prob:  [0.9061291]\n",
            "\tStuck, ep reward -51.51829258576295\n",
            "\tStuck, ep reward -43.44578096322364\n",
            "Episode: 130\tAverage Reward: -154\tAverage steps: 740\t Total: 36047\n",
            "Evaluation. Timesteps: 36047 Episodes: 130\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -81. All rewards: [-77, -72, -47, -59, -152]\n",
            "\tStuck, ep reward -55.51609308251461\n",
            "\tStuck, ep reward -37.775423208390315\n",
            "\tDead after pair with prob:  [0.8896779]\n",
            "\tDead after pair with prob:  [0.9027574]\n",
            "\tStuck, ep reward -109.95032476821902\n",
            "Evaluation. Timesteps: 40038 Episodes: 135\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -68. All rewards: [-69, -93, -63, -59, -58]\n",
            "\tStuck, ep reward -72.29383426895025\n",
            "\tStuck, ep reward -104.20776575425202\n",
            "\tStuck, ep reward -117.38565890947227\n",
            "\tStuck, ep reward -81.71146695229069\n",
            "\tStuck, ep reward -117.97539960171426\n",
            "Episode: 140\tAverage Reward: -108\tAverage steps: 898\t Total: 45033\n",
            "Evaluation. Timesteps: 45033 Episodes: 140\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -84. All rewards: [-157, -5, -35, -57, -166]\n",
            "\tStuck, ep reward -176.65440372749828\n",
            "\tStuck, ep reward -83.17826525329868\n",
            "\tStuck, ep reward -60.64756424291175\n",
            "\tStuck, ep reward -50.22147615111149\n",
            "\tDead after pair with prob:  [0.9057762]\n",
            "Evaluation. Timesteps: 49356 Episodes: 145\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -54. All rewards: [-20, -46, -116, -20, -68]\n",
            "\tStuck, ep reward -33.351966531174504\n",
            "\tStuck, ep reward -71.68568117834789\n",
            "\tStuck, ep reward -21.903318427986378\n",
            "\tDead after pair with prob:  [0.9152647]\n",
            "\tDead after pair with prob:  [0.95648557]\n",
            "Episode: 150\tAverage Reward: -121\tAverage steps: 830\t Total: 53334\n",
            "Evaluation. Timesteps: 53334 Episodes: 150\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead True\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead True\tUpdate True\n",
            "\t\tMean reward: -79. All rewards: [-68, -90, -44, -78, -112]\n",
            "\tStuck, ep reward -26.81165797502205\n",
            "\tStuck, ep reward -78.33026347120357\n",
            "\tStuck, ep reward -75.76214505335679\n",
            "\tStuck, ep reward -69.31209156754888\n",
            "\tStuck, ep reward -34.59735610555639\n",
            "Evaluation. Timesteps: 58329 Episodes: 155\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -82. All rewards: [-90, -53, -27, -183, -58]\n",
            "\tStuck, ep reward -106.26579148228213\n",
            "\tStuck, ep reward -124.42915022325417\n",
            "\tDead after pair with prob:  [0.8917025]\n",
            "\tDead after pair with prob:  [0.5687001]\n",
            "Episode: 160\tAverage Reward: -44\tAverage steps: 783\t Total: 61166\n",
            "Evaluation. Timesteps: 61166 Episodes: 160\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -70. All rewards: [-58, -55, -111, -79, -49]\n",
            "\tStuck, ep reward -35.08112507572177\n",
            "\tStuck, ep reward -41.754712143328156\n",
            "\tStuck, ep reward -60.30188071970287\n",
            "\tStuck, ep reward -39.8550774569246\n",
            "Evaluation. Timesteps: 65602 Episodes: 165\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -50. All rewards: [-33, -95, -16, -45, -60]\n",
            "\tStuck, ep reward -53.529265816743305\n",
            "\tStuck, ep reward -38.682174689238124\n",
            "\tStuck, ep reward -2.039437106310883\n",
            "\tStuck, ep reward -61.465872550935735\n",
            "\tStuck, ep reward -64.52731080074757\n",
            "Episode: 170\tAverage Reward: -18\tAverage steps: 943\t Total: 70597\n",
            "Evaluation. Timesteps: 70597 Episodes: 170\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -40. All rewards: [-72, -19, -34, -38, -38]\n",
            "\tStuck, ep reward -86.80702353012737\n",
            "\tStuck, ep reward -56.48609106999199\n",
            "\tStuck, ep reward -45.174759486835626\n",
            "\tStuck, ep reward -38.99960336043443\n",
            "\tStuck, ep reward -49.9400825616099\n",
            "Evaluation. Timesteps: 75592 Episodes: 175\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -46. All rewards: [-39, -32, -48, -66, -43]\n",
            "\tStuck, ep reward -58.84009007421855\n",
            "\tStuck, ep reward -22.941745075906457\n",
            "\tStuck, ep reward -23.35042999604227\n",
            "\tStuck, ep reward -38.88520716272045\n",
            "\tStuck, ep reward -42.691008769416364\n",
            "Episode: 180\tAverage Reward: -46\tAverage steps: 999\t Total: 80587\n",
            "Evaluation. Timesteps: 80587 Episodes: 180\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -31. All rewards: [-27, -32, -35, -37, -25]\n",
            "\tStuck, ep reward -57.339251289131205\n",
            "\tStuck, ep reward -28.66546997658902\n",
            "\tStuck, ep reward -41.8630772841306\n",
            "\tStuck, ep reward -55.250652229317915\n",
            "\tStuck, ep reward -51.88499961690066\n",
            "Evaluation. Timesteps: 85582 Episodes: 185\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tDead False\tUpdate False\n",
            "\t\tMean reward: -51. All rewards: [-39, -54, -45, -76, -42]\n",
            "\tStuck, ep reward -60.19067710785084\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhMFCYmMyEJe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}